#!/usr/bin/env python3
"""
ç·¨é›†ä¼šè­°ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ 
- ã‚¯ãƒªã‚¹ãƒã‚¹ä»¥å¤–ã§è¦‹è½ã¨ã—ãŒã¡ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ©Ÿä¼šã‚’è‡ªå‹•ç™ºè¦‹
- ç›´è¿‘30æ—¥é–“ã®æˆé•·åˆ†æã¨å‰å¹´åŒæ™‚æœŸæ¯”è¼ƒ
- ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªç·¨é›†è¨ˆç”»æ”¯æ´
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import logging

# OAuthèªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .oauth_google_apis import OAuthGoogleAPIsIntegration
except ImportError:
    from oauth_google_apis import OAuthGoogleAPIsIntegration

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/editorial_meeting_recommender.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EditorialMeetingRecommender:
    def __init__(self, credentials_path='config/oauth_credentials.json', token_path='config/token.json'):
        """
        ç·¨é›†ä¼šè­°æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        
        Args:
            credentials_path (str): OAuthèªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            token_path (str): ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.api = OAuthGoogleAPIsIntegration(credentials_path, token_path)
        self.non_christmas_keywords = self._define_non_christmas_keywords()
        
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs('logs', exist_ok=True)
        os.makedirs('data/processed', exist_ok=True)
        os.makedirs('data/editorial_meeting', exist_ok=True)
    
    def _define_non_christmas_keywords(self) -> Dict[str, List[str]]:
        """ã‚¯ãƒªã‚¹ãƒã‚¹ä»¥å¤–ã®å­£ç¯€ã‚¤ãƒ™ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®šç¾©"""
        return {
            'new_year': [
                'æ­£æœˆ', 'å¹´è³€', 'ãŠå¹´è³€', 'æ–°å¹´', 'è¿æ˜¥', 'åˆè©£',
                'ãŠæ­£æœˆ', 'æ­£æœˆã‚®ãƒ•ãƒˆ', 'æ–°å¹´ã‚®ãƒ•ãƒˆ', 'å¹´è³€çŠ¶'
            ],
            'coming_of_age': [
                'æˆäººå¼', 'æŒ¯è¢–', 'æˆäººç¥ã„', 'æˆäººå¼ã‚®ãƒ•ãƒˆ',
                'æˆäººå¼ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ', 'æŒ¯è¢–ã‚®ãƒ•ãƒˆ'
            ],
            'valentine_prep': [
                'ãƒãƒ¬ãƒ³ã‚¿ã‚¤ãƒ³', 'ãƒãƒ§ã‚³', 'ãƒãƒ§ã‚³ãƒ¬ãƒ¼ãƒˆ', 'ãƒãƒ¬ãƒ³ã‚¿ã‚¤ãƒ³ã‚®ãƒ•ãƒˆ',
                'ãƒãƒ¬ãƒ³ã‚¿ã‚¤ãƒ³æº–å‚™', 'æ‰‹ä½œã‚Šãƒãƒ§ã‚³'
            ],
            'school_entrance': [
                'å…¥å­¦ç¥ã„', 'å…¥åœ’ç¥ã„', 'å…¥å­¦å¼', 'å…¥åœ’å¼',
                'å…¥å­¦ç¥ã„ã‚®ãƒ•ãƒˆ', 'å…¥åœ’ç¥ã„ã‚®ãƒ•ãƒˆ', 'å’æ¥­ç¥ã„'
            ],
            'white_day': [
                'ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‡ãƒ¼', 'ãŠè¿”ã—', 'ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‡ãƒ¼ç°è¿”ã—',
                'ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‡ãƒ¼ã‚®ãƒ•ãƒˆ', 'ãŠè¿”ã—ã‚®ãƒ•ãƒˆ'
            ],
            'winter_events': [
                'å†¬ã‚®ãƒ•ãƒˆ', 'å¯’ä¸­è¦‹èˆã„', 'ç¯€åˆ†', 'ã²ãªç¥­ã‚Š',
                'æ˜¥ã‚®ãƒ•ãƒˆ', 'å’æ¥­ã‚·ãƒ¼ã‚ºãƒ³', 'æ˜¥ã®ã‚®ãƒ•ãƒˆ'
            ]
        }
    
    def get_ga4_data_for_period(self, start_date: str, end_date: str) -> pd.DataFrame:
        """
        æŒ‡å®šæœŸé–“ã®GA4ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå£²ä¸Šãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
        
        Args:
            start_date (str): é–‹å§‹æ—¥ (YYYY-MM-DD)
            end_date (str): çµ‚äº†æ—¥ (YYYY-MM-DD)
        
        Returns:
            pd.DataFrame: GA4ãƒ‡ãƒ¼ã‚¿
        """
        if not self.api.ga4_service:
            logger.error("GA4ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return pd.DataFrame()
        
        try:
            # GA4ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            request_body = {
                'dateRanges': [{'startDate': start_date, 'endDate': end_date}],
                'dimensions': [
                    {'name': 'pagePath'},
                    {'name': 'pageTitle'}
                ],
                'metrics': [
                    {'name': 'sessions'},
                    {'name': 'totalUsers'},
                    {'name': 'screenPageViews'},
                    {'name': 'conversions'},
                    {'name': 'totalRevenue'},
                    {'name': 'purchaseRevenue'}
                ],
                'limit': 10000
            }
            
            # APIå‘¼ã³å‡ºã—
            response = self.api.ga4_service.properties().runReport(
                property=f'properties/{self.api.ga4_property_id}',
                body=request_body
            ).execute()
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
            data = []
            if 'rows' in response:
                for row in response['rows']:
                    row_data = {}
                    
                    # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å€¤ã®å–å¾—
                    row_data['pagePath'] = row['dimensionValues'][0].get('value', '')
                    row_data['pageTitle'] = row['dimensionValues'][1].get('value', '')
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã®å–å¾—
                    row_data['sessions'] = float(row['metricValues'][0].get('value', '0'))
                    row_data['totalUsers'] = float(row['metricValues'][1].get('value', '0'))
                    row_data['screenPageViews'] = float(row['metricValues'][2].get('value', '0'))
                    row_data['conversions'] = float(row['metricValues'][3].get('value', '0'))
                    row_data['totalRevenue'] = float(row['metricValues'][4].get('value', '0'))
                    row_data['purchaseRevenue'] = float(row['metricValues'][5].get('value', '0'))
                    
                    data.append(row_data)
            
            df = pd.DataFrame(data)
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ ({start_date} - {end_date})")
            return df
            
        except Exception as e:
            logger.error(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def get_gsc_data_for_period(self, start_date: str, end_date: str, 
                              dimensions: List[str] = None, row_limit: int = 25000) -> pd.DataFrame:
        """
        æŒ‡å®šæœŸé–“ã®GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        
        Args:
            start_date (str): é–‹å§‹æ—¥ (YYYY-MM-DD)
            end_date (str): çµ‚äº†æ—¥ (YYYY-MM-DD)
            dimensions (list): å–å¾—ã™ã‚‹ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
            row_limit (int): å–å¾—è¡Œæ•°ä¸Šé™
        
        Returns:
            pd.DataFrame: GSCãƒ‡ãƒ¼ã‚¿
        """
        if not self.api.gsc_service:
            logger.error("GSCã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return pd.DataFrame()
        
        if not dimensions:
            dimensions = ['date', 'query', 'page', 'country', 'device']
        
        try:
            # GSCãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            request = {
                'startDate': start_date,
                'endDate': end_date,
                'dimensions': dimensions,
                'rowLimit': row_limit,
                'startRow': 0
            }
            
            # APIå‘¼ã³å‡ºã—
            response = self.api.gsc_service.searchanalytics().query(
                siteUrl=self.api.gsc_site_url,
                body=request
            ).execute()
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
            data = []
            if 'rows' in response:
                for row in response['rows']:
                    row_data = {
                        'clicks': row.get('clicks', 0),
                        'impressions': row.get('impressions', 0),
                        'ctr': row.get('ctr', 0),
                        'position': row.get('position', 0)
                    }
                    
                    # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å€¤ã®è¿½åŠ 
                    for i, dimension in enumerate(dimensions):
                        if i < len(row.get('keys', [])):
                            row_data[dimension] = row['keys'][i]
                    
                    data.append(row_data)
            
            df = pd.DataFrame(data)
            logger.info(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ ({start_date} - {end_date})")
            return df
            
        except Exception as e:
            logger.error(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def analyze_recent_growth(self) -> Dict[str, pd.DataFrame]:
        """
        ç›´è¿‘30æ—¥é–“ã§ä¼¸ã³ã¦ã„ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ†æ
        
        Returns:
            Dict[str, pd.DataFrame]: æˆé•·åˆ†æçµæœ
        """
        logger.info("ç›´è¿‘30æ—¥é–“ã®æˆé•·åˆ†æé–‹å§‹")
        
        try:
            # æœŸé–“è¨­å®š
            end_date = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')  # GSCã¯3æ—¥å‰ã¾ã§
            recent_start = (datetime.now() - timedelta(days=33)).strftime('%Y-%m-%d')  # ç›´è¿‘30æ—¥
            previous_start = (datetime.now() - timedelta(days=63)).strftime('%Y-%m-%d')  # å‰30æ—¥
            
            # GSCãƒ‡ãƒ¼ã‚¿å–å¾—
            recent_gsc_data = self.get_gsc_data_for_period(recent_start, end_date)
            previous_gsc_data = self.get_gsc_data_for_period(previous_start, recent_start)
            
            # GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå£²ä¸Šã¨ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ç”¨ï¼‰
            recent_ga4_data = self.get_ga4_data_for_period(recent_start, end_date)
            
            if recent_gsc_data.empty or previous_gsc_data.empty:
                logger.warning("æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return {}
            
            # ãƒšãƒ¼ã‚¸åˆ¥ã§é›†è¨ˆ
            recent_pages = self._aggregate_page_data(recent_gsc_data)
            previous_pages = self._aggregate_page_data(previous_gsc_data)
            
            # æˆé•·ç‡è¨ˆç®—
            growth_analysis = self._calculate_growth_metrics(recent_pages, previous_pages)
            
            # GA4ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆå£²ä¸Šã¨ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
            growth_analysis = self._integrate_ga4_data(growth_analysis, recent_ga4_data)
            
            logger.info(f"æˆé•·åˆ†æå®Œäº†: {len(growth_analysis)}ãƒšãƒ¼ã‚¸")
            return {
                'recent_gsc_data': recent_gsc_data,
                'previous_gsc_data': previous_gsc_data,
                'recent_ga4_data': recent_ga4_data,
                'growth_analysis': growth_analysis
            }
            
        except Exception as e:
            logger.error(f"æˆé•·åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _aggregate_page_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """ãƒšãƒ¼ã‚¸åˆ¥ã§GSCãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ"""
        if data.empty or 'page' not in data.columns:
            return pd.DataFrame()
        
        page_stats = data.groupby('page').agg({
            'clicks': 'sum',
            'impressions': 'sum',
            'ctr': 'mean',
            'position': 'mean'
        }).reset_index()
        
        # CTRã‚’å†è¨ˆç®—
        page_stats['ctr_calculated'] = (page_stats['clicks'] / page_stats['impressions'] * 100).fillna(0)
        page_stats['avg_position'] = page_stats['position'].round(2)
        
        return page_stats
    
    def _calculate_growth_metrics(self, recent_data: pd.DataFrame, previous_data: pd.DataFrame) -> pd.DataFrame:
        """æˆé•·ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        if recent_data.empty or previous_data.empty:
            return pd.DataFrame()
        
        # ãƒãƒ¼ã‚¸
        merged = pd.merge(
            recent_data, 
            previous_data, 
            on='page', 
            suffixes=('_recent', '_previous'),
            how='outer'
        ).fillna(0)
        
        # æˆé•·ç‡è¨ˆç®—
        merged['clicks_growth_rate'] = (
            (merged['clicks_recent'] - merged['clicks_previous']) / 
            merged['clicks_previous'].replace(0, 1) * 100
        ).fillna(0)
        
        merged['impressions_growth_rate'] = (
            (merged['impressions_recent'] - merged['impressions_previous']) / 
            merged['impressions_previous'].replace(0, 1) * 100
        ).fillna(0)
        
        # ãƒã‚¸ã‚·ãƒ§ãƒ³æ”¹å–„
        merged['position_improvement'] = merged['avg_position_previous'] - merged['avg_position_recent']
        
        # CTRæ”¹å–„
        merged['ctr_improvement'] = merged['ctr_calculated_recent'] - merged['ctr_calculated_previous']
        
        return merged
    
    def _integrate_ga4_data(self, growth_data: pd.DataFrame, ga4_data: pd.DataFrame) -> pd.DataFrame:
        """GA4ãƒ‡ãƒ¼ã‚¿ã‚’æˆé•·åˆ†æãƒ‡ãƒ¼ã‚¿ã«çµ±åˆ"""
        if growth_data.empty or ga4_data.empty:
            return growth_data
        
        try:
            # GA4ãƒ‡ãƒ¼ã‚¿ã‚’ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ã§é›†è¨ˆ
            ga4_aggregated = ga4_data.groupby('pagePath').agg({
                'sessions': 'sum',
                'totalUsers': 'sum',
                'screenPageViews': 'sum',
                'conversions': 'sum',
                'totalRevenue': 'sum',
                'purchaseRevenue': 'sum',
                'pageTitle': 'first'  # æœ€åˆã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            }).reset_index()
            
            # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆè¤‡æ•°ã®å£²ä¸ŠæŒ‡æ¨™ã‹ã‚‰æœ€å¤§å€¤ã‚’å–å¾—ï¼‰
            ga4_aggregated['max_revenue'] = ga4_aggregated[['totalRevenue', 'purchaseRevenue']].max(axis=1)
            
            # ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ã‚’çµ±ä¸€ï¼ˆGSCã®'page'ã¨GA4ã®'pagePath'ã‚’ãƒãƒƒãƒãƒ³ã‚°ï¼‰
            # GSCã®ãƒšãƒ¼ã‚¸URLã‹ã‚‰GA4ã®ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ã«å¤‰æ›
            def normalize_page_path(url):
                if pd.isna(url):
                    return ''
                # ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã‚’é™¤å»ã—ã¦ãƒ‘ã‚¹éƒ¨åˆ†ã®ã¿å–å¾—
                if 'moodmark' in url:
                    parts = url.split('/moodmark')
                    if len(parts) > 1:
                        return '/moodmark' + parts[1].split('?')[0]  # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é™¤å»
                return url
            
            growth_data['normalized_page'] = growth_data['page'].apply(normalize_page_path)
            ga4_aggregated['normalized_page'] = ga4_aggregated['pagePath'].apply(normalize_page_path)
            
            # ãƒãƒ¼ã‚¸
            merged = pd.merge(
                growth_data,
                ga4_aggregated[['normalized_page', 'sessions', 'totalUsers', 'screenPageViews', 'conversions', 'totalRevenue', 'purchaseRevenue', 'max_revenue', 'pageTitle']],
                on='normalized_page',
                how='left'
            )
            
            # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
            merged = merged.drop('normalized_page', axis=1)
            
            # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
            merged['sessions'] = merged['sessions'].fillna(0)
            merged['totalUsers'] = merged['totalUsers'].fillna(0)
            merged['screenPageViews'] = merged['screenPageViews'].fillna(0)
            merged['conversions'] = merged['conversions'].fillna(0)
            merged['totalRevenue'] = merged['totalRevenue'].fillna(0)
            merged['purchaseRevenue'] = merged['purchaseRevenue'].fillna(0)
            merged['max_revenue'] = merged['max_revenue'].fillna(0)
            merged['pageTitle'] = merged['pageTitle'].fillna('')
            
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(merged)}ãƒšãƒ¼ã‚¸")
            return merged
            
        except Exception as e:
            logger.error(f"GA4ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            return growth_data
    
    def analyze_year_over_year(self) -> Dict[str, Any]:
        """
        å‰å¹´åŒæ™‚æœŸï¼ˆ2023å¹´12æœˆ-2024å¹´1æœˆï¼‰ã¨ã®æ¯”è¼ƒåˆ†æ
        
        Returns:
            Dict[str, Any]: å‰å¹´æ¯”è¼ƒåˆ†æçµæœ
        """
        logger.info("å‰å¹´åŒæ™‚æœŸæ¯”è¼ƒåˆ†æé–‹å§‹")
        
        try:
            # å‰å¹´æœŸé–“ï¼ˆ2023å¹´12æœˆ-2024å¹´1æœˆï¼‰
            last_year_start = '2023-12-01'
            last_year_end = '2024-01-31'
            
            # ä»Šå¹´ã®åŒã˜æœŸé–“ï¼ˆ2024å¹´12æœˆ-2025å¹´1æœˆã®äºˆæ¸¬ã¾ãŸã¯ç¾åœ¨ã¾ã§ï¼‰
            current_start = '2024-12-01'
            current_end = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            last_year_data = self.get_gsc_data_for_period(last_year_start, last_year_end)
            current_data = self.get_gsc_data_for_period(current_start, current_end)
            
            if last_year_data.empty:
                logger.warning("å‰å¹´ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return {}
            
            # åˆ†æå®Ÿè¡Œ
            yoy_analysis = {
                'last_year_data': last_year_data,
                'current_data': current_data,
                'trending_keywords': self._identify_trending_keywords(last_year_data),
                'seasonal_patterns': self._analyze_seasonal_patterns(last_year_data),
                'preparation_gaps': self._identify_preparation_gaps(last_year_data, current_data)
            }
            
            logger.info("å‰å¹´åŒæ™‚æœŸæ¯”è¼ƒåˆ†æå®Œäº†")
            return yoy_analysis
            
        except Exception as e:
            logger.error(f"å‰å¹´æ¯”è¼ƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _identify_trending_keywords(self, data: pd.DataFrame) -> List[Dict]:
        """å‰å¹´ã«ãƒˆãƒ¬ãƒ³ãƒ‰ã ã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®š"""
        if data.empty or 'query' not in data.columns:
            return []
        
        # ã‚¯ã‚¨ãƒªåˆ¥ã§é›†è¨ˆ
        query_stats = data.groupby('query').agg({
            'clicks': 'sum',
            'impressions': 'sum',
            'ctr': 'mean',
            'position': 'mean'
        }).reset_index()
        
        # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        high_performance = query_stats[
            (query_stats['clicks'] >= 100) & 
            (query_stats['position'] <= 10)
        ].sort_values('clicks', ascending=False)
        
        return high_performance.head(50).to_dict('records')
    
    def _analyze_seasonal_patterns(self, data: pd.DataFrame) -> Dict[str, List]:
        """å­£ç¯€ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        if data.empty:
            return {}
        
        seasonal_patterns = {}
        
        # å„å­£ç¯€ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ†ã‚´ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        for category, keywords in self.non_christmas_keywords.items():
            pattern_data = self._filter_data_by_keywords(data, keywords)
            if not pattern_data.empty:
                # ãƒšãƒ¼ã‚¸åˆ¥ã§é›†è¨ˆ
                page_stats = pattern_data.groupby('page').agg({
                    'clicks': 'sum',
                    'impressions': 'sum',
                    'position': 'mean'
                }).reset_index()
                
                seasonal_patterns[category] = page_stats.sort_values('clicks', ascending=False).head(20).to_dict('records')
        
        return seasonal_patterns
    
    def _filter_data_by_keywords(self, data: pd.DataFrame, keywords: List[str]) -> pd.DataFrame:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if data.empty or 'query' not in data.columns:
            return pd.DataFrame()
        
        keyword_pattern = '|'.join(keywords)
        filtered_data = data[
            data['query'].str.contains(keyword_pattern, case=False, na=False)
        ].copy()
        
        return filtered_data
    
    def _identify_preparation_gaps(self, last_year_data: pd.DataFrame, current_data: pd.DataFrame) -> List[Dict]:
        """ä»Šå¹´ã®æº–å‚™ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®š"""
        gaps = []
        
        # å‰å¹´ã«ä¼¸ã³ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ä»Šå¹´ã®æº–å‚™çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
        last_year_keywords = self._identify_trending_keywords(last_year_data)
        
        if current_data.empty:
            # ç¾åœ¨ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€å…¨ã¦ã®å‰å¹´ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ã‚®ãƒ£ãƒƒãƒ—ã¨ã—ã¦å ±å‘Š
            for keyword in last_year_keywords[:20]:
                gaps.append({
                    'keyword': keyword['query'],
                    'last_year_clicks': keyword['clicks'],
                    'preparation_status': 'not_started',
                    'recommendation': 'æ—©æ€¥ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æº–å‚™ã‚’é–‹å§‹'
                })
        else:
            # ä»Šå¹´ã®ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒ
            current_keywords = self._identify_trending_keywords(current_data)
            current_keyword_set = set([kw['query'] for kw in current_keywords])
            
            for keyword in last_year_keywords[:20]:
                if keyword['query'] not in current_keyword_set:
                    gaps.append({
                        'keyword': keyword['query'],
                        'last_year_clicks': keyword['clicks'],
                        'preparation_status': 'missing',
                        'recommendation': 'å‰å¹´å®Ÿç¸¾ã®ã‚ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æº–å‚™ãŒä¸è¶³'
                    })
        
        return gaps
    
    def calculate_maintenance_priority_score(self, page_data: Dict) -> Dict:
        """
        ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        
        Args:
            page_data (Dict): ãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            Dict: ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°çµæœ
        """
        try:
            # å„è¦ç´ ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
            impression_score = self._calculate_impression_score(page_data)
            ctr_opportunity_score = self._calculate_ctr_opportunity_score(page_data)
            ranking_opportunity_score = self._calculate_ranking_opportunity_score(page_data)
            growth_score = self._calculate_growth_score(page_data)
            
            # é‡ã¿ä»˜ã‘ç·åˆã‚¹ã‚³ã‚¢ï¼ˆã‚ˆã‚Šå®Ÿç”¨çš„ãªé…åˆ†ï¼‰
            weights = {
                'clicks': 0.35,        # ã‚¯ãƒªãƒƒã‚¯æ•°ï¼ˆæœ€é‡è¦ï¼‰
                'impression': 0.25,    # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°
                'ranking_opportunity': 0.20,  # é †ä½æ”¹å–„æ©Ÿä¼š
                'ctr_opportunity': 0.10,      # CTRæ”¹å–„æ©Ÿä¼š
                'growth': 0.10         # æˆé•·ç‡
            }
            
            # ã‚¯ãƒªãƒƒã‚¯æ•°ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
            clicks_score = self._calculate_clicks_score(page_data)
            
            total_score = (
                impression_score * weights['impression'] +
                ctr_opportunity_score * weights['ctr_opportunity'] +
                ranking_opportunity_score * weights['ranking_opportunity'] +
                growth_score * weights['growth'] +
                clicks_score * weights['clicks']
            )
            
            return {
                'total_score': round(total_score, 2),
                'impression_score': round(impression_score, 2),
                'ctr_opportunity_score': round(ctr_opportunity_score, 2),
                'ranking_opportunity_score': round(ranking_opportunity_score, 2),
                'growth_score': round(growth_score, 2),
                'clicks_score': round(clicks_score, 2),
                'weights': weights
            }
            
        except Exception as e:
            logger.error(f"ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return {'total_score': 0}
    
    def _calculate_impression_score(self, page_data: Dict) -> float:
        """ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        impressions = page_data.get('impressions_recent', 0)
        if impressions == 0:
            return 0
        
        # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§æ­£è¦åŒ–
        import math
        score = min(100, math.log10(impressions + 1) * 20)
        return score
    
    def _calculate_ctr_opportunity_score(self, page_data: Dict) -> float:
        """CTRæ”¹å–„æ©Ÿä¼šã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        ctr = page_data.get('ctr_calculated_recent', 0)
        impressions = page_data.get('impressions_recent', 0)
        
        if impressions < 100:  # æœ€å°ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°
            return 0
        
        # CTRãŒä½ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼ˆæ”¹å–„ä½™åœ°å¤§ï¼‰
        if ctr < 1.0:
            return 100
        elif ctr < 2.0:
            return 80
        elif ctr < 3.0:
            return 60
        elif ctr < 5.0:
            return 40
        else:
            return 20
    
    def _calculate_ranking_opportunity_score(self, page_data: Dict) -> float:
        """é †ä½æ”¹å–„æ©Ÿä¼šã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        position = page_data.get('avg_position_recent', 0)
        
        if position == 0:
            return 0
        
        # 10-20ä½ãŒæœ€é©æ”¹å–„æ©Ÿä¼š
        if 10 <= position <= 20:
            return 100
        elif 5 <= position < 10:
            return 80
        elif 20 < position <= 30:
            return 60
        elif position > 30:
            return 40
        else:  # ä¸Šä½5ä½ä»¥å†…
            return 20
    
    def _calculate_growth_score(self, page_data: Dict) -> float:
        """æˆé•·ç‡ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        growth_rate = page_data.get('clicks_growth_rate', 0)
        
        if growth_rate >= 50:
            return 100
        elif growth_rate >= 25:
            return 80
        elif growth_rate >= 10:
            return 60
        elif growth_rate >= 0:
            return 40
        else:
            return 20
    
    def _calculate_clicks_score(self, page_data: Dict) -> float:
        """ã‚¯ãƒªãƒƒã‚¯æ•°ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        clicks = page_data.get('clicks_recent', 0)
        if clicks == 0:
            return 0
        
        # ã‚ˆã‚Šå®Ÿç”¨çš„ãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        if clicks >= 10000:
            return 100
        elif clicks >= 5000:
            return 90
        elif clicks >= 2000:
            return 80
        elif clicks >= 1000:
            return 70
        elif clicks >= 500:
            return 60
        elif clicks >= 200:
            return 50
        elif clicks >= 100:
            return 40
        elif clicks >= 50:
            return 30
        elif clicks >= 20:
            return 20
        else:
            return 10
    
    def filter_non_christmas_content(self, data: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """
        ã‚¯ãƒªã‚¹ãƒã‚¹ä»¥å¤–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        
        Args:
            data (pd.DataFrame): GSCãƒ‡ãƒ¼ã‚¿
        
        Returns:
            Dict[str, pd.DataFrame]: ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
        """
        logger.info("ã‚¯ãƒªã‚¹ãƒã‚¹ä»¥å¤–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºé–‹å§‹")
        
        non_christmas_data = {}
        
        for category, keywords in self.non_christmas_keywords.items():
            filtered_data = self._filter_data_by_keywords(data, keywords)
            if not filtered_data.empty:
                non_christmas_data[category] = filtered_data
                logger.info(f"{category}: {len(filtered_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")
        
        return non_christmas_data
    
    def generate_editorial_recommendations(self, meeting_date: str = None) -> Dict[str, Any]:
        """
        ç·¨é›†ä¼šè­°æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        
        Args:
            meeting_date (str): ç·¨é›†ä¼šè­°æ—¥ä»˜ (YYYY-MM-DD)
        
        Returns:
            Dict[str, Any]: æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆ
        """
        logger.info("ç·¨é›†ä¼šè­°æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        
        try:
            # æ—¥ä»˜è¨­å®š
            if not meeting_date:
                meeting_date = datetime.now().strftime('%Y-%m-%d')
            
            # åˆ†æå®Ÿè¡Œ
            growth_analysis = self.analyze_recent_growth()
            yoy_analysis = self.analyze_year_over_year()
            
            # æ¨å¥¨è¨˜äº‹ã®é¸å®š
            recommendations = self._generate_recommendations(growth_analysis, yoy_analysis)
            
            # ãƒ¬ãƒãƒ¼ãƒˆçµ±åˆ
            report = {
                'report_metadata': {
                    'title': 'ç·¨é›†ä¼šè­°ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆ',
                    'meeting_date': meeting_date,
                    'generated_at': datetime.now().isoformat(),
                    'site_url': self.api.gsc_site_url,
                    'account': 'nakamura@likepass.net'
                },
                'growth_analysis': growth_analysis,
                'yoy_analysis': yoy_analysis,
                'recommendations': recommendations,
                'non_christmas_opportunities': self._extract_non_christmas_opportunities(yoy_analysis)
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            self._save_reports(report, meeting_date)
            
            logger.info("ç·¨é›†ä¼šè­°æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
            return report
            
        except Exception as e:
            logger.error(f"æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _generate_recommendations(self, growth_analysis: Dict, yoy_analysis: Dict) -> Dict[str, List]:
        """æ¨å¥¨è¨˜äº‹ã®ç”Ÿæˆ"""
        recommendations = {
            'top_priority': [],
            'high_growth': [],
            'seasonal_opportunities': [],
            'preparation_gaps': []
        }
        
        try:
            # æˆé•·åˆ†æã‹ã‚‰ä¸Šä½æ¨å¥¨ã‚’é¸å®š
            if 'growth_analysis' in growth_analysis:
                growth_data = growth_analysis['growth_analysis']
                if not growth_data.empty:
                    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
                    scored_pages = []
                    for _, row in growth_data.iterrows():
                        page_data = row.to_dict()
                        scores = self.calculate_maintenance_priority_score(page_data)
                        
                        scored_pages.append({
                            'page': row['page'],
                            'page_title': row.get('pageTitle', ''),
                            'scores': scores,
                            'metrics': {
                                'recent_clicks': int(row.get('clicks_recent', 0)),
                                'recent_impressions': int(row.get('impressions_recent', 0)),
                                'recent_ctr': round(row.get('ctr_calculated_recent', 0), 2),
                                'recent_position': round(row.get('avg_position_recent', 0), 1),
                                'clicks_growth_rate': round(row.get('clicks_growth_rate', 0), 1),
                                'impressions_growth_rate': round(row.get('impressions_growth_rate', 0), 1),
                                'recent_revenue': round(row.get('max_revenue', 0), 0),
                                'recent_sessions': int(row.get('sessions', 0)),
                                'recent_conversions': int(row.get('conversions', 0))
                            }
                        })
                    
                    # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
                    scored_pages.sort(key=lambda x: x['scores']['total_score'], reverse=True)
                    
                    # ä¸Šä½20ä»¶ã‚’æ¨å¥¨
                    recommendations['top_priority'] = scored_pages[:20]
                    recommendations['high_growth'] = scored_pages[20:40]
            
            # å‰å¹´åˆ†æã‹ã‚‰å­£ç¯€æ©Ÿä¼šã‚’æŠ½å‡º
            if 'seasonal_patterns' in yoy_analysis:
                for category, pages in yoy_analysis['seasonal_patterns'].items():
                    if pages:
                        recommendations['seasonal_opportunities'].extend(pages[:10])
            
            # æº–å‚™ã‚®ãƒ£ãƒƒãƒ—
            if 'preparation_gaps' in yoy_analysis:
                recommendations['preparation_gaps'] = yoy_analysis['preparation_gaps'][:20]
            
            return recommendations
            
        except Exception as e:
            logger.error(f"æ¨å¥¨ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return recommendations
    
    def _extract_non_christmas_opportunities(self, yoy_analysis: Dict) -> Dict[str, List]:
        """ã‚¯ãƒªã‚¹ãƒã‚¹ä»¥å¤–ã®æ©Ÿä¼šã‚’æŠ½å‡º"""
        opportunities = {}
        
        if 'seasonal_patterns' in yoy_analysis:
            for category, pages in yoy_analysis['seasonal_patterns'].items():
                if pages:
                    opportunities[category] = pages[:10]
        
        return opportunities
    
    def _save_reports(self, report: Dict[str, Any], meeting_date: str):
        """ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ï¼ˆMarkdownã€JSONã€CSVï¼‰"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # JSONè©³ç´°ãƒ‡ãƒ¼ã‚¿
            json_file = f'data/editorial_meeting/editorial_recommendations_{timestamp}.json'
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            # Markdownãƒ¬ãƒãƒ¼ãƒˆ
            markdown_file = f'data/editorial_meeting/editorial_recommendations_{timestamp}.md'
            markdown_content = self._format_report_as_markdown(report)
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # CSVç·¨é›†ä¼šè­°ç”¨ä¸€è¦§è¡¨
            csv_file = f'data/editorial_meeting/editorial_recommendations_{timestamp}.csv'
            csv_data = self._format_recommendations_as_csv(report)
            csv_data.to_csv(csv_file, index=False, encoding='utf-8-sig')
            
            logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†:")
            logger.info(f"  JSON: {json_file}")
            logger.info(f"  Markdown: {markdown_file}")
            logger.info(f"  CSV: {csv_file}")
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _format_report_as_markdown(self, report: Dict[str, Any]) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        metadata = report.get('report_metadata', {})
        recommendations = report.get('recommendations', {})
        non_christmas = report.get('non_christmas_opportunities', {})
        
        content = f"""# {metadata.get('title', 'ç·¨é›†ä¼šè­°æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆ')}

**ç·¨é›†ä¼šè­°æ—¥**: {metadata.get('meeting_date', 'N/A')}  
**ç”Ÿæˆæ—¥æ™‚**: {metadata.get('generated_at', 'N/A')}  
**ã‚µã‚¤ãƒˆURL**: {metadata.get('site_url', 'N/A')}  
**ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ**: {metadata.get('account', 'N/A')}

## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ğŸ¯ æœ€å„ªå…ˆãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨˜äº‹ TOP10

| é †ä½ | ãƒšãƒ¼ã‚¸URL | ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ« | ç·åˆã‚¹ã‚³ã‚¢ | ç¾åœ¨é †ä½ | ç›´è¿‘30æ—¥ã‚¯ãƒªãƒƒã‚¯ | å‰æœˆæ¯”æˆé•·ç‡ | ç›´è¿‘30æ—¥å£²ä¸Š | æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|------|-----------|----------------|------------|----------|------------------|--------------|--------------|----------------|
"""
        
        # æœ€å„ªå…ˆè¨˜äº‹ã®è¡¨ç¤º
        top_priority = recommendations.get('top_priority', [])
        for i, item in enumerate(top_priority[:10], 1):
            page_url = item['page'][:60] + "..." if len(item['page']) > 60 else item['page']
            page_title = item.get('page_title', '')[:30] + "..." if len(item.get('page_title', '')) > 30 else item.get('page_title', '')
            content += f"| {i} | {page_url} | {page_title} | {item['scores']['total_score']} | {item['metrics']['recent_position']}ä½ | {item['metrics']['recent_clicks']:,} | +{item['metrics']['clicks_growth_rate']}% | Â¥{item['metrics']['recent_revenue']:,} | ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ¨å¥¨ |\n"
        
        content += f"""
## ğŸ“ˆ ç›´è¿‘ä¼¸ã³ã¦ã„ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ TOP20

| ãƒšãƒ¼ã‚¸URL | ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ« | ç·åˆã‚¹ã‚³ã‚¢ | ç›´è¿‘30æ—¥ã‚¯ãƒªãƒƒã‚¯ | ã‚¯ãƒªãƒƒã‚¯æˆé•·ç‡ | ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æˆé•·ç‡ | ç¾åœ¨CTR | ç¾åœ¨é †ä½ | ç›´è¿‘30æ—¥å£²ä¸Š |
|-----------|----------------|------------|------------------|----------------|----------------------|---------|----------|--------------|
"""
        
        # é«˜æˆé•·è¨˜äº‹ã®è¡¨ç¤º
        high_growth = recommendations.get('high_growth', [])
        for item in high_growth[:20]:
            page_url = item['page'][:50] + "..." if len(item['page']) > 50 else item['page']
            page_title = item.get('page_title', '')[:25] + "..." if len(item.get('page_title', '')) > 25 else item.get('page_title', '')
            content += f"| {page_url} | {page_title} | {item['scores']['total_score']} | {item['metrics']['recent_clicks']:,} | +{item['metrics']['clicks_growth_rate']}% | +{item['metrics']['impressions_growth_rate']}% | {item['metrics']['recent_ctr']}% | {item['metrics']['recent_position']}ä½ | Â¥{item['metrics']['recent_revenue']:,} |\n"
        
        content += f"""
## ğŸ„ ã‚¯ãƒªã‚¹ãƒã‚¹ä»¥å¤–ã®å­£ç¯€ã‚¤ãƒ™ãƒ³ãƒˆæ©Ÿä¼š

"""
        
        # å­£ç¯€ã‚¤ãƒ™ãƒ³ãƒˆæ©Ÿä¼š
        for category, pages in non_christmas.items():
            if pages:
                category_name = category.replace('_', ' ').title()
                content += f"### {category_name}\n\n"
                content += "| ãƒšãƒ¼ã‚¸URL | ã‚¯ãƒªãƒƒã‚¯æ•° | ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•° | å¹³å‡é †ä½ |\n"
                content += "|-----------|------------|------------------|----------|\n"
                
                for page in pages[:10]:
                    page_url = page['page'][:60] + "..." if len(page['page']) > 60 else page['page']
                    content += f"| {page_url} | {page['clicks']:,} | {page['impressions']:,} | {page['position']:.1f}ä½ |\n"
                content += "\n"
        
        # æº–å‚™ã‚®ãƒ£ãƒƒãƒ—
        preparation_gaps = recommendations.get('preparation_gaps', [])
        if preparation_gaps:
            content += f"""
## âš ï¸ å‰å¹´å®Ÿç¸¾ã‹ã‚‰è¦‹ãŸæº–å‚™ã‚®ãƒ£ãƒƒãƒ—

| ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ | å‰å¹´ã‚¯ãƒªãƒƒã‚¯æ•° | æº–å‚™çŠ¶æ³ | æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|------------|----------------|----------|----------------|
"""
            for gap in preparation_gaps[:15]:
                content += f"| {gap['keyword']} | {gap['last_year_clicks']:,} | {gap['preparation_status']} | {gap['recommendation']} |\n"
        
        content += f"""
## ğŸ“‹ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

### 11æœˆæœ«ç´å“ï¼ˆ12æœˆUPï¼‰æ¨å¥¨è¨˜äº‹

1. **æœ€å„ªå…ˆãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹**: ç·åˆã‚¹ã‚³ã‚¢ä¸Šä½10è¨˜äº‹ã®æ”¹å–„
2. **æˆé•·æ©Ÿä¼šæ´»ç”¨**: ç›´è¿‘ä¼¸ã³ã¦ã„ã‚‹è¨˜äº‹ã®ã•ã‚‰ãªã‚‹æœ€é©åŒ–
3. **å­£ç¯€æº–å‚™**: ã‚¯ãƒªã‚¹ãƒã‚¹ä»¥å¤–ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„æº–å‚™
4. **ã‚®ãƒ£ãƒƒãƒ—è§£æ¶ˆ**: å‰å¹´å®Ÿç¸¾ãŒã‚ã‚‹ãŒä»Šå¹´æº–å‚™ä¸è¶³ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œ

### ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æœŸé™

- **11æœˆ25æ—¥**: æœ€å„ªå…ˆè¨˜äº‹ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Œäº†
- **11æœˆ30æ—¥**: é«˜æˆé•·è¨˜äº‹ã®æœ€é©åŒ–å®Œäº†
- **12æœˆ5æ—¥**: å­£ç¯€ã‚¤ãƒ™ãƒ³ãƒˆè¨˜äº‹ã®æº–å‚™å®Œäº†

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯nakamura@likepass.netã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
        
        return content
    
    def _format_recommendations_as_csv(self, report: Dict[str, Any]) -> pd.DataFrame:
        """CSVå½¢å¼ã®ç·¨é›†ä¼šè­°ç”¨ä¸€è¦§è¡¨ã‚’ä½œæˆ"""
        try:
            recommendations = report.get('recommendations', {})
            
            csv_data = []
            
            # æœ€å„ªå…ˆè¨˜äº‹
            for i, item in enumerate(recommendations.get('top_priority', []), 1):
                csv_data.append({
                    'å„ªå…ˆé †ä½': i,
                    'å®Œå…¨URL': item['page'],
                    'ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«': item.get('page_title', ''),
                    'è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«': self._extract_article_title(item['page']),
                    'ç·åˆã‚¹ã‚³ã‚¢': item['scores']['total_score'],
                    'ç¾åœ¨ã®é †ä½': f"{item['metrics']['recent_position']}ä½",
                    'ç›´è¿‘30æ—¥ã‚¯ãƒªãƒƒã‚¯æ•°': item['metrics']['recent_clicks'],
                    'å‰æœˆæ¯”æˆé•·ç‡': f"+{item['metrics']['clicks_growth_rate']}%",
                    'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°': item['metrics']['recent_impressions'],
                    'ç¾åœ¨CTR': f"{item['metrics']['recent_ctr']}%",
                    'ç›´è¿‘30æ—¥å£²ä¸Š': f"Â¥{item['metrics']['recent_revenue']:,}",
                    'ç›´è¿‘30æ—¥ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°': item['metrics']['recent_sessions'],
                    'ç›´è¿‘30æ—¥ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°': item['metrics']['recent_conversions'],
                    'æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³': 'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ¨å¥¨',
                    'ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æœŸé™': '11æœˆæœ«æ¨å¥¨'
                })
            
            return pd.DataFrame(csv_data)
            
        except Exception as e:
            logger.error(f"CSVå½¢å¼å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def _extract_article_title(self, page_path: str) -> str:
        """ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ã‹ã‚‰è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        try:
            # ãƒ‘ã‚¹ã®æœ€å¾Œã®éƒ¨åˆ†ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦ä½¿ç”¨
            title = page_path.split('/')[-1]
            # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
            import urllib.parse
            title = urllib.parse.unquote(title)
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’é™¤å»
            title = title.replace('.html', '').replace('.htm', '')
            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»
            if '?' in title:
                title = title.split('?')[0]
            return title[:80] + "..." if len(title) > 80 else title
        except:
            return page_path[:80] + "..." if len(page_path) > 80 else page_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç·¨é›†ä¼šè­°ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--meeting-date', help='ç·¨é›†ä¼šè­°æ—¥ä»˜ (YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    print("=== ç·¨é›†ä¼šè­°ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ  ===")
    if args.meeting_date:
        print(f"ç·¨é›†ä¼šè­°æ—¥: {args.meeting_date}")
    print()
    
    # æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    recommender = EditorialMeetingRecommender()
    
    # èªè¨¼ç¢ºèª
    if not recommender.api.credentials:
        print("âŒ èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        print("ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š")
        print("1. nakamura@likepass.netã§Google Cloud Consoleã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‹")
        print("2. config/oauth_credentials.json ãŒå­˜åœ¨ã™ã‚‹ã‹")
        print("3. GA4ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¨GSCã‚µã‚¤ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚‹ã‹")
        return
    
    print("âœ… èªè¨¼æˆåŠŸ: nakamura@likepass.net")
    
    # æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = recommender.generate_editorial_recommendations(args.meeting_date)
    
    if report:
        print("\n=== æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº† ===")
        print(f"ç·¨é›†ä¼šè­°æ—¥: {report['report_metadata']['meeting_date']}")
        print(f"ç”Ÿæˆæ—¥æ™‚: {report['report_metadata']['generated_at']}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        recommendations = report.get('recommendations', {})
        print(f"\n--- æ¨å¥¨è¨˜äº‹ã‚µãƒãƒªãƒ¼ ---")
        print(f"æœ€å„ªå…ˆãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹: {len(recommendations.get('top_priority', []))}è¨˜äº‹")
        print(f"é«˜æˆé•·è¨˜äº‹: {len(recommendations.get('high_growth', []))}è¨˜äº‹")
        print(f"å­£ç¯€æ©Ÿä¼š: {len(recommendations.get('seasonal_opportunities', []))}è¨˜äº‹")
        print(f"æº–å‚™ã‚®ãƒ£ãƒƒãƒ—: {len(recommendations.get('preparation_gaps', []))}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        
        # æœ€å„ªå…ˆè¨˜äº‹ã®è¡¨ç¤º
        top_priority = recommendations.get('top_priority', [])
        if top_priority:
            print(f"\n--- æœ€å„ªå…ˆãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨˜äº‹ TOP5 ---")
            for i, item in enumerate(top_priority[:5], 1):
                print(f"{i}. {item['page'][:60]}... (ã‚¹ã‚³ã‚¢: {item['scores']['total_score']})")
        
        print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã¯ data/editorial_meeting/ ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
    else:
        print("æ¨å¥¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()
