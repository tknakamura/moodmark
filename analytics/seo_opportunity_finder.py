#!/usr/bin/env python3
"""
SEOæ©Ÿä¼šç™ºè¦‹ãƒ„ãƒ¼ãƒ«
- æ¤œç´¢é †ä½4-10ä½ã®ã‚¯ã‚¨ãƒªæŠ½å‡ºï¼ˆé †ä½å‘ä¸Šã§å¤§å¹…ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å¢—ãŒè¦‹è¾¼ã‚ã‚‹ï¼‰
- é«˜ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ»ä½CTRã‚¯ã‚¨ãƒªã®ç‰¹å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«/ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ”¹å–„å€™è£œï¼‰
- ç«¶åˆã«è² ã‘ã¦ã„ã‚‹ã‚¯ã‚¨ãƒªã®åˆ†æ
- å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå‰å¹´åŒæœŸæ¯”ã§ã®æ©Ÿä¼šç™ºè¦‹ï¼‰
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import logging

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¾ãŸã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
try:
    from .oauth_google_apis import OAuthGoogleAPIsIntegration
except ImportError:
    from oauth_google_apis import OAuthGoogleAPIsIntegration

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/seo_opportunity_finder.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SEOOpportunityFinder:
    def __init__(self):
        """SEOæ©Ÿä¼šç™ºè¦‹ãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        self.api_integration = OAuthGoogleAPIsIntegration()
        self.config = self._load_config()
        
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs('logs', exist_ok=True)
        os.makedirs('data/processed', exist_ok=True)
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        config_file = 'config/analytics_config.json'
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_file}")
                return {}
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def get_gsc_data_for_period(self, start_date: str, end_date: str, site_url: str):
        """æœŸé–“æŒ‡å®šã§GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            if not self.api_integration.gsc_service:
                logger.warning("GSCã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return pd.DataFrame()
            
            # GSCãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            request = {
                'startDate': start_date,
                'endDate': end_date,
                'dimensions': ['page', 'query'],
                'rowLimit': 25000,
                'startRow': 0
            }
            
            # APIå‘¼ã³å‡ºã—
            response = self.api_integration.gsc_service.searchanalytics().query(
                siteUrl=site_url,
                body=request
            ).execute()
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
            data = []
            if 'rows' in response:
                for row in response['rows']:
                    row_data = {
                        'page': row['keys'][0] if len(row.get('keys', [])) > 0 else '',
                        'query': row['keys'][1] if len(row.get('keys', [])) > 1 else '',
                        'clicks': row.get('clicks', 0),
                        'impressions': row.get('impressions', 0),
                        'ctr': row.get('ctr', 0),
                        'position': row.get('position', 0)
                    }
                    data.append(row_data)
            
            df = pd.DataFrame(data)
            logger.info(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ (ã‚µã‚¤ãƒˆ: {site_url})")
            return df
            
        except Exception as e:
            logger.error(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def find_ranking_improvement_opportunities(self, gsc_data: pd.DataFrame, site_name: str) -> List[Dict]:
        """æ¤œç´¢é †ä½4-10ä½ã®ã‚¯ã‚¨ãƒªã‚’æŠ½å‡ºï¼ˆé †ä½å‘ä¸Šã§å¤§å¹…ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å¢—ãŒè¦‹è¾¼ã‚ã‚‹ï¼‰"""
        try:
            if gsc_data.empty:
                return []
            
            # é †ä½4-10ä½ã®ã‚¯ã‚¨ãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            opportunities = gsc_data[
                (gsc_data['position'] >= 4.0) & 
                (gsc_data['position'] <= 10.0) &
                (gsc_data['impressions'] >= 100)  # æœ€ä½ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°
            ].copy()
            
            if opportunities.empty:
                logger.warning(f"{site_name}: é †ä½æ”¹å–„æ©Ÿä¼šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
            
            # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆ
            opportunities = opportunities.sort_values('impressions', ascending=False)
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            result = []
            for _, row in opportunities.head(50).iterrows():  # TOP50
                # é †ä½æ”¹å–„æ™‚ã®äºˆæƒ³ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å¢—åŠ ã‚’è¨ˆç®—
                current_position = row['position']
                current_clicks = row['clicks']
                current_impressions = row['impressions']
                
                # é †ä½åˆ¥CTRã®æ¨å®šå€¤ï¼ˆæ¥­ç•Œå¹³å‡ï¼‰
                position_ctr_map = {
                    1: 0.30, 2: 0.15, 3: 0.10, 4: 0.07, 5: 0.05,
                    6: 0.04, 7: 0.03, 8: 0.02, 9: 0.02, 10: 0.01
                }
                
                # 3ä½é”æˆæ™‚ã®äºˆæƒ³ã‚¯ãƒªãƒƒã‚¯æ•°
                target_position = 3
                target_ctr = position_ctr_map.get(target_position, 0.10)
                predicted_clicks = current_impressions * target_ctr
                traffic_increase = predicted_clicks - current_clicks
                
                result.append({
                    'query': row['query'],
                    'page': row['page'],
                    'current_position': round(current_position, 1),
                    'current_clicks': int(current_clicks),
                    'current_impressions': int(current_impressions),
                    'current_ctr': round(row['ctr'] * 100, 2),
                    'target_position': target_position,
                    'predicted_traffic_increase': int(traffic_increase),
                    'priority_score': int(current_impressions * (10 - current_position)),  # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢
                    'improvement_potential': 'high' if traffic_increase > 1000 else 'medium' if traffic_increase > 500 else 'low'
                })
            
            logger.info(f"{site_name}: é †ä½æ”¹å–„æ©Ÿä¼š{len(result)}ä»¶ã‚’ç™ºè¦‹")
            return result
            
        except Exception as e:
            logger.error(f"{site_name}: é †ä½æ”¹å–„æ©Ÿä¼šåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def find_ctr_improvement_opportunities(self, gsc_data: pd.DataFrame, site_name: str) -> List[Dict]:
        """é«˜ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ»ä½CTRã‚¯ã‚¨ãƒªã®ç‰¹å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«/ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ”¹å–„å€™è£œï¼‰"""
        try:
            if gsc_data.empty:
                return []
            
            # å¹³å‡CTRã‚’è¨ˆç®—
            avg_ctr = gsc_data['ctr'].mean()
            
            # é«˜ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ãƒ»ä½CTRã‚¯ã‚¨ãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            opportunities = gsc_data[
                (gsc_data['impressions'] >= 1000) &  # é«˜ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³
                (gsc_data['ctr'] < avg_ctr * 0.7) &  # å¹³å‡CTRã®70%æœªæº€
                (gsc_data['position'] <= 10)  # ä¸Šä½10ä½ä»¥å†…
            ].copy()
            
            if opportunities.empty:
                logger.warning(f"{site_name}: CTRæ”¹å–„æ©Ÿä¼šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
            
            # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆ
            opportunities = opportunities.sort_values('impressions', ascending=False)
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            result = []
            for _, row in opportunities.head(30).iterrows():  # TOP30
                current_ctr = row['ctr']
                current_impressions = row['impressions']
                current_clicks = row['clicks']
                
                # CTRæ”¹å–„æ™‚ã®äºˆæƒ³ã‚¯ãƒªãƒƒã‚¯å¢—åŠ 
                target_ctr = avg_ctr * 1.2  # å¹³å‡CTRã®120%ã‚’ç›®æ¨™
                predicted_clicks = current_impressions * target_ctr
                click_increase = predicted_clicks - current_clicks
                
                result.append({
                    'query': row['query'],
                    'page': row['page'],
                    'current_ctr': round(current_ctr * 100, 2),
                    'current_clicks': int(current_clicks),
                    'current_impressions': int(current_impressions),
                    'current_position': round(row['position'], 1),
                    'target_ctr': round(target_ctr * 100, 2),
                    'predicted_click_increase': int(click_increase),
                    'improvement_potential': 'high' if click_increase > 500 else 'medium' if click_increase > 200 else 'low',
                    'suggested_actions': self._generate_ctr_improvement_suggestions(row['query'], row['position'])
                })
            
            logger.info(f"{site_name}: CTRæ”¹å–„æ©Ÿä¼š{len(result)}ä»¶ã‚’ç™ºè¦‹")
            return result
            
        except Exception as e:
            logger.error(f"{site_name}: CTRæ”¹å–„æ©Ÿä¼šåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _generate_ctr_improvement_suggestions(self, query: str, position: float) -> List[str]:
        """CTRæ”¹å–„ã®ãŸã‚ã®å…·ä½“çš„ãªææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        
        if position <= 3:
            suggestions.append("ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã«å¹´å·ã‚„æœ€æ–°æ€§ã‚’è¿½åŠ ")
            suggestions.append("ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã«ä¾¡æ ¼å¸¯ã‚„ç‰¹å¾´ã‚’æ˜è¨˜")
        elif position <= 6:
            suggestions.append("ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã®æœ€é©åŒ–")
            suggestions.append("ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®æ”¹å–„")
            suggestions.append("æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ")
        else:
            suggestions.append("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å……å®Ÿ")
            suggestions.append("å†…éƒ¨ãƒªãƒ³ã‚¯ã®å¼·åŒ–")
            suggestions.append("ãƒšãƒ¼ã‚¸é€Ÿåº¦ã®æœ€é©åŒ–")
        
        # ã‚¯ã‚¨ãƒªã«å¿œã˜ãŸå…·ä½“çš„ãªææ¡ˆ
        if "ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ" in query:
            suggestions.append("ã‚®ãƒ•ãƒˆæ„Ÿã‚’æ¼”å‡ºã™ã‚‹ç”»åƒã®è¿½åŠ ")
        if "èª•ç”Ÿæ—¥" in query or "ã‚¯ãƒªã‚¹ãƒã‚¹" in query:
            suggestions.append("å­£ç¯€æ€§ã‚’å¼·èª¿ã—ãŸã‚¿ã‚¤ãƒˆãƒ«")
        if "ç”·æ€§" in query or "å¥³æ€§" in query:
            suggestions.append("æ€§åˆ¥ã«ç‰¹åŒ–ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„")
        
        return suggestions[:3]  # ä¸Šä½3ã¤ã®ææ¡ˆ
    
    def analyze_seasonal_trends(self, current_data: pd.DataFrame, previous_data: pd.DataFrame, site_name: str) -> Dict:
        """å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆå‰å¹´åŒæœŸæ¯”ã§ã®æ©Ÿä¼šç™ºè¦‹ï¼‰"""
        try:
            if current_data.empty or previous_data.empty:
                return {
                    'growing_queries': [],
                    'declining_queries': [],
                    'new_queries': [],
                    'trend_analysis': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚åˆ†æä¸å¯'
                }
            
            # ã‚¯ã‚¨ãƒªåˆ¥ã®é›†è¨ˆ
            current_queries = current_data.groupby('query').agg({
                'clicks': 'sum',
                'impressions': 'sum',
                'ctr': 'mean',
                'position': 'mean'
            }).reset_index()
            
            previous_queries = previous_data.groupby('query').agg({
                'clicks': 'sum',
                'impressions': 'sum',
                'ctr': 'mean',
                'position': 'mean'
            }).reset_index()
            
            # å‰å¹´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸ã‚’ä½œæˆ
            previous_dict = {row['query']: row for _, row in previous_queries.iterrows()}
            
            growing_queries = []
            declining_queries = []
            new_queries = []
            
            for _, current_row in current_queries.iterrows():
                query = current_row['query']
                previous_row = previous_dict.get(query)
                
                if previous_row:
                    # æˆé•·ç‡ã‚’è¨ˆç®—
                    clicks_growth = ((current_row['clicks'] - previous_row['clicks']) / previous_row['clicks'] * 100) if previous_row['clicks'] > 0 else 0
                    impressions_growth = ((current_row['impressions'] - previous_row['impressions']) / previous_row['impressions'] * 100) if previous_row['impressions'] > 0 else 0
                    
                    if clicks_growth > 50 and current_row['clicks'] > 100:  # 50%ä»¥ä¸Šæˆé•·ã‹ã¤100ã‚¯ãƒªãƒƒã‚¯ä»¥ä¸Š
                        growing_queries.append({
                            'query': query,
                            'current_clicks': int(current_row['clicks']),
                            'previous_clicks': int(previous_row['clicks']),
                            'clicks_growth': round(clicks_growth, 1),
                            'current_impressions': int(current_row['impressions']),
                            'impressions_growth': round(impressions_growth, 1),
                            'current_position': round(current_row['position'], 1),
                            'previous_position': round(previous_row['position'], 1)
                        })
                    elif clicks_growth < -30 and previous_row['clicks'] > 100:  # 30%ä»¥ä¸Šæ¸›å°‘
                        declining_queries.append({
                            'query': query,
                            'current_clicks': int(current_row['clicks']),
                            'previous_clicks': int(previous_row['clicks']),
                            'clicks_growth': round(clicks_growth, 1),
                            'reason': 'é †ä½ä½ä¸‹' if current_row['position'] > previous_row['position'] else 'éœ€è¦æ¸›å°‘'
                        })
                else:
                    # æ–°è¦ã‚¯ã‚¨ãƒª
                    if current_row['clicks'] > 50:  # 50ã‚¯ãƒªãƒƒã‚¯ä»¥ä¸Šã®æ–°è¦ã‚¯ã‚¨ãƒª
                        new_queries.append({
                            'query': query,
                            'clicks': int(current_row['clicks']),
                            'impressions': int(current_row['impressions']),
                            'position': round(current_row['position'], 1),
                            'ctr': round(current_row['ctr'] * 100, 2)
                        })
            
            # æˆé•·ã‚¯ã‚¨ãƒªã‚’ã‚¯ãƒªãƒƒã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆ
            growing_queries.sort(key=lambda x: x['current_clicks'], reverse=True)
            declining_queries.sort(key=lambda x: x['clicks_growth'])
            new_queries.sort(key=lambda x: x['clicks'], reverse=True)
            
            logger.info(f"{site_name}: æˆé•·ã‚¯ã‚¨ãƒª{len(growing_queries)}ä»¶ã€æ¸›å°‘ã‚¯ã‚¨ãƒª{len(declining_queries)}ä»¶ã€æ–°è¦ã‚¯ã‚¨ãƒª{len(new_queries)}ä»¶ã‚’ç™ºè¦‹")
            
            return {
                'growing_queries': growing_queries[:20],  # TOP20
                'declining_queries': declining_queries[:20],  # TOP20
                'new_queries': new_queries[:20],  # TOP20
                'trend_analysis': f"æˆé•·ã‚¯ã‚¨ãƒª{len(growing_queries)}ä»¶ã€æ¸›å°‘ã‚¯ã‚¨ãƒª{len(declining_queries)}ä»¶ã€æ–°è¦ã‚¯ã‚¨ãƒª{len(new_queries)}ä»¶"
            }
            
        except Exception as e:
            logger.error(f"{site_name}: å­£ç¯€æ€§ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'growing_queries': [],
                'declining_queries': [],
                'new_queries': [],
                'trend_analysis': f'åˆ†æã‚¨ãƒ©ãƒ¼: {e}'
            }
    
    def compare_sites_performance(self, moodmark_data: pd.DataFrame, moodmarkgift_data: pd.DataFrame) -> Dict:
        """ä¸¡ã‚µã‚¤ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
        try:
            comparison = {
                'moodmark': self._calculate_site_metrics(moodmark_data, 'moodmark'),
                'moodmarkgift': self._calculate_site_metrics(moodmarkgift_data, 'moodmarkgift'),
                'comparison_insights': []
            }
            
            # æ¯”è¼ƒã‚¤ãƒ³ã‚µã‚¤ãƒˆã®ç”Ÿæˆ
            moodmark_metrics = comparison['moodmark']
            moodmarkgift_metrics = comparison['moodmarkgift']
            
            if moodmark_metrics['total_clicks'] > moodmarkgift_metrics['total_clicks']:
                comparison['comparison_insights'].append({
                    'insight': 'moodmarkãŒmoodmarkgiftã‚ˆã‚Šå¤šãã®ã‚¯ãƒªãƒƒã‚¯ã‚’ç²å¾—',
                    'difference': f"{moodmark_metrics['total_clicks'] - moodmarkgift_metrics['total_clicks']:,}ã‚¯ãƒªãƒƒã‚¯",
                    'recommendation': 'moodmarkgiftã®SEOå¼·åŒ–ã‚’æ¤œè¨'
                })
            
            if moodmarkgift_metrics['avg_ctr'] > moodmark_metrics['avg_ctr']:
                comparison['comparison_insights'].append({
                    'insight': 'moodmarkgiftã®CTRãŒmoodmarkã‚ˆã‚Šé«˜ã„',
                    'difference': f"{moodmarkgift_metrics['avg_ctr'] - moodmark_metrics['avg_ctr']:.2f}%",
                    'recommendation': 'moodmarkgiftã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’moodmarkã«é©ç”¨'
                })
            
            return comparison
            
        except Exception as e:
            logger.error(f"ã‚µã‚¤ãƒˆæ¯”è¼ƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _calculate_site_metrics(self, data: pd.DataFrame, site_name: str) -> Dict:
        """ã‚µã‚¤ãƒˆåˆ¥ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        if data.empty:
            return {
                'total_clicks': 0,
                'total_impressions': 0,
                'avg_ctr': 0,
                'avg_position': 0,
                'top_queries_count': 0
            }
        
        return {
            'total_clicks': int(data['clicks'].sum()),
            'total_impressions': int(data['impressions'].sum()),
            'avg_ctr': round(data['ctr'].mean() * 100, 2),
            'avg_position': round(data['position'].mean(), 1),
            'top_queries_count': len(data[data['clicks'] > 10])
        }
    
    def generate_seo_opportunity_report(self, start_date: str, end_date: str, previous_start_date: str = None, previous_end_date: str = None):
        """SEOæ©Ÿä¼šç™ºè¦‹ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            logger.info("SEOæ©Ÿä¼šç™ºè¦‹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
            
            # ã‚µã‚¤ãƒˆè¨­å®šã®å–å¾—
            sites = self.config.get('sites', {})
            if not sites:
                logger.error("ã‚µã‚¤ãƒˆè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
            
            report = {
                'report_metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'current_period': f"{start_date} - {end_date}",
                    'previous_period': f"{previous_start_date} - {previous_end_date}" if previous_start_date else None,
                    'analysis_type': 'seo_opportunity_finder'
                },
                'sites': {}
            }
            
            # å„ã‚µã‚¤ãƒˆã®åˆ†æ
            for site_key, site_config in sites.items():
                site_url = site_config.get('gsc_site_url')
                if not site_url:
                    continue
                
                logger.info(f"{site_key}ã®SEOæ©Ÿä¼šåˆ†æé–‹å§‹")
                
                # ç¾åœ¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
                current_data = self.get_gsc_data_for_period(start_date, end_date, site_url)
                
                # å‰å¹´åŒæœŸã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                previous_data = pd.DataFrame()
                if previous_start_date and previous_end_date:
                    previous_data = self.get_gsc_data_for_period(previous_start_date, previous_end_date, site_url)
                
                # å„ç¨®åˆ†æã®å®Ÿè¡Œ
                ranking_opportunities = self.find_ranking_improvement_opportunities(current_data, site_key)
                ctr_opportunities = self.find_ctr_improvement_opportunities(current_data, site_key)
                seasonal_trends = self.analyze_seasonal_trends(current_data, previous_data, site_key)
                
                site_report = {
                    'site_url': site_url,
                    'current_period_data': self._calculate_site_metrics(current_data, site_key),
                    'ranking_improvement_opportunities': ranking_opportunities,
                    'ctr_improvement_opportunities': ctr_opportunities,
                    'seasonal_trends': seasonal_trends,
                    'quick_wins': self._identify_quick_wins(ranking_opportunities, ctr_opportunities)
                }
                
                report['sites'][site_key] = site_report
                
                # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                if not current_data.empty:
                    filename = f'gsc_{site_key}_seo_analysis_{start_date.replace("-", "")}_{end_date.replace("-", "")}.csv'
                    self.api_integration.export_to_csv(current_data, filename)
            
            # ã‚µã‚¤ãƒˆé–“æ¯”è¼ƒ
            if len(report['sites']) >= 2:
                moodmark_data = self.get_gsc_data_for_period(start_date, end_date, sites['moodmark']['gsc_site_url'])
                moodmarkgift_data = self.get_gsc_data_for_period(start_date, end_date, sites['moodmark_idea']['gsc_site_url'])
                report['site_comparison'] = self.compare_sites_performance(moodmark_data, moodmarkgift_data)
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = f'data/processed/seo_opportunities_{start_date.replace("-", "")}_{end_date.replace("-", "")}.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._generate_markdown_report(report, start_date, end_date)
            
            logger.info(f"SEOæ©Ÿä¼šç™ºè¦‹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
            
        except Exception as e:
            logger.error(f"SEOæ©Ÿä¼šç™ºè¦‹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _identify_quick_wins(self, ranking_opportunities: List[Dict], ctr_opportunities: List[Dict]) -> List[Dict]:
        """å³åŠ¹æ€§ã®é«˜ã„æ–½ç­–ã‚’ç‰¹å®š"""
        quick_wins = []
        
        # é †ä½æ”¹å–„æ©Ÿä¼šã‹ã‚‰å³åŠ¹æ€§ã®é«˜ã„ã‚‚ã®ã‚’æŠ½å‡º
        for opp in ranking_opportunities[:10]:
            if opp['improvement_potential'] == 'high' and opp['predicted_traffic_increase'] > 1000:
                quick_wins.append({
                    'type': 'ranking_improvement',
                    'query': opp['query'],
                    'action': f"é †ä½{opp['current_position']}ä½â†’{opp['target_position']}ä½ã¸ã®æ”¹å–„",
                    'expected_impact': f"+{opp['predicted_traffic_increase']}ã‚¯ãƒªãƒƒã‚¯/æœˆ",
                    'effort': 'medium',
                    'priority': 'high'
                })
        
        # CTRæ”¹å–„æ©Ÿä¼šã‹ã‚‰å³åŠ¹æ€§ã®é«˜ã„ã‚‚ã®ã‚’æŠ½å‡º
        for opp in ctr_opportunities[:5]:
            if opp['improvement_potential'] == 'high' and opp['predicted_click_increase'] > 500:
                quick_wins.append({
                    'type': 'ctr_improvement',
                    'query': opp['query'],
                    'action': f"CTR{opp['current_ctr']}%â†’{opp['target_ctr']}%ã¸ã®æ”¹å–„",
                    'expected_impact': f"+{opp['predicted_click_increase']}ã‚¯ãƒªãƒƒã‚¯/æœˆ",
                    'effort': 'low',
                    'priority': 'high'
                })
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        quick_wins.sort(key=lambda x: (x['priority'] == 'high', x['expected_impact']), reverse=True)
        
        return quick_wins[:10]  # TOP10
    
    def _generate_markdown_report(self, report: Dict, start_date: str, end_date: str):
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            markdown = f"""# ğŸ” SEOæ©Ÿä¼šç™ºè¦‹ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
**åˆ†ææœŸé–“**: {start_date} - {end_date}
**åˆ†ææ–¹å¼**: GSC APIé€£æºã«ã‚ˆã‚‹åŒ…æ‹¬çš„SEOåˆ†æ

## ğŸ“Š åˆ†ææ¦‚è¦

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€Google Search Consoleã®ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦SEOæ”¹å–„æ©Ÿä¼šã‚’ç‰¹å®šã—ãŸã‚‚ã®ã§ã™ã€‚

### åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆ
"""
            
            for site_key, site_data in report['sites'].items():
                site_url = site_data.get('site_url', '')
                metrics = site_data.get('current_period_data', {})
                markdown += f"- **{site_key}**: {site_url}\n"
                markdown += f"  - ç·ã‚¯ãƒªãƒƒã‚¯æ•°: {metrics.get('total_clicks', 0):,}\n"
                markdown += f"  - ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {metrics.get('total_impressions', 0):,}\n"
                markdown += f"  - å¹³å‡CTR: {metrics.get('avg_ctr', 0):.2f}%\n"
                markdown += f"  - å¹³å‡æ¤œç´¢é †ä½: {metrics.get('avg_position', 0):.1f}ä½\n\n"
            
            # å„ã‚µã‚¤ãƒˆã®è©³ç´°åˆ†æ
            for site_key, site_data in report['sites'].items():
                markdown += f"## ğŸŒ {site_key.upper()}\n\n"
                
                # é †ä½æ”¹å–„æ©Ÿä¼š
                ranking_opps = site_data.get('ranking_improvement_opportunities', [])
                if ranking_opps:
                    markdown += "### ğŸ“ˆ é †ä½æ”¹å–„æ©Ÿä¼šï¼ˆTOP10ï¼‰\n\n"
                    markdown += "| é †ä½ | ã‚¯ã‚¨ãƒª | ç¾åœ¨é †ä½ | ç¾åœ¨ã‚¯ãƒªãƒƒã‚¯æ•° | äºˆæƒ³ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å¢—åŠ  | å„ªå…ˆåº¦ |\n"
                    markdown += "|------|--------|----------|----------------|---------------------|--------|\n"
                    
                    for i, opp in enumerate(ranking_opps[:10], 1):
                        priority_emoji = "ğŸ”´" if opp['improvement_potential'] == 'high' else "ğŸŸ¡" if opp['improvement_potential'] == 'medium' else "ğŸŸ¢"
                        markdown += f"| {i} | {opp['query']} | {opp['current_position']}ä½ | {opp['current_clicks']:,} | +{opp['predicted_traffic_increase']:,} | {priority_emoji} |\n"
                    markdown += "\n"
                
                # CTRæ”¹å–„æ©Ÿä¼š
                ctr_opps = site_data.get('ctr_improvement_opportunities', [])
                if ctr_opps:
                    markdown += "### ğŸ¯ CTRæ”¹å–„æ©Ÿä¼šï¼ˆTOP10ï¼‰\n\n"
                    markdown += "| é †ä½ | ã‚¯ã‚¨ãƒª | ç¾åœ¨CTR | ç›®æ¨™CTR | äºˆæƒ³ã‚¯ãƒªãƒƒã‚¯å¢—åŠ  | æ¨å¥¨æ–½ç­– |\n"
                    markdown += "|------|--------|---------|---------|------------------|----------|\n"
                    
                    for i, opp in enumerate(ctr_opps[:10], 1):
                        suggestions = ", ".join(opp.get('suggested_actions', [])[:2])
                        markdown += f"| {i} | {opp['query']} | {opp['current_ctr']}% | {opp['target_ctr']}% | +{opp['predicted_click_increase']:,} | {suggestions} |\n"
                    markdown += "\n"
                
                # å­£ç¯€æ€§ãƒˆãƒ¬ãƒ³ãƒ‰
                trends = site_data.get('seasonal_trends', {})
                if trends.get('growing_queries'):
                    markdown += "### ğŸ“Š æˆé•·ä¸­ã®ã‚¯ã‚¨ãƒªï¼ˆTOP10ï¼‰\n\n"
                    markdown += "| é †ä½ | ã‚¯ã‚¨ãƒª | ç¾åœ¨ã‚¯ãƒªãƒƒã‚¯æ•° | å‰å¹´åŒæœŸæ¯” | æˆé•·ç‡ |\n"
                    markdown += "|------|--------|----------------|------------|--------|\n"
                    
                    for i, query in enumerate(trends['growing_queries'][:10], 1):
                        markdown += f"| {i} | {query['query']} | {query['current_clicks']:,} | {query['previous_clicks']:,} | +{query['clicks_growth']}% |\n"
                    markdown += "\n"
                
                # ã‚¯ã‚¤ãƒƒã‚¯ã‚¦ã‚£ãƒ³
                quick_wins = site_data.get('quick_wins', [])
                if quick_wins:
                    markdown += "### âš¡ å³åŠ¹æ€§ã®é«˜ã„æ–½ç­–ï¼ˆTOP5ï¼‰\n\n"
                    for i, win in enumerate(quick_wins[:5], 1):
                        effort_emoji = "ğŸŸ¢" if win['effort'] == 'low' else "ğŸŸ¡" if win['effort'] == 'medium' else "ğŸ”´"
                        markdown += f"{i}. **{win['query']}**\n"
                        markdown += f"   - æ–½ç­–: {win['action']}\n"
                        markdown += f"   - æœŸå¾…åŠ¹æœ: {win['expected_impact']}\n"
                        markdown += f"   - å·¥æ•°: {effort_emoji} {win['effort']}\n\n"
                
                markdown += "---\n\n"
            
            # ã‚µã‚¤ãƒˆé–“æ¯”è¼ƒ
            if 'site_comparison' in report:
                comparison = report['site_comparison']
                markdown += "## ğŸ”„ ã‚µã‚¤ãƒˆé–“æ¯”è¼ƒ\n\n"
                
                if comparison.get('comparison_insights'):
                    markdown += "### ğŸ’¡ æ¯”è¼ƒã‚¤ãƒ³ã‚µã‚¤ãƒˆ\n\n"
                    for insight in comparison['comparison_insights']:
                        markdown += f"- **{insight['insight']}**: {insight['difference']}\n"
                        markdown += f"  - æ¨å¥¨: {insight['recommendation']}\n\n"
            
            markdown += """## ğŸ“‹ ã¾ã¨ã‚

### ä¸»è¦ãªç™ºè¦‹
- é †ä½æ”¹å–„æ©Ÿä¼šã®ç‰¹å®šã«ã‚ˆã‚Šã€å¤§å¹…ãªãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å¢—åŠ ãŒæœŸå¾…ã§ãã‚‹
- CTRæ”¹å–„ã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚ˆã‚ŠåŠ¹ç‡çš„ã«æ´»ç”¨å¯èƒ½
- å­£ç¯€æ€§ãƒˆãƒ¬ãƒ³ãƒ‰ã®æŠŠæ¡ã«ã‚ˆã‚Šã€å…ˆæ‰‹ã®æ–½ç­–å®Ÿæ–½ãŒå¯èƒ½

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. å³åŠ¹æ€§ã®é«˜ã„æ–½ç­–ã®å„ªå…ˆå®Ÿæ–½
2. é †ä½æ”¹å–„ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æœ€é©åŒ–
3. CTRæ”¹å–„ã®ãŸã‚ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–
4. å®šæœŸçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨æ”¹å–„

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯SEOæ©Ÿä¼šç™ºè¦‹ãƒ„ãƒ¼ãƒ«ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            markdown_file = f'data/processed/seo_opportunities_{start_date.replace("-", "")}_{end_date.replace("-", "")}.md'
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown)
            
            logger.info(f"Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {markdown_file}")
            
        except Exception as e:
            logger.error(f"Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SEOæ©Ÿä¼šç™ºè¦‹ãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--start-date', required=True, help='åˆ†æé–‹å§‹æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--end-date', required=True, help='åˆ†æçµ‚äº†æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--previous-start-date', help='å‰å¹´åŒæœŸé–‹å§‹æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--previous-end-date', help='å‰å¹´åŒæœŸçµ‚äº†æ—¥ (YYYY-MM-DD)')
    
    args = parser.parse_args()
    
    print("=== SEOæ©Ÿä¼šç™ºè¦‹ãƒ„ãƒ¼ãƒ« ===")
    print(f"åˆ†ææœŸé–“: {args.start_date} - {args.end_date}")
    if args.previous_start_date:
        print(f"å‰å¹´åŒæœŸé–“: {args.previous_start_date} - {args.previous_end_date}")
    print()
    
    finder = SEOOpportunityFinder()
    finder.generate_seo_opportunity_report(
        start_date=args.start_date,
        end_date=args.end_date,
        previous_start_date=args.previous_start_date,
        previous_end_date=args.previous_end_date
    )
    
    print("=== SEOæ©Ÿä¼šç™ºè¦‹åˆ†æå®Œäº† ===")
    print(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: data/processed/seo_opportunities_{args.start_date.replace('-', '')}_{args.end_date.replace('-', '')}.json")
    print(f"Markdownãƒ¬ãƒãƒ¼ãƒˆ: data/processed/seo_opportunities_{args.start_date.replace('-', '')}_{args.end_date.replace('-', '')}.md")

if __name__ == "__main__":
    main()







