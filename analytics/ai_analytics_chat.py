#!/usr/bin/env python3
"""
GA4/GSC AIåˆ†æãƒãƒ£ãƒƒãƒˆçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
OpenAI APIã‚’ä½¿ç”¨ã—ã¦GA4/GSCãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€è‡ªç„¶è¨€èªã§å›ç­”ã‚’ç”Ÿæˆ
"""

import os
import sys
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Any, Generator
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

from analytics.google_apis_integration import GoogleAPIsIntegration
from analytics.seo_analyzer import SEOAnalyzer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AIAnalyticsChat:
    """GA4/GSCãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹AIãƒãƒ£ãƒƒãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, credentials_file=None, openai_api_key=None, default_site_name='moodmark'):
        """
        åˆæœŸåŒ–
        
        Args:
            credentials_file (str): Googleèªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            openai_api_key (str): OpenAI APIã‚­ãƒ¼
            default_site_name (str): ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚µã‚¤ãƒˆå ('moodmark' ã¾ãŸã¯ 'moodmarkgift')
        """
        # OpenAI APIã‚­ãƒ¼ã®å–å¾—
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if OpenAI is None:
            raise ImportError("openaiãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install openai ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        self.client = OpenAI(api_key=self.openai_api_key)
        
        # Google APIsçµ±åˆã®åˆæœŸåŒ–
        self.google_apis = GoogleAPIsIntegration(credentials_file=credentials_file)
        
        # SEOåˆ†æã®åˆæœŸåŒ–
        self.seo_analyzer = SEOAnalyzer()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µã‚¤ãƒˆå
        self.default_site_name = default_site_name
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.system_prompt = """ã‚ãªãŸã¯Google Analytics 4 (GA4)ã€Google Search Console (GSC)ã€ãŠã‚ˆã³SEOåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€æä¾›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€ã‚ã‹ã‚Šã‚„ã™ãã€å®Ÿç”¨çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã®æ„å›³ã‚’ç†è§£ã—ã€é©åˆ‡ã«å›ç­”ã™ã‚‹ã€‘
- è³ªå•ã®ç¨®é¡ï¼ˆSEOæ”¹å–„ã€å•†å“ææ¡ˆã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€å¹´æ¬¡æ¯”è¼ƒãªã©ï¼‰ã‚’è‡ªå‹•çš„ã«åˆ¤æ–­ã—ã¦ãã ã•ã„
- è³ªå•ã®æ„å›³ã«å¿œã˜ã¦ã€é©åˆ‡ãªå›ç­”å½¢å¼ã¨å«ã‚ã‚‹ã¹ãæƒ…å ±ã‚’æŸ”è»Ÿã«é¸æŠã—ã¦ãã ã•ã„
- ä¾‹ï¼š
  * å•†å“ææ¡ˆã®è³ªå• â†’ å•†å“ã‚«ãƒ†ã‚´ãƒªã€å…·ä½“çš„ãªå•†å“åã€æ²è¼‰ç†ç”±ã‚’ææ¡ˆ
  * SEOæ”¹å–„ã®è³ªå• â†’ ç¾çŠ¶åˆ†æã€èª²é¡Œæ•´ç†ã€æ”¹å–„ææ¡ˆã®3æ®µéšæ§‹é€ 
  * ãƒ‡ãƒ¼ã‚¿åˆ†æã®è³ªå• â†’ æ•°å€¤ã®èª¬æ˜ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã€ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
  * å¹´æ¬¡æ¯”è¼ƒã®è³ªå• â†’ æ˜¨å¹´ã¨ä»Šå¹´ã®æ¯”è¼ƒã€å¢—æ¸›ç‡ã€åŸå› åˆ†æ

ã€ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨ã€‘
- æä¾›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼ˆGA4ã€GSCã€SEOåˆ†æï¼‰ã‚’æœ€å¤§é™ã«æ´»ç”¨ã—ã¦ãã ã•ã„
- ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ã®å†…å®¹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
- ãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ…‹ãŒã€Œâœ— å–å¾—å¤±æ•—ã€ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®ãƒ‡ãƒ¼ã‚¿ã¯åˆ©ç”¨ã§ããªã„ã“ã¨ã‚’ç†è§£ã—ã¦ãã ã•ã„

ã€å›ç­”ã®æ§‹é€ ã€‘
- è³ªå•ã®ç¨®é¡ã«å¿œã˜ã¦ã€é©åˆ‡ãªæ§‹é€ ã§å›ç­”ã—ã¦ãã ã•ã„
- æ•°å€¤ã¯å¿…ãšå…·ä½“çš„ã«ç¤ºã—ã¦ãã ã•ã„
- å°‚é–€ç”¨èªã‚’ä½¿ã†å ´åˆã¯ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„
- æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„

ã€SEOæ”¹å–„ã«é–¢ã™ã‚‹å›ç­”ã®æ§‹é€ ã€‘
SEOæ”¹å–„ã«é–¢ã™ã‚‹è³ªå•ã«ã¯ã€ä»¥ä¸‹ã®3æ®µéšã®æ§‹é€ ã§å›ç­”ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ï¼š

1. ã€ç¾çŠ¶åˆ†æã€‘
   - ç¾åœ¨ã®SEOè¦ç´ ã®çŠ¶æ…‹ã‚’æ•°å€¤ã¨å…±ã«æ˜ç¢ºã«ç¤ºã™
   - ã‚¿ã‚¤ãƒˆãƒ«ã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã€è¦‹å‡ºã—æ§‹é€ ã€ç”»åƒaltå±æ€§ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãªã©ã®ç¾çŠ¶ã‚’åˆ†æ
   - æœ€é©å€¤ã¨ã®æ¯”è¼ƒã‚’ç¤ºã™ï¼ˆä¾‹ï¼šã‚¿ã‚¤ãƒˆãƒ«ã¯45æ–‡å­—ã§ã€æœ€é©ç¯„å›²30-60æ–‡å­—å†…ï¼‰
   - H2/H3ã®è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å…·ä½“çš„ãªè¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ—æŒ™ã—ã€ãã®å†…å®¹ã‚’åˆ†æã—ã¦ãã ã•ã„
   - è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªè©•ä¾¡ï¼ˆé•·ã•ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡ã€é‡è¤‡ãªã©ï¼‰ã‚’å…·ä½“çš„ã«ç¤ºã—ã¦ãã ã•ã„

2. ã€èª²é¡Œæ•´ç†ã€‘
   - ç¾çŠ¶åˆ†æã‹ã‚‰è¦‹ã¤ã‹ã£ãŸå•é¡Œç‚¹ã‚’æ•´ç†
   - å„ªå…ˆåº¦ã®é«˜ã„èª²é¡Œã‹ã‚‰é †ã«åˆ—æŒ™
   - å„èª²é¡ŒãŒSEOã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜

3. ã€æ”¹å–„ææ¡ˆã€‘
   - å„èª²é¡Œã«å¯¾ã™ã‚‹å…·ä½“çš„ãªæ”¹å–„æ–¹æ³•ã‚’æç¤º
   - å®Ÿè£…å¯èƒ½ãªå…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’æç¤ºï¼ˆä¾‹ï¼šã‚¿ã‚¤ãƒˆãƒ«ã®æ”¹å–„æ¡ˆã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®æ”¹å–„æ¡ˆï¼‰
   - è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®æ”¹å–„ææ¡ˆã§ã¯ã€å…·ä½“çš„ãªè¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„
   - å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦æ”¹å–„ã™ã¹ãé †åºã‚’æç¤º

ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„SEOã«ãŠã‘ã‚‹è¦‹å‡ºã—æ§‹é€ ã®é‡è¦æ€§ã€‘
- è¦‹å‡ºã—æ§‹é€ ï¼ˆH1ã€H2ã€H3ï¼‰ã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„SEOã«ãŠã„ã¦æ¥µã‚ã¦é‡è¦ã§ã™
- è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãŒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ ã¨å†…å®¹ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®é‡è¦ãªæ‰‹ãŒã‹ã‚Šã§ã™
- H2/H3ã®è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã¯ã€å…·ä½“çš„ã«åˆ—æŒ™ã—ã€ãã®å“è³ªã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡ã€é‡è¤‡ãªã©ã®å•é¡Œã‚’å…·ä½“çš„ã«æŒ‡æ‘˜ã—ã€æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„"""
    
    def _extract_date_range(self, question: str) -> tuple:
        """
        è³ªå•ã‹ã‚‰æ—¥ä»˜ç¯„å›²ã‚’æŠ½å‡º
        
        Args:
            question (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            
        Returns:
            tuple: (start_date, end_date, date_range_days) ã¾ãŸã¯ (None, None, date_range_days)
                   start_date, end_dateã¯ 'YYYY-MM-DD' å½¢å¼ã€ç‰¹å®šã®æœˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è¨­å®šã•ã‚Œã‚‹
        """
        import re
        from datetime import datetime, timedelta
        question_lower = question.lower()
        
        # ç‰¹å®šã®å¹´æœˆã‚’æŠ½å‡ºï¼ˆä¾‹: "2025å¹´10æœˆ"ï¼‰
        year_month_match = re.search(r'(\d{4})å¹´\s*(\d{1,2})æœˆ', question)
        if year_month_match:
            year = int(year_month_match.group(1))
            month = int(year_month_match.group(2))
            
            # ãã®æœˆã®æœ€åˆã®æ—¥ã¨æœ€å¾Œã®æ—¥ã‚’è¨ˆç®—
            if month == 12:
                next_month = datetime(year + 1, 1, 1)
            else:
                next_month = datetime(year, month + 1, 1)
            
            start_date = datetime(year, month, 1)
            end_date = next_month - timedelta(days=1)
            
            date_range_days = (end_date - start_date).days + 1
            
            logger.info(f"ç‰¹å®šã®æœˆã‚’æ¤œå‡º: {year}å¹´{month}æœˆ ({start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')})")
            return (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), date_range_days)
        
        # ç‰¹å®šã®æ—¥ä»˜ç¯„å›²ã‚’æŠ½å‡ºï¼ˆä¾‹: "11/1-11/7", "11æœˆ1æ—¥-11æœˆ7æ—¥"ï¼‰
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "11/1-11/7" å½¢å¼
        date_range_match = re.search(r'(\d{1,2})/(\d{1,2})\s*[-~ï½]\s*(\d{1,2})/(\d{1,2})', question)
        if date_range_match:
            start_month = int(date_range_match.group(1))
            start_day = int(date_range_match.group(2))
            end_month = int(date_range_match.group(3))
            end_day = int(date_range_match.group(4))
            current_year = datetime.now().year
            
            try:
                start_date = datetime(current_year, start_month, start_day)
                end_date = datetime(current_year, end_month, end_day)
                
                if end_date < start_date:
                    # å¹´ã‚’ã¾ãŸãå ´åˆï¼ˆä¾‹: 12/25-1/5ï¼‰
                    end_date = datetime(current_year + 1, end_month, end_day)
                
                date_range_days = (end_date - start_date).days + 1
                
                logger.info(f"ç‰¹å®šã®æ—¥ä»˜ç¯„å›²ã‚’æ¤œå‡º: {start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')}")
                return (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), date_range_days)
            except ValueError as e:
                logger.warning(f"æ—¥ä»˜ç¯„å›²ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: "11æœˆ1æ—¥-11æœˆ7æ—¥" å½¢å¼
        date_range_match2 = re.search(r'(\d{1,2})æœˆ\s*(\d{1,2})æ—¥\s*[-~ï½]\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥', question)
        if date_range_match2:
            start_month = int(date_range_match2.group(1))
            start_day = int(date_range_match2.group(2))
            end_month = int(date_range_match2.group(3))
            end_day = int(date_range_match2.group(4))
            current_year = datetime.now().year
            
            try:
                start_date = datetime(current_year, start_month, start_day)
                end_date = datetime(current_year, end_month, end_day)
                
                if end_date < start_date:
                    end_date = datetime(current_year + 1, end_month, end_day)
                
                date_range_days = (end_date - start_date).days + 1
                
                logger.info(f"ç‰¹å®šã®æ—¥ä»˜ç¯„å›²ã‚’æ¤œå‡ºï¼ˆæ—¥æœ¬èªå½¢å¼ï¼‰: {start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')}")
                return (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), date_range_days)
            except ValueError as e:
                logger.warning(f"æ—¥ä»˜ç¯„å›²ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        # ç‰¹å®šã®æœˆã‚’æŠ½å‡ºï¼ˆä¾‹: "10æœˆ" - ä»Šå¹´ã‚’ä»®å®šï¼‰
        month_match = re.search(r'(\d{1,2})æœˆ', question_lower)
        if month_match and 'å¹´' not in question_lower:
            month = int(month_match.group(1))
            current_year = datetime.now().year
            
            if month == 12:
                next_month = datetime(current_year + 1, 1, 1)
            else:
                next_month = datetime(current_year, month + 1, 1)
            
            start_date = datetime(current_year, month, 1)
            end_date = next_month - timedelta(days=1)
            
            date_range_days = (end_date - start_date).days + 1
            
            logger.info(f"ç‰¹å®šã®æœˆã‚’æ¤œå‡ºï¼ˆä»Šå¹´ï¼‰: {current_year}å¹´{month}æœˆ ({start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')})")
            return (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), date_range_days)
        
        # ç›¸å¯¾çš„ãªæ—¥ä»˜ç¯„å›²
        if 'ä»Šæ—¥' in question_lower or 'æœ¬æ—¥' in question_lower:
            return (None, None, 1)
        elif 'æ˜¨æ—¥' in question_lower:
            yesterday = datetime.now() - timedelta(days=1)
            return (yesterday.strftime('%Y-%m-%d'), yesterday.strftime('%Y-%m-%d'), 1)
        elif 'ä»Šé€±' in question_lower or 'ã“ã®é€±' in question_lower:
            return (None, None, 7)
        elif 'å…ˆé€±' in question_lower or 'å‰é€±' in question_lower:
            return (None, None, 7)
        elif 'ä»Šæœˆ' in question_lower:
            # ä»Šæœˆã®æœ€åˆã®æ—¥ã‹ã‚‰ä»Šæ—¥ã¾ã§
            today = datetime.now()
            start_of_month = datetime(today.year, today.month, 1)
            days = (today - start_of_month).days + 1
            return (start_of_month.strftime('%Y-%m-%d'), today.strftime('%Y-%m-%d'), days)
        elif 'å…ˆæœˆ' in question_lower or 'å‰æœˆ' in question_lower:
            # å…ˆæœˆã®æœ€åˆã®æ—¥ã‹ã‚‰æœ€å¾Œã®æ—¥ã¾ã§
            today = datetime.now()
            if today.month == 1:
                last_month = datetime(today.year - 1, 12, 1)
            else:
                last_month = datetime(today.year, today.month - 1, 1)
            
            if last_month.month == 12:
                next_month = datetime(last_month.year + 1, 1, 1)
            else:
                next_month = datetime(last_month.year, last_month.month + 1, 1)
            
            end_of_last_month = next_month - timedelta(days=1)
            days = (end_of_last_month - last_month).days + 1
            
            return (last_month.strftime('%Y-%m-%d'), end_of_last_month.strftime('%Y-%m-%d'), days)
        elif 'éå»' in question_lower:
            # "éå»7æ—¥"ã®ã‚ˆã†ãªè¡¨ç¾ã‚’æ¢ã™
            match = re.search(r'éå»(\d+)æ—¥', question_lower)
            if match:
                return (None, None, int(match.group(1)))
        elif 'æœ€è¿‘' in question_lower:
            return (None, None, 7)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯30æ—¥
        return (None, None, 30)
    
    def _get_ga4_summary(self, date_range_days: int, start_date: str = None, end_date: str = None, page_url: str = None) -> Dict[str, Any]:
        """
        GA4ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Args:
            date_range_days (int): æ—¥æ•°
            start_date (str): é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            end_date (str): çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            page_url (str): ãƒšãƒ¼ã‚¸URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€æŒ‡å®šã•ã‚ŒãŸå ´åˆã¯å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼‰
            
        Returns:
            dict: ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿
        """
        try:
            # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹å ´åˆ
            if page_url:
                logger.info(f"å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®GA4ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: URL={page_url}, æœŸé–“={date_range_days}æ—¥" + (f" ({start_date} ï½ {end_date})" if start_date and end_date else ""))
                page_data = self.google_apis.get_page_specific_ga4_data(
                    page_url=page_url,
                    date_range_days=date_range_days,
                    start_date=start_date,
                    end_date=end_date
                )
                
                if 'error' in page_data:
                    error_msg = page_data.get('error', 'Unknown error')
                    is_timeout = page_data.get('timeout', False)
                    logger.warning(f"å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {error_msg}")
                    
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®å ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚ˆã‚Šè©³ç´°ã«
                    if is_timeout:
                        return {
                            "error": error_msg,
                            "total_sessions": 0,
                            "total_users": 0,
                            "total_pageviews": 0,
                            "avg_bounce_rate": 0.0,
                            "avg_session_duration": 0.0,
                            "is_page_specific": True,
                            "timeout": True
                        }
                    
                    return {
                        "error": error_msg,
                        "total_sessions": 0,
                        "total_users": 0,
                        "total_pageviews": 0,
                        "avg_bounce_rate": 0.0,
                        "avg_session_duration": 0.0,
                        "is_page_specific": True
                    }
                
                # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                summary = {
                    "total_sessions": page_data.get('sessions', 0),
                    "total_users": page_data.get('users', 0),
                    "total_pageviews": page_data.get('pageviews', 0),
                    "avg_bounce_rate": page_data.get('bounce_rate', 0.0),
                    "avg_session_duration": page_data.get('avg_session_duration', 0.0),
                    "date_range_days": date_range_days,
                    "is_page_specific": True,
                    "page_url": page_url,
                    "page_path": page_data.get('page_path', '')
                }
                
                logger.info(f"å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®GA4ã‚µãƒãƒªãƒ¼: ã‚»ãƒƒã‚·ãƒ§ãƒ³={summary['total_sessions']:,}, ãƒ¦ãƒ¼ã‚¶ãƒ¼={summary['total_users']:,}, PV={summary['total_pageviews']:,}")
                return summary
            
            # ã‚µã‚¤ãƒˆå…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹å ´åˆ
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: æœŸé–“={date_range_days}æ—¥" + (f" ({start_date} ï½ {end_date})" if start_date and end_date else ""))
            
            # åŸºæœ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
            # GA4 APIã§ã¯ 'users' ã§ã¯ãªã 'activeUsers'ã€'pageviews' ã§ã¯ãªã 'screenPageViews' ã‚’ä½¿ç”¨
            if start_date and end_date:
                ga4_data = self.google_apis.get_ga4_data_custom_range(
                    start_date=start_date,
                    end_date=end_date,
                    metrics=['sessions', 'activeUsers', 'screenPageViews', 'bounceRate', 'averageSessionDuration'],
                    dimensions=['date']
                )
            else:
                ga4_data = self.google_apis.get_ga4_data(
                    date_range_days=date_range_days,
                    metrics=['sessions', 'activeUsers', 'screenPageViews', 'bounceRate', 'averageSessionDuration'],
                    dimensions=['date']
                )
            
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(ga4_data)}è¡Œ")
            
            if ga4_data.empty:
                logger.warning("GA4ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚èªè¨¼çŠ¶æ…‹ã¨APIæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                logger.warning(f"  GA4ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ID: {self.google_apis.ga4_property_id}")
                logger.warning(f"  GA4ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–: {self.google_apis.ga4_service is not None}")
                
                # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                error_msg = "GA4ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                if not self.google_apis.ga4_property_id:
                    error_msg += " GA4_PROPERTY_IDç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                elif not self.google_apis.ga4_service:
                    error_msg += " GA4ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚èªè¨¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                else:
                    error_msg += " ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«GA4ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                    error_msg += " GA4ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«é–²è¦§è€…ä»¥ä¸Šã®æ¨©é™ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚"
                
                return {
                    "error": error_msg,
                    "total_sessions": 0,
                    "total_users": 0,
                    "total_pageviews": 0,
                    "avg_bounce_rate": 0.0,
                    "avg_session_duration": 0.0,
                    "is_page_specific": False
                }
            
            summary = {
                "total_sessions": int(ga4_data['sessions'].sum()) if 'sessions' in ga4_data.columns else 0,
                "total_users": int(ga4_data['activeUsers'].sum()) if 'activeUsers' in ga4_data.columns else 0,  # GA4 APIã§ã¯ 'activeUsers' ã‚’ä½¿ç”¨
                "total_pageviews": int(ga4_data['screenPageViews'].sum()) if 'screenPageViews' in ga4_data.columns else 0,  # GA4 APIã§ã¯ 'screenPageViews' ã‚’ä½¿ç”¨
                "avg_bounce_rate": float(ga4_data['bounceRate'].mean()) if 'bounceRate' in ga4_data.columns else 0,
                "avg_session_duration": float(ga4_data['averageSessionDuration'].mean()) if 'averageSessionDuration' in ga4_data.columns else 0,
                "date_range_days": date_range_days,
                "data_points": len(ga4_data),
                "is_page_specific": False
            }
            
            logger.info(f"GA4ã‚µãƒãƒªãƒ¼: ã‚»ãƒƒã‚·ãƒ§ãƒ³={summary['total_sessions']:,}, ãƒ¦ãƒ¼ã‚¶ãƒ¼={summary['total_users']:,}, PV={summary['total_pageviews']:,}")
            
            return summary
            
        except Exception as e:
            logger.error(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"}
    
    def _get_gsc_summary(self, date_range_days: int, start_date: str = None, end_date: str = None, site_name: str = 'moodmark') -> Dict[str, Any]:
        """
        GSCãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Args:
            date_range_days (int): æ—¥æ•°
            start_date (str): é–‹å§‹æ—¥ (YYYY-MM-DDå½¢å¼ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            end_date (str): çµ‚äº†æ—¥ (YYYY-MM-DDå½¢å¼ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
            site_name (str): ã‚µã‚¤ãƒˆå ('moodmark' ã¾ãŸã¯ 'moodmarkgift')
            
        Returns:
            dict: ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿
        """
        try:
            logger.info(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: ã‚µã‚¤ãƒˆ={site_name}, æœŸé–“={date_range_days}æ—¥" + (f" ({start_date} ï½ {end_date})" if start_date and end_date else ""))
            
            # GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if start_date and end_date:
                gsc_data = self.google_apis._get_gsc_data_custom_range(
                    start_date=start_date,
                    end_date=end_date,
                    site_name=site_name
                )
            else:
                gsc_data = self.google_apis.get_gsc_data(
                    date_range_days=date_range_days,
                    dimensions=['date', 'page', 'query'],
                    site_name=site_name
                )
            
            logger.info(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(gsc_data)}è¡Œ")
            
            if gsc_data.empty:
                logger.warning(f"GSCãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼ˆã‚µã‚¤ãƒˆ: {site_name}ï¼‰ã€‚èªè¨¼çŠ¶æ…‹ã¨APIæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                return {
                    "error": f"GSCãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚µã‚¤ãƒˆ: {site_name}ï¼‰ã€‚èªè¨¼çŠ¶æ…‹ã¨APIæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                    "total_clicks": 0,
                    "total_impressions": 0,
                    "avg_ctr": 0.0,
                    "avg_position": 0.0
                }
            
            # ãƒšãƒ¼ã‚¸åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå–å¾—æ¸ˆã¿ã®gsc_dataã‹ã‚‰é›†è¨ˆã—ã¦é‡è¤‡å–å¾—ã‚’é¿ã‘ã‚‹ï¼‰
            if 'page' in gsc_data.columns:
                gsc_pages = gsc_data.groupby('page').agg({
                    'clicks': 'sum',
                    'impressions': 'sum',
                    'ctr': 'mean',
                    'position': 'mean'
                }).reset_index()
                # CTRã‚’è¨ˆç®—ã—ç›´ã—
                gsc_pages['ctr_calculated'] = (gsc_pages['clicks'] / gsc_pages['impressions'] * 100).round(2)
                gsc_pages['avg_position'] = gsc_pages['position'].round(2)
                gsc_pages = gsc_pages.sort_values('clicks', ascending=False).head(50)
            else:
                # pageã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯ç©ºã®DataFrameã‚’è¿”ã™
                import pandas as pd
                gsc_pages = pd.DataFrame()
            
            # ã‚¯ã‚¨ãƒªåˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆå–å¾—æ¸ˆã¿ã®gsc_dataã‹ã‚‰é›†è¨ˆã—ã¦é‡è¤‡å–å¾—ã‚’é¿ã‘ã‚‹ï¼‰
            if 'query' in gsc_data.columns:
                gsc_queries = gsc_data.groupby('query').agg({
                    'clicks': 'sum',
                    'impressions': 'sum',
                    'ctr': 'mean',
                    'position': 'mean'
                }).reset_index()
                # CTRã‚’è¨ˆç®—ã—ç›´ã—
                gsc_queries['ctr_calculated'] = (gsc_queries['clicks'] / gsc_queries['impressions'] * 100).round(2)
                gsc_queries['avg_position'] = gsc_queries['position'].round(2)
                gsc_queries = gsc_queries.sort_values('clicks', ascending=False).head(50)
            else:
                # queryã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯ç©ºã®DataFrameã‚’è¿”ã™
                import pandas as pd
                gsc_queries = pd.DataFrame()
            
            # ã‚µãƒãƒªãƒ¼è¨ˆç®—ï¼ˆgsc_dataã‹ã‚‰ç›´æ¥è¨ˆç®—ã—ã¦æ­£ç¢ºæ€§ã‚’å‘ä¸Šï¼‰
            total_clicks = int(gsc_data['clicks'].sum()) if not gsc_data.empty else 0
            total_impressions = int(gsc_data['impressions'].sum()) if not gsc_data.empty else 0
            avg_ctr = float((total_clicks / total_impressions * 100)) if total_impressions > 0 else 0.0
            avg_position = float(gsc_data['position'].mean()) if not gsc_data.empty else 0.0
            
            summary = {
                "total_clicks": total_clicks,
                "total_impressions": total_impressions,
                "avg_ctr": round(avg_ctr, 2),
                "avg_position": round(avg_position, 2),
                "top_pages_count": len(gsc_pages),
                "top_queries_count": len(gsc_queries),
                "date_range_days": date_range_days
            }
            
            logger.info(f"GSCã‚µãƒãƒªãƒ¼: ã‚¯ãƒªãƒƒã‚¯={summary['total_clicks']:,}, ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³={summary['total_impressions']:,}, CTR={summary['avg_ctr']:.2f}%")
            
            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã¨ãƒˆãƒƒãƒ—ã‚¯ã‚¨ãƒªã®æƒ…å ±
            if not gsc_pages.empty:
                summary["top_pages"] = gsc_pages.head(10).to_dict('records')
            if not gsc_queries.empty:
                summary["top_queries"] = gsc_queries.head(10).to_dict('records')
            
            return summary
            
        except Exception as e:
            logger.error(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"}
    
    def _extract_urls(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰URLã‚’æŠ½å‡ºã—ã€æ­£è¦åŒ–ã™ã‚‹
        
        Args:
            text (str): ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            list: æ­£è¦åŒ–ã•ã‚ŒãŸURLã®ãƒªã‚¹ãƒˆ
        """
        import re
        from urllib.parse import urlparse, urlunparse
        
        # URLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        urls = re.findall(url_pattern, text)
        
        # URLã®æ­£è¦åŒ–
        normalized_urls = []
        for url in urls:
            try:
                parsed = urlparse(url)
                # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€ï¼ˆãƒ‘ã‚¹ãŒã‚ã‚‹å ´åˆã¯ä¿æŒã€ãƒ«ãƒ¼ãƒˆã®å ´åˆã¯è¿½åŠ ï¼‰
                path = parsed.path.rstrip('/') or '/'
                if parsed.path and not parsed.path.endswith('/'):
                    path = parsed.path
                
                # æ­£è¦åŒ–ã•ã‚ŒãŸURLã‚’æ§‹ç¯‰
                normalized = urlunparse((
                    parsed.scheme,
                    parsed.netloc,
                    path,
                    parsed.params,
                    parsed.query,
                    ''  # fragmentã¯å‰Šé™¤
                ))
                normalized_urls.append(normalized)
                logger.info(f"URLæŠ½å‡ºãƒ»æ­£è¦åŒ–: {url} -> {normalized}")
            except Exception as e:
                logger.warning(f"URLæ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                normalized_urls.append(url)  # æ­£è¦åŒ–ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®URLã‚’ä½¿ç”¨
        
        if normalized_urls:
            logger.info(f"æŠ½å‡ºã•ã‚ŒãŸURLæ•°: {len(normalized_urls)}")
            for i, url in enumerate(normalized_urls, 1):
                logger.info(f"  {i}. {url}")
        else:
            logger.info("URLã¯æŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        return normalized_urls
    
    def _detect_site_from_url(self, url: str) -> str:
        """
        URLã‹ã‚‰ã‚µã‚¤ãƒˆåã‚’è‡ªå‹•åˆ¤å®š
        
        Args:
            url (str): ãƒšãƒ¼ã‚¸URL
            
        Returns:
            str: ã‚µã‚¤ãƒˆå ('moodmark' ã¾ãŸã¯ 'moodmarkgift')
        """
        if '/moodmarkgift/' in url:
            return 'moodmarkgift'
        elif '/moodmark/' in url:
            return 'moodmark'
        else:
            return self.default_site_name
    
    def _build_data_context(self, question: str, site_name: str = None, progress_callback=None, timeout: int = 300) -> str:
        """
        è³ªå•ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        
        Args:
            question (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            site_name (str): ã‚µã‚¤ãƒˆå ('moodmark' ã¾ãŸã¯ 'moodmarkgift')ã€Noneã®å ´åˆã¯è‡ªå‹•åˆ¤å®šã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            timeout (int): ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯300ç§’ï¼ˆ5åˆ†ï¼‰
            
        Returns:
            str: ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡å­—åˆ—
        """
        import time
        start_time = time.time()
        
        def check_timeout():
            """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯"""
            elapsed = time.time() - start_time
            if elapsed > timeout:
                raise TimeoutError(f"ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ{timeout}ç§’ï¼‰")
            return elapsed
        
        logger.info("=" * 60)
        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰é–‹å§‹")
        logger.info(f"è³ªå•: {question[:100]}...")
        
        date_range_result = self._extract_date_range(question)
        if isinstance(date_range_result, tuple) and len(date_range_result) == 3:
            start_date, end_date, date_range_days = date_range_result
            date_range = date_range_days
            logger.info(f"æŠ½å‡ºã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²: {date_range}æ—¥")
            if start_date and end_date:
                logger.info(f"  é–‹å§‹æ—¥: {start_date}, çµ‚äº†æ—¥: {end_date}")
        else:
            # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
            date_range = date_range_result if isinstance(date_range_result, int) else 30
            start_date, end_date = None, None
            logger.info(f"æŠ½å‡ºã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²: {date_range}æ—¥")
        
        context_parts = []
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ…‹ã‚’è¿½è·¡
        data_status = {
            'seo_analysis': False,
            'ga4_data': False,
            'gsc_data': False,
            'gsc_page_specific': False
        }
        
        # URLã‚’æŠ½å‡º
        urls = self._extract_urls(question)
        logger.info(f"æŠ½å‡ºã•ã‚ŒãŸURL: {urls}")
        
        # ã‚µã‚¤ãƒˆåã®æ±ºå®šï¼ˆURLã‹ã‚‰è‡ªå‹•åˆ¤å®šã€ã¾ãŸã¯æŒ‡å®šã•ã‚ŒãŸå€¤ã€ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        if site_name is None:
            if urls:
                # URLã‹ã‚‰è‡ªå‹•åˆ¤å®š
                site_name = self._detect_site_from_url(urls[0])
                logger.info(f"URLã‹ã‚‰ã‚µã‚¤ãƒˆã‚’è‡ªå‹•åˆ¤å®š: {site_name}")
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
                site_name = self.default_site_name
                logger.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µã‚¤ãƒˆã‚’ä½¿ç”¨: {site_name}")
        else:
            logger.info(f"æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ãƒˆã‚’ä½¿ç”¨: {site_name}")
        
        # SEOåˆ†æãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        question_lower = question.lower()
        needs_seo_analysis = any(keyword in question_lower for keyword in [
            'seo', 'ã‚¿ã‚¤ãƒˆãƒ«', 'ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³', 'è¦‹å‡ºã—', 'ãƒ¡ã‚¿', 'alt', 
            'æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿', 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', 'html', 'css', 'ãƒšãƒ¼ã‚¸åˆ†æ', 
            'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ', 'æ”¹å–„ææ¡ˆ', 'æœ€é©åŒ–'
        ]) or len(urls) > 0
        
        # å¹´æ¬¡æ¯”è¼ƒãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        needs_yearly_comparison = any(keyword in question_lower for keyword in [
            'æ˜¨å¹´', 'ä»Šå¹´', 'å‰å¹´', 'å‰å¹´æ¯”', 'yoy', 'year over year', 'æ¯”è¼ƒ',
            'æ¯”ã¹ã¦', 'å¯¾æ¯”', 'å¤‰åŒ–', 'å¢—æ¸›', 'æ¨ç§»', 'ãƒˆãƒ¬ãƒ³ãƒ‰'
        ])
        
        # ç‰¹å®šãƒšãƒ¼ã‚¸ã®åˆ†æãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        needs_page_specific_analysis = len(urls) > 0
        
        # GA4ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        needs_ga4 = any(keyword in question_lower for keyword in [
            'ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯', 'ã‚»ãƒƒã‚·ãƒ§ãƒ³', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼', 'ãƒã‚¦ãƒ³ã‚¹', 
            'æ»åœ¨æ™‚é–“', 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³', 'å£²ä¸Š', 'åç›Š', 'ã‚¢ã‚¯ã‚»ã‚¹', 'é›†å®¢',
            'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯', 'æµå…¥', 'è¨ªå•', 'æ¥è¨ª', 'æœˆé–“', 'æ•°å€¤', 'ãƒ¬ãƒãƒ¼ãƒˆ',
            'report', 'ãƒ‡ãƒ¼ã‚¿', 'çµ±è¨ˆ', 'åˆ†æ', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'å®Ÿç¸¾',
            'ãƒšãƒ¼ã‚¸åˆ†æ', 'ãƒšãƒ¼ã‚¸ã®åˆ†æ', 'åˆ†æã—ã¦', 'åˆ†æã—ã¦ãã ã•ã„'
        ]) or needs_yearly_comparison or needs_page_specific_analysis
        
        # GSCãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        needs_gsc = any(keyword in question_lower for keyword in [
            'æ¤œç´¢', 'seo', 'ã‚¯ãƒªãƒƒã‚¯', 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³', 'ctr', 'ãƒã‚¸ã‚·ãƒ§ãƒ³', 
            'é †ä½', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ã‚¯ã‚¨ãƒª', 'æ¤œç´¢æµå…¥', 'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯', 'é›†å®¢',
            'æœˆé–“', 'æ•°å€¤', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'report', 'ãƒ‡ãƒ¼ã‚¿', 'çµ±è¨ˆ', 'åˆ†æ'
        ]) or needs_yearly_comparison or needs_page_specific_analysis
        
        # SEOåˆ†æã‚’å®Ÿè¡Œï¼ˆURLãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ã¾ãŸã¯SEOé–¢é€£ã®è³ªå•ã®å ´åˆï¼‰
        if needs_seo_analysis:
            if not urls:
                logger.warning("SEOåˆ†æãŒå¿…è¦ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸãŒã€URLãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                logger.info("  è³ªå•å†…å®¹: " + question[:200])
                context_parts.append("âš ï¸ SEOåˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
                context_parts.append("ä¾‹: ã€Œã“ã®ãƒšãƒ¼ã‚¸ã®SEOæ”¹å–„ç‚¹ã¯ï¼Ÿ https://example.com/pageã€")
                context_parts.append("")
        
        if needs_seo_analysis and urls:
            logger.info(f"SEOåˆ†æãŒå¿…è¦ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸã€‚URLæ•°: {len(urls)}")
            if progress_callback:
                progress_callback("[STEP] ğŸ” SEOåˆ†æã‚’å®Ÿè¡Œä¸­...\n")
            for idx, url in enumerate(urls[:3], 1):  # æœ€å¤§3ã¤ã®URLã¾ã§
                logger.info(f"[{idx}/{min(len(urls), 3)}] SEOåˆ†æã‚’é–‹å§‹: {url}")
                try:
                    # ãƒšãƒ¼ã‚¸å–å¾—é–‹å§‹
                    logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ—1: ãƒšãƒ¼ã‚¸å–å¾—ã‚’é–‹å§‹...")
                    # JavaScriptå®Ÿè¡Œç’°å¢ƒã®ä½¿ç”¨ã‚’ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡
                    use_js = os.getenv('USE_SELENIUM', 'false').lower() == 'true' or os.getenv('USE_PLAYWRIGHT', 'false').lower() == 'true'
                    seo_analysis = self.seo_analyzer.analyze_page(url, use_js=use_js)
                    
                    # åˆ†æçµæœã®æ¤œè¨¼
                    if 'error' in seo_analysis:
                        error_msg = seo_analysis.get('error', 'Unknown error')
                        logger.error(f"  SEOåˆ†æã‚¨ãƒ©ãƒ¼: {error_msg}")
                        
                        # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è©³ç´°ã«è¨˜éŒ²
                        context_parts.append(f"âš ï¸ SEOåˆ†æã‚¨ãƒ©ãƒ¼ ({url})")
                        context_parts.append(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_msg}")
                        
                        # ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã«å¿œã˜ãŸè©³ç´°æƒ…å ±
                        if 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ' in error_msg or 'timeout' in error_msg.lower():
                            context_parts.append("")
                            context_parts.append("ã€ã‚¨ãƒ©ãƒ¼è©³ç´°ã€‘")
                            context_parts.append("ãƒšãƒ¼ã‚¸ã®å–å¾—ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚")
                            context_parts.append("è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
                            context_parts.append("- ã‚µãƒ¼ãƒãƒ¼ã®å¿œç­”ãŒé…ã„")
                            context_parts.append("- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã®å•é¡Œ")
                            context_parts.append("- ãƒšãƒ¼ã‚¸ãŒé‡ã„ï¼ˆå¤§é‡ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹ï¼‰")
                            context_parts.append("")
                            context_parts.append("ã€å¯¾å‡¦æ–¹æ³•ã€‘")
                            context_parts.append("- ã‚µãƒ¼ãƒãƒ¼ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¢ºèª")
                            context_parts.append("- ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿é€Ÿåº¦ã‚’æœ€é©åŒ–")
                            context_parts.append("- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª")
                        elif 'æ¥ç¶š' in error_msg or 'Connection' in error_msg:
                            context_parts.append("")
                            context_parts.append("ã€ã‚¨ãƒ©ãƒ¼è©³ç´°ã€‘")
                            context_parts.append("ãƒšãƒ¼ã‚¸ã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                            context_parts.append("è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
                            context_parts.append("- URLãŒæ­£ã—ããªã„")
                            context_parts.append("- ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„")
                            context_parts.append("- ã‚µãƒ¼ãƒãƒ¼ãŒãƒ€ã‚¦ãƒ³ã—ã¦ã„ã‚‹")
                            context_parts.append("- ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã‚„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹")
                            context_parts.append("")
                            context_parts.append("ã€å¯¾å‡¦æ–¹æ³•ã€‘")
                            context_parts.append("- URLãŒæ­£ã—ã„ã‹ç¢ºèª")
                            context_parts.append("- ãƒ–ãƒ©ã‚¦ã‚¶ã§ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ç¢ºèª")
                            context_parts.append("- ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèª")
                        
                        context_parts.append("")
                        context_parts.append("æ³¨æ„: ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ãŸãŸã‚ã€å®Ÿéš›ã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ†æã¯ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                        context_parts.append("GSCãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¯ãƒªãƒƒã‚¯æ•°ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãªã©ï¼‰ã®ã¿ã«åŸºã¥ã„ã¦åˆ†æã‚’è¡Œã„ã¾ã™ã€‚")
                        context_parts.append("")
                        continue
                    
                    # åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                    basic = seo_analysis.get('basic', {})
                    content = seo_analysis.get('content', {})
                    logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ—2: åˆ†æå®Œäº†")
                    logger.info(f"    ã‚¿ã‚¤ãƒˆãƒ«: {basic.get('title', 'N/A')[:50]}...")
                    logger.info(f"    ã‚¿ã‚¤ãƒˆãƒ«é•·: {basic.get('title_length', 0)}æ–‡å­—")
                    logger.info(f"    H1æ•°: {content.get('h1_count', 0)}")
                    logger.info(f"    æ–‡å­—æ•°: {content.get('char_count', 0):,}")
                    
                    if 'error' not in seo_analysis:
                        data_status['seo_analysis'] = True
                        context_parts.append(f"=== SEOåˆ†æçµæœ: {url} ===")
                        context_parts.append("")
                        
                        # ã€ç¾çŠ¶åˆ†æã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                        context_parts.append("ã€ç¾çŠ¶åˆ†æã€‘")
                        
                        # åŸºæœ¬SEO
                        basic = seo_analysis.get('basic', {})
                        context_parts.append("â–  åŸºæœ¬SEOè¦ç´ ")
                        context_parts.append(f"  ã‚¿ã‚¤ãƒˆãƒ«: {basic.get('title', 'N/A')}")
                        context_parts.append(f"  ã‚¿ã‚¤ãƒˆãƒ«é•·: {basic.get('title_length', 0)}æ–‡å­— (æœ€é©ç¯„å›²: 30-60æ–‡å­—) {'âœ“ æœ€é©' if basic.get('title_optimal') else 'âœ— è¦æ”¹å–„'}")
                        context_parts.append(f"  ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³: {basic.get('description', 'N/A')[:150]}...")
                        context_parts.append(f"  ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³é•·: {basic.get('description_length', 0)}æ–‡å­— (æœ€é©ç¯„å›²: 120-160æ–‡å­—) {'âœ“ æœ€é©' if basic.get('description_optimal') else 'âœ— è¦æ”¹å–„'}")
                        context_parts.append(f"  Canonical URL: {basic.get('canonical_url', 'N/A')}")
                        context_parts.append("")
                        
                        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ 
                        content = seo_analysis.get('content', {})
                        headings = content.get('headings', {})
                        context_parts.append("â–  ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ§‹é€ ")
                        context_parts.append(f"  H1æ•°: {content.get('h1_count', 0)} (ç†æƒ³çš„ã«ã¯1ã¤) {'âœ“ æœ€é©' if content.get('h1_optimal') else 'âœ— è¦æ”¹å–„'}")
                        
                        # H1ã®è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆ
                        if headings.get('h1'):
                            context_parts.append(f"  H1è¦‹å‡ºã—:")
                            for h1 in headings['h1'][:3]:  # æœ€å¤§3ã¤ã¾ã§
                                context_parts.append(f"    - {h1}")
                        
                        context_parts.append(f"  H2æ•°: {content.get('h2_count', 0)}")
                        # H2ã®è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„SEOã§é‡è¦ï¼‰
                        if headings.get('h2'):
                            context_parts.append(f"  H2è¦‹å‡ºã—ï¼ˆå…¨{len(headings['h2'])}å€‹ï¼‰:")
                            for idx, h2 in enumerate(headings['h2'], 1):
                                context_parts.append(f"    {idx}. {h2}")
                        
                        context_parts.append(f"  H3æ•°: {content.get('h3_count', 0)}")
                        # H3ã®è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„SEOã§é‡è¦ï¼‰
                        if headings.get('h3'):
                            context_parts.append(f"  H3è¦‹å‡ºã—ï¼ˆå…¨{len(headings['h3'])}å€‹ï¼‰:")
                            for idx, h3 in enumerate(headings['h3'], 1):
                                context_parts.append(f"    {idx}. {h3}")
                        
                        context_parts.append(f"  æ–‡å­—æ•°: {content.get('char_count', 0):,}æ–‡å­—")
                        context_parts.append(f"  èªæ•°: {content.get('word_count', 0):,}èª (æ¨å¥¨: 300èªä»¥ä¸Š) {'âœ“' if content.get('content_length_optimal') else 'âœ—'}")
                        
                        # è¦‹å‡ºã—æ§‹é€ ã®è©•ä¾¡
                        h1_count = content.get('h1_count', 0)
                        h2_count = content.get('h2_count', 0)
                        h3_count = content.get('h3_count', 0)
                        
                        context_parts.append("")
                        context_parts.append("â–  è¦‹å‡ºã—æ§‹é€ ã®è©•ä¾¡")
                        if h1_count == 1 and h2_count > 0:
                            context_parts.append("  è¦‹å‡ºã—éšå±¤: âœ“ é©åˆ‡ï¼ˆH1 â†’ H2 â†’ H3ã®éšå±¤æ§‹é€ ï¼‰")
                        elif h1_count == 0:
                            context_parts.append("  è¦‹å‡ºã—éšå±¤: âœ— H1ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆè¦æ”¹å–„ï¼‰")
                        elif h1_count > 1:
                            context_parts.append("  è¦‹å‡ºã—éšå±¤: âœ— H1ãŒè¤‡æ•°ã‚ã‚Šã¾ã™ï¼ˆè¦æ”¹å–„ï¼‰")
                        elif h2_count == 0:
                            context_parts.append("  è¦‹å‡ºã—éšå±¤: âš  H2ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆH1ã®ä¸‹ã«H2ã‚’è¿½åŠ æ¨å¥¨ï¼‰")
                        else:
                            context_parts.append("  è¦‹å‡ºã—éšå±¤: âœ“ åŸºæœ¬çš„ãªæ§‹é€ ã¯é©åˆ‡")
                        
                        # è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªè©•ä¾¡
                        heading_quality = content.get('heading_quality', {})
                        if heading_quality:
                            context_parts.append("")
                            context_parts.append("â–  è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªè©•ä¾¡")
                            
                            # SEOã‚¹ã‚³ã‚¢
                            seo_score = heading_quality.get('seo_score', 0)
                            context_parts.append(f"  SEOæœ€é©åŒ–ã‚¹ã‚³ã‚¢: {seo_score}/100")
                            
                            # H2ã®å“è³ªè©•ä¾¡
                            h2_quality = heading_quality.get('h2', {})
                            if h2_quality.get('total', 0) > 0:
                                context_parts.append("")
                                context_parts.append("  ã€H2è¦‹å‡ºã—ã®å“è³ªã€‘")
                                context_parts.append(f"    ç·æ•°: {h2_quality['total']}å€‹")
                                context_parts.append(f"    æœ€é©ãªé•·ã•ï¼ˆ30-60æ–‡å­—ï¼‰: {h2_quality.get('optimal_length', 0)}å€‹")
                                
                                if h2_quality.get('too_short'):
                                    context_parts.append(f"    çŸ­ã™ãã‚‹è¦‹å‡ºã—ï¼ˆ30æ–‡å­—æœªæº€ï¼‰: {len(h2_quality['too_short'])}å€‹")
                                    for item in h2_quality['too_short'][:5]:  # æœ€å¤§5å€‹ã¾ã§
                                        context_parts.append(f"      - ã€Œ{item['text']}ã€ ({item['length']}æ–‡å­—)")
                                
                                if h2_quality.get('too_long'):
                                    context_parts.append(f"    é•·ã™ãã‚‹è¦‹å‡ºã—ï¼ˆ60æ–‡å­—è¶…ï¼‰: {len(h2_quality['too_long'])}å€‹")
                                    for item in h2_quality['too_long'][:5]:  # æœ€å¤§5å€‹ã¾ã§
                                        context_parts.append(f"      - ã€Œ{item['text']}ã€ ({item['length']}æ–‡å­—)")
                                
                                if h2_quality.get('duplicates'):
                                    context_parts.append(f"    é‡è¤‡è¦‹å‡ºã—: {len(h2_quality['duplicates'])}å€‹")
                                    for dup in h2_quality['duplicates'][:3]:  # æœ€å¤§3å€‹ã¾ã§
                                        context_parts.append(f"      - ã€Œ{dup}ã€")
                                
                                if h2_quality.get('length_stats'):
                                    stats = h2_quality['length_stats']
                                    context_parts.append(f"    é•·ã•çµ±è¨ˆ: æœ€å°{stats['min']}æ–‡å­—ã€æœ€å¤§{stats['max']}æ–‡å­—ã€å¹³å‡{stats['avg']:.1f}æ–‡å­—")
                            
                            # H3ã®å“è³ªè©•ä¾¡
                            h3_quality = heading_quality.get('h3', {})
                            if h3_quality.get('total', 0) > 0:
                                context_parts.append("")
                                context_parts.append("  ã€H3è¦‹å‡ºã—ã®å“è³ªã€‘")
                                context_parts.append(f"    ç·æ•°: {h3_quality['total']}å€‹")
                                context_parts.append(f"    æœ€é©ãªé•·ã•ï¼ˆ20-50æ–‡å­—ï¼‰: {h3_quality.get('optimal_length', 0)}å€‹")
                                
                                if h3_quality.get('too_short'):
                                    context_parts.append(f"    çŸ­ã™ãã‚‹è¦‹å‡ºã—ï¼ˆ20æ–‡å­—æœªæº€ï¼‰: {len(h3_quality['too_short'])}å€‹")
                                    for item in h3_quality['too_short'][:5]:  # æœ€å¤§5å€‹ã¾ã§
                                        context_parts.append(f"      - ã€Œ{item['text']}ã€ ({item['length']}æ–‡å­—)")
                                
                                if h3_quality.get('too_long'):
                                    context_parts.append(f"    é•·ã™ãã‚‹è¦‹å‡ºã—ï¼ˆ50æ–‡å­—è¶…ï¼‰: {len(h3_quality['too_long'])}å€‹")
                                    for item in h3_quality['too_long'][:5]:  # æœ€å¤§5å€‹ã¾ã§
                                        context_parts.append(f"      - ã€Œ{item['text']}ã€ ({item['length']}æ–‡å­—)")
                                
                                if h3_quality.get('duplicates'):
                                    context_parts.append(f"    é‡è¤‡è¦‹å‡ºã—: {len(h3_quality['duplicates'])}å€‹")
                                    for dup in h3_quality['duplicates'][:3]:  # æœ€å¤§3å€‹ã¾ã§
                                        context_parts.append(f"      - ã€Œ{dup}ã€")
                                
                                if h3_quality.get('length_stats'):
                                    stats = h3_quality['length_stats']
                                    context_parts.append(f"    é•·ã•çµ±è¨ˆ: æœ€å°{stats['min']}æ–‡å­—ã€æœ€å¤§{stats['max']}æ–‡å­—ã€å¹³å‡{stats['avg']:.1f}æ–‡å­—")
                            
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
                            keyword_analysis = heading_quality.get('keyword_analysis', {})
                            if keyword_analysis.get('main_keywords'):
                                context_parts.append("")
                                context_parts.append("  ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã€‘")
                                context_parts.append(f"    ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keyword_analysis['main_keywords'])}")
                                context_parts.append(f"    H2ã§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡: {keyword_analysis.get('h2_keyword_coverage', 0):.1%}")
                                context_parts.append(f"    H3ã§ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡: {keyword_analysis.get('h3_keyword_coverage', 0):.1%}")
                        
                        context_parts.append("")
                        context_parts.append("ä¸Šè¨˜ã®è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªè©•ä¾¡ã‚’åŸºã«ã€å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚")
                        context_parts.append("ç‰¹ã«ã€é•·ã™ãã‚‹/çŸ­ã™ãã‚‹è¦‹å‡ºã—ã€é‡è¤‡è¦‹å‡ºã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ç‡ã®æ”¹å–„ã«ã¤ã„ã¦å…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚")
                        context_parts.append("")
                        
                        # ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£
                        accessibility = seo_analysis.get('accessibility', {})
                        context_parts.append("â–  ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£")
                        context_parts.append(f"  ç”»åƒaltå±æ€§ã‚«ãƒãƒ¬ãƒƒã‚¸: {accessibility.get('alt_coverage', 0):.1%}")
                        context_parts.append(f"  altå±æ€§ãªã—ç”»åƒ: {accessibility.get('images_without_alt', 0)}ä»¶ / ç·ç”»åƒæ•°: {accessibility.get('total_images', 0)}ä»¶")
                        context_parts.append(f"  ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆãªã—: {accessibility.get('links_without_text', 0)}ä»¶ / ç·ãƒªãƒ³ã‚¯æ•°: {accessibility.get('total_links', 0)}ä»¶")
                        context_parts.append("")
                        
                        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
                        structured = seo_analysis.get('structured_data', {})
                        context_parts.append("â–  æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿")
                        context_parts.append(f"  JSON-LD: {structured.get('json_ld_count', 0)}ä»¶")
                        context_parts.append(f"  ãƒã‚¤ã‚¯ãƒ­ãƒ‡ãƒ¼ã‚¿: {structured.get('microdata_count', 0)}ä»¶")
                        context_parts.append(f"  RDFa: {structured.get('rdfa_count', 0)}ä»¶")
                        context_parts.append(f"  æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡: {'âœ“ ã‚ã‚Š' if structured.get('has_structured_data') else 'âœ— ãªã—ï¼ˆè¿½åŠ æ¨å¥¨ï¼‰'}")
                        
                        # ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¤ãƒ—ã®è©³ç´°
                        schema_types = structured.get('schema_types', [])
                        if schema_types:
                            context_parts.append(f"  æ¤œå‡ºã•ã‚ŒãŸã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¤ãƒ—: {', '.join(schema_types)}")
                            context_parts.append(f"  ç·ã‚¹ã‚­ãƒ¼ãƒæ•°: {structured.get('total_schemas', 0)}")
                        
                        # ã‚¹ã‚­ãƒ¼ãƒè©³ç´°
                        schema_details = structured.get('schema_details', [])
                        if schema_details:
                            context_parts.append("")
                            context_parts.append("  ã€ã‚¹ã‚­ãƒ¼ãƒè©³ç´°ã€‘")
                            for i, detail in enumerate(schema_details[:5], 1):  # æœ€å¤§5å€‹ã¾ã§
                                schema_type = detail.get('schema_type', 'Unknown')
                                properties = detail.get('properties', {})
                                context_parts.append(f"    {i}. {schema_type}")
                                if properties:
                                    prop_list = ', '.join(properties.keys())
                                    context_parts.append(f"       ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: {prop_list}")
                        
                        # å“è³ªè©•ä¾¡
                        quality = structured.get('quality_score', {})
                        if quality:
                            context_parts.append("")
                            context_parts.append(f"  å“è³ªã‚¹ã‚³ã‚¢: {quality.get('score', 0)}/{quality.get('max_score', 100)}")
                            if quality.get('issues'):
                                context_parts.append("  ã€èª²é¡Œã€‘")
                                for issue in quality['issues']:
                                    context_parts.append(f"    - {issue}")
                            if quality.get('recommendations'):
                                context_parts.append("  ã€æ¨å¥¨äº‹é …ã€‘")
                                for rec in quality['recommendations']:
                                    context_parts.append(f"    - {rec}")
                        
                        # JavaScriptã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹å¯èƒ½æ€§ã®è­¦å‘Š
                        if structured.get('potential_js_structured_data'):
                            context_parts.append("")
                            context_parts.append("  âš ï¸ æ³¨æ„: JavaScriptã§æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒå‹•çš„ã«ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                            context_parts.append("    ç¾åœ¨ã®è§£ææ–¹æ³•ã§ã¯ã€JavaScriptã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¯æ¤œå‡ºã§ãã¾ã›ã‚“")
                            context_parts.append("    ã‚ˆã‚Šæ­£ç¢ºãªåˆ†æã«ã¯ã€JavaScriptå®Ÿè¡Œç’°å¢ƒï¼ˆSelenium/Playwrightï¼‰ãŒå¿…è¦ã§ã™")
                        
                        context_parts.append("")
                        
                        # ãƒªãƒ³ã‚¯æ§‹é€ 
                        links = seo_analysis.get('links', {})
                        context_parts.append("â–  ãƒªãƒ³ã‚¯æ§‹é€ ")
                        context_parts.append(f"  å†…éƒ¨ãƒªãƒ³ã‚¯: {links.get('internal_links', 0)}ä»¶")
                        context_parts.append(f"  å¤–éƒ¨ãƒªãƒ³ã‚¯: {links.get('external_links', 0)}ä»¶")
                        context_parts.append(f"  nofollowãƒªãƒ³ã‚¯: {links.get('nofollow_links', 0)}ä»¶")
                        context_parts.append("")
                        
                        # Page Speed Insightsãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                        # æ³¨æ„: å‡¦ç†æ™‚é–“ãŒé•·ã„ãŸã‚ï¼ˆãƒ¢ãƒã‚¤ãƒ«+ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã§ç´„90ç§’ï¼‰ã€ç„¡åŠ¹åŒ–ã—ã¦ã„ã¾ã™
                        # å¿…è¦ã«å¿œã˜ã¦ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦æœ‰åŠ¹åŒ–ã§ãã¾ã™
                        # logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ—3: Page Speed Insightsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹...")
                        # try:
                        #     psi_data_mobile = self.google_apis.get_pagespeed_insights(url, strategy='mobile')
                        #     psi_data_desktop = self.google_apis.get_pagespeed_insights(url, strategy='desktop')
                        #     
                        #     if 'error' not in psi_data_mobile or 'error' not in psi_data_desktop:
                        #         context_parts.append("")
                        #         context_parts.append("=== Page Speed Insightsåˆ†æçµæœ ===")
                        #         context_parts.append("")
                        #         
                        #         # ãƒ¢ãƒã‚¤ãƒ«çµæœ
                        #         if 'error' not in psi_data_mobile:
                        #             context_parts.append("ã€ãƒ¢ãƒã‚¤ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘")
                        #             lhr_mobile = psi_data_mobile.get('lighthouseResult', {})
                        #             categories_mobile = lhr_mobile.get('categories', {})
                        #             
                        #             if 'performance' in categories_mobile:
                        #                 perf_score = categories_mobile['performance'].get('score', 0)
                        #                 context_parts.append(f"  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {perf_score:.0f}/100")
                        #             
                        #             if 'accessibility' in categories_mobile:
                        #                 acc_score = categories_mobile['accessibility'].get('score', 0)
                        #                 context_parts.append(f"  ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {acc_score:.0f}/100")
                        #             
                        #             if 'best-practices' in categories_mobile:
                        #                 bp_score = categories_mobile['best-practices'].get('score', 0)
                        #                 context_parts.append(f"  ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚¹ã‚³ã‚¢: {bp_score:.0f}/100")
                        #             
                        #             if 'seo' in categories_mobile:
                        #                 seo_score = categories_mobile['seo'].get('score', 0)
                        #                 context_parts.append(f"  SEOã‚¹ã‚³ã‚¢: {seo_score:.0f}/100")
                        #             
                        #             # Core Web Vitals
                        #             cwv_mobile = psi_data_mobile.get('coreWebVitals', {})
                        #             if cwv_mobile:
                        #                 context_parts.append("")
                        #                 context_parts.append("  ã€Core Web Vitalsã€‘")
                        #                 if 'LCP' in cwv_mobile:
                        #                     lcp = cwv_mobile['LCP']
                        #                     context_parts.append(f"    LCP (Largest Contentful Paint): {lcp.get('percentile', 0):.0f}ms ({lcp.get('category', 'UNKNOWN')})")
                        #                 if 'FID' in cwv_mobile:
                        #                     fid = cwv_mobile['FID']
                        #                     context_parts.append(f"    FID (First Input Delay): {fid.get('percentile', 0):.0f}ms ({fid.get('category', 'UNKNOWN')})")
                        #                 if 'CLS' in cwv_mobile:
                        #                     cls = cwv_mobile['CLS']
                        #                     context_parts.append(f"    CLS (Cumulative Layout Shift): {cls.get('percentile', 0):.3f} ({cls.get('category', 'UNKNOWN')})")
                        #             
                        #             # ä¸»è¦ãªç›£æŸ»é …ç›®
                        #             audits_mobile = lhr_mobile.get('audits', {})
                        #             if audits_mobile:
                        #                 context_parts.append("")
                        #                 context_parts.append("  ã€ä¸»è¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã€‘")
                        #                 if 'first-contentful-paint' in audits_mobile:
                        #                     fcp = audits_mobile['first-contentful-paint']
                        #                     context_parts.append(f"    FCP (First Contentful Paint): {fcp.get('displayValue', 'N/A')}")
                        #                 if 'largest-contentful-paint' in audits_mobile:
                        #                     lcp_audit = audits_mobile['largest-contentful-paint']
                        #                     context_parts.append(f"    LCP (Largest Contentful Paint): {lcp_audit.get('displayValue', 'N/A')}")
                        #                 if 'total-blocking-time' in audits_mobile:
                        #                     tbt = audits_mobile['total-blocking-time']
                        #                     context_parts.append(f"    TBT (Total Blocking Time): {tbt.get('displayValue', 'N/A')}")
                        #                 if 'cumulative-layout-shift' in audits_mobile:
                        #                     cls_audit = audits_mobile['cumulative-layout-shift']
                        #                     context_parts.append(f"    CLS (Cumulative Layout Shift): {cls_audit.get('displayValue', 'N/A')}")
                        #                 if 'speed-index' in audits_mobile:
                        #                     si = audits_mobile['speed-index']
                        #                     context_parts.append(f"    Speed Index: {si.get('displayValue', 'N/A')}")
                        #         
                        #         # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—çµæœ
                        #         if 'error' not in psi_data_desktop:
                        #             context_parts.append("")
                        #             context_parts.append("ã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘")
                        #             lhr_desktop = psi_data_desktop.get('lighthouseResult', {})
                        #             categories_desktop = lhr_desktop.get('categories', {})
                        #             
                        #             if 'performance' in categories_desktop:
                        #                 perf_score = categories_desktop['performance'].get('score', 0)
                        #                 context_parts.append(f"  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {perf_score:.0f}/100")
                        #             
                        #             if 'accessibility' in categories_desktop:
                        #                 acc_score = categories_desktop['accessibility'].get('score', 0)
                        #                 context_parts.append(f"  ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {acc_score:.0f}/100")
                        #             
                        #             if 'best-practices' in categories_desktop:
                        #                 bp_score = categories_desktop['best-practices'].get('score', 0)
                        #                 context_parts.append(f"  ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚¹ã‚³ã‚¢: {bp_score:.0f}/100")
                        #             
                        #             if 'seo' in categories_desktop:
                        #                 seo_score = categories_desktop['seo'].get('score', 0)
                        #                 context_parts.append(f"  SEOã‚¹ã‚³ã‚¢: {seo_score:.0f}/100")
                        #         
                        #         context_parts.append("")
                        #         logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ—3: Page Speed Insightsãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
                        #     else:
                        #         # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹
                        #         error_msg_mobile = psi_data_mobile.get('error', 'Unknown error')
                        #         error_msg_desktop = psi_data_desktop.get('error', 'Unknown error')
                        #         logger.warning(f"  Page Speed Insightsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒã‚¤ãƒ«={error_msg_mobile}, ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—={error_msg_desktop}")
                        #         context_parts.append("")
                        #         context_parts.append("=== Page Speed Insightsåˆ†æçµæœ ===")
                        #         context_parts.append("âš ï¸ Page Speed Insightsãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        #         context_parts.append(f"  ãƒ¢ãƒã‚¤ãƒ«: {error_msg_mobile}")
                        #         context_parts.append(f"  ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—: {error_msg_desktop}")
                        #         context_parts.append("  ä»–ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆGA4ã€GSCã€SEOåˆ†æï¼‰ã§åˆ†æã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
                        #         context_parts.append("")
                        # except Exception as e:
                        #     # ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹
                        #     logger.warning(f"  Page Speed Insightsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                        #     context_parts.append("")
                        #     context_parts.append("=== Page Speed Insightsåˆ†æçµæœ ===")
                        #     context_parts.append("âš ï¸ Page Speed Insightsãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                        #     context_parts.append(f"  ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
                        #     context_parts.append("  ä»–ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆGA4ã€GSCã€SEOåˆ†æï¼‰ã§åˆ†æã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
                        #     context_parts.append("")
                        
                        # Page Speed Insightsã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰
                        logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ—3: Page Speed Insightsãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†æ™‚é–“çŸ­ç¸®ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰")
                        
                        # æŠ€è¡“çš„SEO
                        technical = seo_analysis.get('technical', {})
                        context_parts.append("â–  æŠ€è¡“çš„SEO")
                        context_parts.append(f"  Open Graph: {'âœ“ ã‚ã‚Š' if technical.get('open_graph') else 'âœ— ãªã—'}")
                        context_parts.append(f"  Twitter Card: {'âœ“ ã‚ã‚Š' if technical.get('twitter_card') else 'âœ— ãªã—'}")
                        context_parts.append(f"  ãƒ¢ãƒã‚¤ãƒ«å¯¾å¿œ: {'âœ“ ã‚ã‚Š' if technical.get('is_mobile_friendly') else 'âœ— ãªã—'}")
                        context_parts.append("")
                        
                        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
                        performance = seo_analysis.get('performance', {})
                        context_parts.append("â–  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
                        context_parts.append(f"  å¤–éƒ¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: {performance.get('external_scripts_count', 0)}ä»¶")
                        context_parts.append(f"  å¤–éƒ¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚·ãƒ¼ãƒˆ: {performance.get('external_stylesheets_count', 0)}ä»¶")
                        context_parts.append(f"  é…å»¶èª­ã¿è¾¼ã¿ç”»åƒ: {performance.get('lazy_loading_images', 0)}ä»¶ / ç·ç”»åƒæ•°: {performance.get('total_images', 0)}ä»¶")
                        context_parts.append("")
                        
                        context_parts.append("ä¸Šè¨˜ã®ç¾çŠ¶åˆ†æçµæœã‚’åŸºã«ã€ã€èª²é¡Œæ•´ç†ã€‘ã¨ã€æ”¹å–„ææ¡ˆã€‘ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚")
                        context_parts.append("")
                        
                        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œè¨¼
                        logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰å®Œäº†")
                        logger.info(f"    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œæ•°: {len(context_parts)}")
                        
                except Exception as e:
                    logger.error(f"SEOåˆ†æã§ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({url}): {e}", exc_info=True)
                    import traceback
                    error_details = traceback.format_exc()
                    logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{error_details}")
                    context_parts.append(f"âŒ SEOåˆ†æã‚¨ãƒ©ãƒ¼ ({url})")
                    context_parts.append(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
                    context_parts.append("ãƒšãƒ¼ã‚¸ã®å–å¾—ã¾ãŸã¯è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚URLãŒæ­£ã—ã„ã‹ã€ãƒšãƒ¼ã‚¸ãŒã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    context_parts.append("")
        
        # å¹´æ¬¡æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        if needs_yearly_comparison:
            logger.info("å¹´æ¬¡æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            page_url_for_comparison = urls[0] if urls else None
            yearly_comparison = self.google_apis.get_yearly_comparison_gsc(
                page_url=page_url_for_comparison,
                date_range_days=date_range,
                site_name=site_name
            )
            
            if 'error' in yearly_comparison:
                error_msg = yearly_comparison.get('error', 'Unknown error')
                logger.error(f"å¹´æ¬¡æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {error_msg}")
                context_parts.append("=== å¹´æ¬¡æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ï¼ˆGSCï¼‰ ===")
                context_parts.append("")
                context_parts.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")
                context_parts.append("")
                context_parts.append("ã€å¯¾å‡¦æ–¹æ³•ã€‘")
                context_parts.append("1. Render.comã®ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
                context_parts.append("   - GOOGLE_CREDENTIALS_FILE: Googleèªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä¾‹: config/google-credentials-474807.jsonï¼‰")
                context_parts.append("   - GSC_SITE_URL: GSCã‚µã‚¤ãƒˆURLï¼ˆä¾‹: https://isetan.mistore.jp/moodmark/ï¼‰")
                context_parts.append("2. èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
                context_parts.append("3. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«GSCã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
                context_parts.append("")
                context_parts.append("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„ãŸã‚ã€å¹´æ¬¡æ¯”è¼ƒåˆ†æã¯å®Ÿè¡Œã§ãã¾ã›ã‚“ã€‚")
                context_parts.append("ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã€è¨­å®šã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")
                context_parts.append("")
            else:
                context_parts.append("=== å¹´æ¬¡æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ï¼ˆGSCï¼‰ ===")
                context_parts.append("")
                context_parts.append("ã€ä»Šå¹´ã®ãƒ‡ãƒ¼ã‚¿ã€‘")
                this_year = yearly_comparison.get('this_year', {})
                context_parts.append(f"æœŸé–“: {this_year.get('period', 'N/A')}")
                context_parts.append(f"ã‚¯ãƒªãƒƒã‚¯æ•°: {this_year.get('clicks', 0):,}")
                context_parts.append(f"ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {this_year.get('impressions', 0):,}")
                context_parts.append(f"CTR: {this_year.get('ctr', 0):.2f}%")
                context_parts.append(f"å¹³å‡æ¤œç´¢é †ä½: {this_year.get('avg_position', 0):.2f}ä½")
                context_parts.append("")
                
                context_parts.append("ã€æ˜¨å¹´ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆåŒã˜æœŸé–“ï¼‰ã€‘")
                last_year = yearly_comparison.get('last_year', {})
                context_parts.append(f"æœŸé–“: {last_year.get('period', 'N/A')}")
                context_parts.append(f"ã‚¯ãƒªãƒƒã‚¯æ•°: {last_year.get('clicks', 0):,}")
                context_parts.append(f"ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {last_year.get('impressions', 0):,}")
                context_parts.append(f"CTR: {last_year.get('ctr', 0):.2f}%")
                context_parts.append(f"å¹³å‡æ¤œç´¢é †ä½: {last_year.get('avg_position', 0):.2f}ä½")
                context_parts.append("")
                
                context_parts.append("ã€å¤‰åŒ–ç‡ã€‘")
                changes = yearly_comparison.get('changes', {})
                context_parts.append(f"ã‚¯ãƒªãƒƒã‚¯æ•°: {changes.get('clicks', 0):+,} ({changes.get('clicks_change_pct', 0):+.1f}%)")
                context_parts.append(f"ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {changes.get('impressions', 0):+,} ({changes.get('impressions_change_pct', 0):+.1f}%)")
                context_parts.append(f"CTR: {changes.get('ctr', 0):+.2f}%ãƒã‚¤ãƒ³ãƒˆ")
                context_parts.append(f"å¹³å‡æ¤œç´¢é †ä½: {changes.get('position', 0):+.2f}ä½")
                context_parts.append("")
                context_parts.append("ä¸Šè¨˜ã®å¹´æ¬¡æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€æ˜¨å¹´ã¨æ¯”ã¹ã¦ä»Šå¹´ã®ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯é›†å®¢ãŒã©ã†å¤‰åŒ–ã—ãŸã‹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚")
                context_parts.append("")
        
        # ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿å–å¾—
        if needs_page_specific_analysis and urls:
            logger.info(f"ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­: {urls[0]}")
            if progress_callback:
                progress_callback("[STEP] ğŸ“Š GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n")
            page_gsc_data = self.google_apis.get_page_specific_gsc_data(
                page_url=urls[0],
                date_range_days=date_range,
                site_name=site_name,
                start_date=start_date,
                end_date=end_date
            )
            
            if 'error' not in page_gsc_data:
                step_elapsed = time.time() - step_start_time
                logger.info(f"ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {step_elapsed:.2f}ç§’")
                if step_elapsed > 5.0:
                    logger.warning(f"âš ï¸ ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã—ãŸ: {step_elapsed:.2f}ç§’")
                
                data_status['gsc_page_specific'] = True
                clicks = page_gsc_data.get('clicks', 0)
                impressions = page_gsc_data.get('impressions', 0)
                
                context_parts.append(f"=== ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿: {urls[0]} ===")
                if start_date and end_date:
                    context_parts.append(f"æœŸé–“: {start_date} ï½ {end_date}")
                else:
                    context_parts.append(f"æœŸé–“: éå»{date_range}æ—¥é–“")
                context_parts.append(f"ã‚¯ãƒªãƒƒã‚¯æ•°: {clicks:,}")
                context_parts.append(f"ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {impressions:,}")
                context_parts.append(f"CTR: {page_gsc_data.get('ctr', 0):.2f}%")
                context_parts.append(f"å¹³å‡æ¤œç´¢é †ä½: {page_gsc_data.get('avg_position', 0):.2f}ä½")
                
                # ãƒ‡ãƒ¼ã‚¿ãŒ0ã®å ´åˆã‚‚æ˜ç¤ºçš„ã«è¡¨ç¤º
                if clicks == 0 and impressions == 0:
                    context_parts.append("")
                    context_parts.append("âš ï¸ æ³¨æ„: ã“ã®æœŸé–“ã€ã“ã®ãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚")
                    context_parts.append("   - ãƒšãƒ¼ã‚¸ãŒã¾ã ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                    context_parts.append("   - æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã«æ¤œç´¢ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãŒãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                    context_parts.append("   - ãƒšãƒ¼ã‚¸URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
                
                context_parts.append("")
            elif 'error' in page_gsc_data:
                error_msg = page_gsc_data.get('error', 'Unknown error')
                error_code = page_gsc_data.get('error_code', '')
                logger.warning(f"ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {error_msg}")
                context_parts.append(f"=== ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿: {urls[0]} ===")
                context_parts.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")
                context_parts.append("")
                
                # 403ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ 
                if error_code == 403 or '403' in str(error_msg) or 'permission' in str(error_msg).lower() or 'forbidden' in str(error_msg).lower():
                    context_parts.append("ã€GSCæ¨©é™ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ–¹æ³•ã€‘")
                    context_parts.append(f"1. Google Search Consoleã§ã€ã‚µã‚¤ãƒˆ '{site_name}' ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’é–‹ã")
                    context_parts.append("2. è¨­å®š > ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ¨©é™ ã«ç§»å‹•")
                    context_parts.append("3. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¿½åŠ ã—ã€'æ‰€æœ‰è€…'ã¾ãŸã¯'ç·¨é›†è€…'æ¨©é™ã‚’ä»˜ä¸")
                    # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—
                    try:
                        if self.google_apis.credentials and hasattr(self.google_apis.credentials, 'service_account_email'):
                            service_account_email = self.google_apis.credentials.service_account_email
                            context_parts.append(f"4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {service_account_email}")
                        elif self.google_apis.credentials and hasattr(self.google_apis.credentials, '_service_account_email'):
                            service_account_email = self.google_apis.credentials._service_account_email
                            context_parts.append(f"4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {service_account_email}")
                        else:
                            context_parts.append("4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯èªè¨¼æƒ…å ±ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    except Exception as e:
                        logger.debug(f"ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                        context_parts.append("4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯èªè¨¼æƒ…å ±ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    context_parts.append("")
                else:
                    context_parts.append("ã€è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ã€‘")
                    context_parts.append("- ãƒšãƒ¼ã‚¸ãŒã¾ã GSCã«ç™»éŒ²ã•ã‚Œã¦ã„ãªã„")
                    context_parts.append("- æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„")
                    context_parts.append("- ãƒšãƒ¼ã‚¸URLãŒæ­£ã—ããªã„")
                    context_parts.append("- GSC APIã®èªè¨¼ã‚¨ãƒ©ãƒ¼")
                    context_parts.append("")
        
        if needs_ga4:
            import time
            step_start_time = time.time()
            check_timeout()  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸã€‚å–å¾—ã‚’é–‹å§‹...")
            if progress_callback:
                progress_callback("[STEP] ğŸ“ˆ GA4ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n")
            # URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            page_url_for_ga4 = urls[0] if urls else None
            ga4_summary = self._get_ga4_summary(date_range, start_date, end_date, page_url=page_url_for_ga4)
            
            step_elapsed = time.time() - step_start_time
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {step_elapsed:.2f}ç§’")
            if step_elapsed > 5.0:
                logger.warning(f"âš ï¸ GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã—ãŸ: {step_elapsed:.2f}ç§’")
            
            if "error" not in ga4_summary:
                data_status['ga4_data'] = True
                is_page_specific = ga4_summary.get('is_page_specific', False)
                if is_page_specific:
                    logger.info(f"å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®GA4ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: ã‚»ãƒƒã‚·ãƒ§ãƒ³={ga4_summary['total_sessions']:,}, ãƒ¦ãƒ¼ã‚¶ãƒ¼={ga4_summary['total_users']:,}, PV={ga4_summary['total_pageviews']:,}")
                    context_parts.append(f"=== å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®Google Analytics 4 (GA4) ãƒ‡ãƒ¼ã‚¿: {page_url_for_ga4} ===")
                else:
                    logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: ã‚»ãƒƒã‚·ãƒ§ãƒ³={ga4_summary['total_sessions']:,}, ãƒ¦ãƒ¼ã‚¶ãƒ¼={ga4_summary['total_users']:,}, PV={ga4_summary['total_pageviews']:,}")
                    context_parts.append("=== Google Analytics 4 (GA4) ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µã‚¤ãƒˆå…¨ä½“ï¼‰ ===")
                if start_date and end_date:
                    context_parts.append(f"æœŸé–“: {start_date} ï½ {end_date}")
                else:
                    context_parts.append(f"æœŸé–“: éå»{date_range}æ—¥é–“")
                context_parts.append(f"ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {ga4_summary['total_sessions']:,}")
                context_parts.append(f"ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {ga4_summary['total_users']:,}")
                context_parts.append(f"ç·ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°: {ga4_summary['total_pageviews']:,}")
                context_parts.append(f"å¹³å‡ãƒã‚¦ãƒ³ã‚¹ç‡: {ga4_summary['avg_bounce_rate']:.2%}")
                context_parts.append(f"å¹³å‡ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“: {ga4_summary['avg_session_duration']:.2f}ç§’")
                context_parts.append("")
            else:
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹
                error_msg = ga4_summary.get('error', 'Unknown error')
                is_timeout = ga4_summary.get('timeout', False)
                logger.warning(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {error_msg}")
                
                context_parts.append("=== Google Analytics 4 (GA4) ãƒ‡ãƒ¼ã‚¿ ===")
                if is_timeout:
                    context_parts.append(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {error_msg}")
                    context_parts.append("GA4ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ããŸãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚")
                    context_parts.append("ä»–ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆGSCã€SEOåˆ†æï¼‰ã§åˆ†æã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
                else:
                    context_parts.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")
                    context_parts.append("GA4ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚èªè¨¼çŠ¶æ…‹ã¨APIæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                context_parts.append("")
        else:
            logger.info("GA4ãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãªã—ã€URLãªã—ã€å¹´æ¬¡æ¯”è¼ƒãªã—ï¼‰")
        
        if needs_gsc:
            import time
            step_start_time = time.time()
            check_timeout()  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            if progress_callback and not (needs_page_specific_analysis and urls):  # ç‰¹å®šãƒšãƒ¼ã‚¸ã®GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã¨é‡è¤‡ã—ãªã„å ´åˆã®ã¿
                progress_callback("[STEP] ğŸ“Š GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n")
            gsc_summary = self._get_gsc_summary(date_range, start_date, end_date, site_name=site_name)
            
            step_elapsed = time.time() - step_start_time
            logger.info(f"GSCãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µã‚¤ãƒˆå…¨ä½“ï¼‰å–å¾—å®Œäº†: {step_elapsed:.2f}ç§’")
            if step_elapsed > 5.0:
                logger.warning(f"âš ï¸ GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã—ãŸ: {step_elapsed:.2f}ç§’")
            if "error" not in gsc_summary:
                data_status['gsc_data'] = True
                context_parts.append("=== Google Search Console (GSC) ãƒ‡ãƒ¼ã‚¿ ===")
                if start_date and end_date:
                    context_parts.append(f"æœŸé–“: {start_date} ï½ {end_date}")
                else:
                    context_parts.append(f"æœŸé–“: éå»{date_range}æ—¥é–“")
                context_parts.append(f"ç·ã‚¯ãƒªãƒƒã‚¯æ•°: {gsc_summary['total_clicks']:,}")
                context_parts.append(f"ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {gsc_summary['total_impressions']:,}")
                context_parts.append(f"å¹³å‡CTR: {gsc_summary['avg_ctr']:.2f}%")
                context_parts.append(f"å¹³å‡æ¤œç´¢é †ä½: {gsc_summary['avg_position']:.2f}ä½")
                
                if 'top_pages' in gsc_summary and gsc_summary['top_pages']:
                    context_parts.append("\nã€ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ï¼ˆã‚¯ãƒªãƒƒã‚¯æ•°é †ï¼‰ã€‘")
                    for i, page in enumerate(gsc_summary['top_pages'][:5], 1):
                        context_parts.append(f"{i}. {page.get('page', 'N/A')}: ã‚¯ãƒªãƒƒã‚¯æ•° {page.get('clicks', 0):,}, ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•° {page.get('impressions', 0):,}")
                
                if 'top_queries' in gsc_summary and gsc_summary['top_queries']:
                    context_parts.append("\nã€ãƒˆãƒƒãƒ—æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆã‚¯ãƒªãƒƒã‚¯æ•°é †ï¼‰ã€‘")
                    for i, query in enumerate(gsc_summary['top_queries'][:5], 1):
                        context_parts.append(f"{i}. {query.get('query', 'N/A')}: ã‚¯ãƒªãƒƒã‚¯æ•° {query.get('clicks', 0):,}, ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•° {query.get('impressions', 0):,}")
                context_parts.append("")
            else:
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹
                error_msg = gsc_summary.get('error', 'Unknown error')
                error_code = gsc_summary.get('error_code', '')
                logger.warning(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {error_msg}")
                context_parts.append("=== Google Search Console (GSC) ãƒ‡ãƒ¼ã‚¿ ===")
                context_parts.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")
                context_parts.append(f"GSCãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚µã‚¤ãƒˆ: {site_name}ï¼‰ã€‚")
                
                # 403ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ 
                if error_code == 403 or '403' in str(error_msg) or 'permission' in str(error_msg).lower() or 'forbidden' in str(error_msg).lower():
                    context_parts.append("")
                    context_parts.append("ã€GSCæ¨©é™ã‚¨ãƒ©ãƒ¼ã®å¯¾å‡¦æ–¹æ³•ã€‘")
                    context_parts.append(f"1. Google Search Consoleã§ã€ã‚µã‚¤ãƒˆ '{site_name}' ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’é–‹ã")
                    context_parts.append("2. è¨­å®š > ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ¨©é™ ã«ç§»å‹•")
                    context_parts.append("3. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è¿½åŠ ã—ã€'æ‰€æœ‰è€…'ã¾ãŸã¯'ç·¨é›†è€…'æ¨©é™ã‚’ä»˜ä¸")
                    # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—
                    try:
                        if self.google_apis.credentials and hasattr(self.google_apis.credentials, 'service_account_email'):
                            service_account_email = self.google_apis.credentials.service_account_email
                            context_parts.append(f"4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {service_account_email}")
                        elif self.google_apis.credentials and hasattr(self.google_apis.credentials, '_service_account_email'):
                            service_account_email = self.google_apis.credentials._service_account_email
                            context_parts.append(f"4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {service_account_email}")
                        else:
                            context_parts.append("4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯èªè¨¼æƒ…å ±ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    except Exception as e:
                        logger.debug(f"ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                        context_parts.append("4. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯èªè¨¼æƒ…å ±ã‹ã‚‰å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    context_parts.append("")
                else:
                    context_parts.append("èªè¨¼çŠ¶æ…‹ã¨APIæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    context_parts.append("")
        
        if not context_parts:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ¢ãƒ¼ãƒ‰: GA4ã¨GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
            ga4_summary = self._get_ga4_summary(date_range)
            gsc_summary = self._get_gsc_summary(date_range, site_name=site_name)
            
            if "error" not in ga4_summary:
                context_parts.append("=== Google Analytics 4 (GA4) ãƒ‡ãƒ¼ã‚¿ ===")
                context_parts.append(f"æœŸé–“: éå»{date_range}æ—¥é–“")
                context_parts.append(f"ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {ga4_summary['total_sessions']:,}")
                context_parts.append(f"ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {ga4_summary['total_users']:,}")
                context_parts.append(f"ç·ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°: {ga4_summary['total_pageviews']:,}")
                context_parts.append("")
            
            if "error" not in gsc_summary:
                context_parts.append("=== Google Search Console (GSC) ãƒ‡ãƒ¼ã‚¿ ===")
                context_parts.append(f"æœŸé–“: éå»{date_range}æ—¥é–“")
                context_parts.append(f"ç·ã‚¯ãƒªãƒƒã‚¯æ•°: {gsc_summary['total_clicks']:,}")
                context_parts.append(f"ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°: {gsc_summary['total_impressions']:,}")
                context_parts.append(f"å¹³å‡CTR: {gsc_summary['avg_ctr']:.2f}%")
                context_parts.append(f"å¹³å‡æ¤œç´¢é †ä½: {gsc_summary['avg_position']:.2f}ä½")
                context_parts.append("")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œè¨¼
        total_elapsed = time.time() - start_time
        context_text = "\n".join(context_parts)
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰å®Œäº†")
        logger.info(f"  ç·å‡¦ç†æ™‚é–“: {total_elapsed:.2f}ç§’")
        logger.info(f"  ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(context_text)}æ–‡å­—")
        logger.info(f"  ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œæ•°: {len(context_parts)}è¡Œ")
        
        if not context_text.strip():
            logger.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
        else:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ…‹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆæ­£ç¢ºãªåˆ¤å®šï¼‰
            has_gsc_data_success = data_status.get('gsc_data', False) or data_status.get('gsc_page_specific', False)
            logger.info(f"  å«ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿: SEO={data_status['seo_analysis']}, GA4={data_status['ga4_data']}, GSC={has_gsc_data_success}")
        
        if total_elapsed > 10.0:
            logger.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã—ãŸ: {total_elapsed:.2f}ç§’")
        
        logger.info("=" * 60)
        
        return context_text
    
    def ask(self, question: str, model: str = "gpt-4o-mini", site_name: str = None) -> str:
        """
        è³ªå•ã«å¯¾ã—ã¦AIãŒå›ç­”ã‚’ç”Ÿæˆ
        
        Args:
            question (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            model (str): ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gpt-4o-miniï¼‰
            site_name (str): ã‚µã‚¤ãƒˆå ('moodmark' ã¾ãŸã¯ 'moodmarkgift')ã€Noneã®å ´åˆã¯è‡ªå‹•åˆ¤å®šã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
            
        Returns:
            str: AIã®å›ç­”
        """
        try:
            logger.info("=" * 60)
            logger.info("AIå›ç­”ç”Ÿæˆé–‹å§‹")
            logger.info(f"è³ªå•: {question[:100]}...")
            logger.info(f"ãƒ¢ãƒ‡ãƒ«: {model}")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
            logger.info("ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ä¸­...")
            data_context = self._build_data_context(question, site_name=site_name)
            
            if not data_context or not data_context.strip():
                logger.warning("ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
                return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰å®Œäº†: {len(data_context)}æ–‡å­—")
            
            # SEOé–¢é€£ã®è³ªå•ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            question_lower = question.lower()
            is_seo_question = any(keyword in question_lower for keyword in [
                'seo', 'ã‚¿ã‚¤ãƒˆãƒ«', 'ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³', 'è¦‹å‡ºã—', 'ãƒ¡ã‚¿', 'alt', 
                'æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿', 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', 'html', 'css', 'ãƒšãƒ¼ã‚¸åˆ†æ', 
                'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ', 'æ”¹å–„ææ¡ˆ', 'æœ€é©åŒ–', 'æ”¹å–„ç‚¹', 'èª²é¡Œ'
            ]) or len(self._extract_urls(question)) > 0
            
            # å¹´æ¬¡æ¯”è¼ƒãŒè¦æ±‚ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            question_lower = question.lower()
            is_yearly_comparison = any(keyword in question_lower for keyword in [
                'æ˜¨å¹´', 'ä»Šå¹´', 'å‰å¹´', 'å‰å¹´æ¯”', 'yoy', 'year over year', 'æ¯”è¼ƒ',
                'æ¯”ã¹ã¦', 'å¯¾æ¯”', 'å¤‰åŒ–', 'å¢—æ¸›', 'æ¨ç§»', 'ãƒˆãƒ¬ãƒ³ãƒ‰'
            ])
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            if is_yearly_comparison:
                user_prompt = f"""ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

**é‡è¦**: å¹´æ¬¡æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€å¿…ãšæ˜¨å¹´ã¨ä»Šå¹´ã®æ•°å€¤ã‚’æ¯”è¼ƒã—ã¦åˆ†æã—ã¦ãã ã•ã„ã€‚
- ã‚¯ãƒªãƒƒã‚¯æ•°ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ã€CTRã€å¹³å‡æ¤œç´¢é †ä½ã®å¤‰åŒ–ã‚’å…·ä½“çš„ã«ç¤ºã—ã¦ãã ã•ã„
- å¢—æ¸›ç‡ã‚’è¨ˆç®—ã—ã¦ã€æ”¹å–„ã—ã¦ã„ã‚‹ã‹æ‚ªåŒ–ã—ã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„
- å¤‰åŒ–ã®åŸå› ã‚’æ¨æ¸¬ã—ã€æ”¹å–„ææ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„

è³ªå•: {question}

ãƒ‡ãƒ¼ã‚¿:
{data_context}

å›ç­”ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„:
- æ˜¨å¹´ã¨ä»Šå¹´ã®æ•°å€¤ã®æ¯”è¼ƒ
- å¢—æ¸›ç‡ã®è¨ˆç®—
- å¤‰åŒ–ã®åˆ†æã¨åŸå› ã®æ¨æ¸¬
- æ”¹å–„ææ¡ˆï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- ã‚ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§èª¬æ˜"""
            elif is_seo_question:
                user_prompt = f"""ä»¥ä¸‹ã®SEOåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

{data_context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}

ã€å›ç­”å½¢å¼ã€‘
å¿…ãšä»¥ä¸‹ã®3æ®µéšã®æ§‹é€ ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

1. ã€ç¾çŠ¶åˆ†æã€‘
   - ç¾åœ¨ã®SEOè¦ç´ ã®çŠ¶æ…‹ã‚’æ•°å€¤ã¨å…±ã«æ˜ç¢ºã«ç¤ºã™
   - æœ€é©å€¤ã¨ã®æ¯”è¼ƒã‚’ç¤ºã™
   - å„è¦ç´ ã®ç¾çŠ¶ã‚’æ•´ç†

2. ã€èª²é¡Œæ•´ç†ã€‘
   - ç¾çŠ¶åˆ†æã‹ã‚‰è¦‹ã¤ã‹ã£ãŸå•é¡Œç‚¹ã‚’å„ªå…ˆåº¦é †ã«æ•´ç†
   - å„èª²é¡ŒãŒSEOã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¬æ˜
   - ç·Šæ€¥åº¦ãƒ»é‡è¦åº¦ã‚’è€ƒæ…®

3. ã€æ”¹å–„ææ¡ˆã€‘
   - å„èª²é¡Œã«å¯¾ã™ã‚‹å…·ä½“çš„ãªæ”¹å–„æ–¹æ³•ã‚’æç¤º
   - å®Ÿè£…å¯èƒ½ãªå…·ä½“çš„ãªæ”¹å–„æ¡ˆã‚’æç¤ºï¼ˆä¾‹ï¼šã‚¿ã‚¤ãƒˆãƒ«ã®æ”¹å–„æ¡ˆã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®æ”¹å–„æ¡ˆï¼‰
   - å„ªå…ˆé †ä½ã‚’ã¤ã‘ã¦æ”¹å–„ã™ã¹ãé †åºã‚’æç¤º
   - å¯èƒ½ã§ã‚ã‚Œã°ã€æ”¹å–„å‰å¾Œã®æ¯”è¼ƒã‚‚ç¤ºã™

ã‚ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§ã€å…·ä½“çš„ãªæ•°å€¤ã¨å…±ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"""
            else:
                user_prompt = f"""ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

{data_context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}

å›ç­”ã¯ä»¥ä¸‹ã®ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„
- é‡è¦ãªæ•°å€¤ã®èª¬æ˜
- æ”¹å–„ææ¡ˆã‚„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- ã‚ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§èª¬æ˜"""
            
            # OpenAI APIã‚’å‘¼ã³å‡ºã—
            logger.info(f"OpenAI APIã‚’å‘¼ã³å‡ºã—ä¸­... (ãƒ¢ãƒ‡ãƒ«: {model})")
            logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(user_prompt)}æ–‡å­—")
            
            try:
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=2000
                )
                
                answer = response.choices[0].message.content
                logger.info(f"å›ç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(answer)}æ–‡å­—")
                logger.info("=" * 60)
                
                return answer
                
            except Exception as api_error:
                logger.error(f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}", exc_info=True)
                raise api_error
            
        except Exception as e:
            logger.error(f"AIå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{error_details}")
            raise  # ã‚¨ãƒ©ãƒ¼ã‚’å†ç™ºç”Ÿã•ã›ã¦ã€UIå´ã§å‡¦ç†
    
    def ask_stream(self, question: str, model: str = "gpt-4o-mini", site_name: str = None, conversation_history: List[Dict] = None) -> Generator[str, None, str]:
        """
        AIã«è³ªå•ã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å–å¾—ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼‰
        
        Args:
            question (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            model (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            site_name (str): ã‚µã‚¤ãƒˆåï¼ˆ'moodmark' ã¾ãŸã¯ 'moodmarkgift'ï¼‰
            conversation_history (List[Dict]): ä¼šè©±å±¥æ­´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Yields:
            str: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã®ãƒãƒ£ãƒ³ã‚¯
        
        Returns:
            str: å®Œå…¨ãªå¿œç­”ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯éƒ¨åˆ†å¿œç­”ï¼‰
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—ã®é€²æ—ã‚’yieldï¼ˆå¾…æ©Ÿæ™‚é–“ä¸­ã®ã‚¹ãƒˆãƒ¬ã‚¹è»½æ¸›ã®ãŸã‚ï¼‰
            yield "[STEP] ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n\n"
            
            # URLã‚’æŠ½å‡ºã—ã¦ã€ã©ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ã‹åˆ¤å®š
            import re
            url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
            urls = re.findall(url_pattern, question)
            question_lower = question.lower()
            
            # SEOåˆ†æãŒå¿…è¦ã‹åˆ¤å®š
            needs_seo_analysis = any(keyword in question_lower for keyword in [
                'seo', 'ã‚¿ã‚¤ãƒˆãƒ«', 'ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³', 'è¦‹å‡ºã—', 'ãƒ¡ã‚¿', 'alt', 
                'æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿', 'ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°', 'html', 'css', 'ãƒšãƒ¼ã‚¸åˆ†æ', 
                'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ', 'æ”¹å–„ææ¡ˆ', 'æœ€é©åŒ–'
            ]) or len(urls) > 0
            
            # GA4/GSCãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã‹åˆ¤å®š
            needs_ga4 = any(keyword in question_lower for keyword in [
                'ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯', 'ã‚»ãƒƒã‚·ãƒ§ãƒ³', 'ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼', 'ãƒã‚¦ãƒ³ã‚¹', 
                'æ»åœ¨æ™‚é–“', 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³', 'å£²ä¸Š', 'åç›Š', 'ã‚¢ã‚¯ã‚»ã‚¹', 'é›†å®¢',
                'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯', 'æµå…¥', 'è¨ªå•', 'æ¥è¨ª', 'æœˆé–“', 'æ•°å€¤', 'ãƒ¬ãƒãƒ¼ãƒˆ',
                'report', 'ãƒ‡ãƒ¼ã‚¿', 'çµ±è¨ˆ', 'åˆ†æ', 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹', 'å®Ÿç¸¾',
                'ãƒšãƒ¼ã‚¸åˆ†æ', 'ãƒšãƒ¼ã‚¸ã®åˆ†æ', 'åˆ†æã—ã¦', 'åˆ†æã—ã¦ãã ã•ã„'
            ]) or len(urls) > 0
            
            needs_gsc = any(keyword in question_lower for keyword in [
                'æ¤œç´¢', 'seo', 'ã‚¯ãƒªãƒƒã‚¯', 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³', 'ctr', 'ãƒã‚¸ã‚·ãƒ§ãƒ³', 
                'é †ä½', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ã‚¯ã‚¨ãƒª', 'æ¤œç´¢æµå…¥', 'ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯', 'é›†å®¢',
                'æœˆé–“', 'æ•°å€¤', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'report', 'ãƒ‡ãƒ¼ã‚¿', 'çµ±è¨ˆ', 'åˆ†æ'
            ]) or len(urls) > 0
            
            # é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚­ãƒ¥ãƒ¼ï¼ˆyieldã™ã‚‹ãŸã‚ï¼‰
            progress_queue = []
            
            # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã‚’å®šç¾©
            def progress_callback(message: str):
                """é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
                progress_queue.append(message)
            
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã®é–‹å§‹ã‚’yield
            if needs_seo_analysis and urls:
                yield "[STEP] ğŸ” SEOåˆ†æã‚’å®Ÿè¡Œä¸­...\n"
            
            if needs_gsc:
                yield "[STEP] ğŸ“Š GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n"
            
            if needs_ga4:
                yield "[STEP] ğŸ“ˆ GA4ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n"
            
            # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆé€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
            data_context = self._build_data_context(question, site_name=site_name, progress_callback=progress_callback)
            
            # ã‚­ãƒ¥ãƒ¼ã«è“„ç©ã•ã‚ŒãŸé€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yield
            for msg in progress_queue:
                yield msg
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ã‚’é€šçŸ¥
            yield "[STEP] âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†\n\n"
            yield "[STEP] ğŸ¤– AIåˆ†æã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...\n\n"
            
            # ä¼šè©±å±¥æ­´ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            conversation_context = ""
            if conversation_history and len(conversation_history) > 0:
                conversation_context = "\n=== ä¼šè©±å±¥æ­´ ===\n"
                # ç›´è¿‘ã®2-3ä»¶ã®ä¼šè©±ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹
                for msg in conversation_history[-3:]:
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')
                    # é•·ã™ãã‚‹å ´åˆã¯çœç•¥
                    content_preview = content[:200] + "..." if len(content) > 200 else content
                    conversation_context += f"{role}: {content_preview}\n"
                conversation_context += "\n"
            
            # ã‚·ãƒ³ãƒ—ãƒ«ã§æŸ”è»Ÿãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±ä¸€
            user_prompt = f"""ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

{conversation_context}ãƒ‡ãƒ¼ã‚¿:
{data_context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}

è³ªå•ã®æ„å›³ã‚’ç†è§£ã—ã€é©åˆ‡ãªå›ç­”å½¢å¼ã¨å«ã‚ã‚‹ã¹ãæƒ…å ±ã‚’é¸æŠã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
- æä¾›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æœ€å¤§é™ã«æ´»ç”¨ã—ã¦ãã ã•ã„
- ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
- è³ªå•ã®ç¨®é¡ã«å¿œã˜ã¦ã€é©åˆ‡ãªæ§‹é€ ã§å›ç­”ã—ã¦ãã ã•ã„
- æ•°å€¤ã¯å¿…ãšå…·ä½“çš„ã«ç¤ºã—ã¦ãã ã•ã„
- ã‚ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„"""
            
            # OpenAI APIã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å‘¼ã³å‡ºã—
            logger.info(f"OpenAI APIã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å‘¼ã³å‡ºã—ä¸­... (ãƒ¢ãƒ‡ãƒ«: {model})")
            logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(user_prompt)}æ–‡å­—")
            
            full_answer = ""  # å®Œå…¨ãªå¿œç­”ã‚’è“„ç©
            
            try:
                stream = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=2000,
                    stream=True  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
                )
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’å‡¦ç†
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        full_answer += content
                        yield content
                
                logger.info(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {len(full_answer)}æ–‡å­—")
                logger.info("=" * 60)
                
                # å®Œå…¨ãªå¿œç­”ã‚’è¿”ã™ï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã®æˆ»ã‚Šå€¤ã¨ã—ã¦ï¼‰
                return full_answer
                
            except Exception as api_error:
                logger.error(f"OpenAI APIã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {api_error}", exc_info=True)
                # éƒ¨åˆ†çš„ãªå¿œç­”ãŒã‚ã‚Œã°è¿”ã™
                if full_answer:
                    yield f"\n\nâš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(api_error)}"
                    return full_answer
                raise api_error
            
        except Exception as e:
            logger.error(f"AIã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°:\n{error_details}")
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’yieldã—ã¦ã‹ã‚‰å†ç™ºç”Ÿ
            yield f"\n\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            raise  # ã‚¨ãƒ©ãƒ¼ã‚’å†ç™ºç”Ÿã•ã›ã¦ã€UIå´ã§å‡¦ç†
    
    def get_available_models(self) -> List[str]:
        """
        åˆ©ç”¨å¯èƒ½ãªOpenAIãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Returns:
            list: ãƒ¢ãƒ‡ãƒ«åã®ãƒªã‚¹ãƒˆ
        """
        return [
            "gpt-4o-mini",
            "gpt-4o",
            "gpt-4-turbo",
            "gpt-3.5-turbo"
        ]

