#!/usr/bin/env python3
"""
ä¿®æ­£ç‰ˆæœŸé–“åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
- pagePathã§ã‚µã‚¤ãƒˆã‚’åˆ†ã‘ã¦åˆ†æ
- æ­£ã—ã„ã‚µã‚¤ãƒˆåˆ¥ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import logging

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¾ãŸã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
try:
    from .oauth_google_apis import OAuthGoogleAPIsIntegration
except ImportError:
    from oauth_google_apis import OAuthGoogleAPIsIntegration

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/corrected_analysis.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CorrectedPeriodAnalysis:
    def __init__(self):
        """ä¿®æ­£ç‰ˆæœŸé–“åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        self.api_integration = OAuthGoogleAPIsIntegration()
        self.config = self._load_config()
        
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs('logs', exist_ok=True)
        os.makedirs('data/processed', exist_ok=True)
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        config_file = 'config/analytics_config.json'
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_file}")
                return {}
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def get_gsc_data_for_period(self, start_date: str, end_date: str, site_url: str):
        """æœŸé–“æŒ‡å®šã§GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            if not self.api_integration.gsc_service:
                logger.warning("GSCã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return pd.DataFrame()
            
            # GSCãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            request = {
                'startDate': start_date,
                'endDate': end_date,
                'dimensions': ['page', 'query'],
                'rowLimit': 10000,
                'startRow': 0
            }
            
            # APIå‘¼ã³å‡ºã—
            response = self.api_integration.gsc_service.searchanalytics().query(
                siteUrl=site_url,
                body=request
            ).execute()
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
            data = []
            if 'rows' in response:
                for row in response['rows']:
                    row_data = {
                        'page': row['keys'][0] if len(row.get('keys', [])) > 0 else '',
                        'query': row['keys'][1] if len(row.get('keys', [])) > 1 else '',
                        'clicks': row.get('clicks', 0),
                        'impressions': row.get('impressions', 0),
                        'ctr': row.get('ctr', 0),
                        'position': row.get('position', 0)
                    }
                    data.append(row_data)
            
            df = pd.DataFrame(data)
            logger.info(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ (ã‚µã‚¤ãƒˆ: {site_url})")
            return df
            
        except Exception as e:
            logger.error(f"GSCãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def get_ga4_data_for_period(self, start_date: str, end_date: str):
        """æœŸé–“æŒ‡å®šã§GA4ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            if not self.api_integration.ga4_service:
                logger.warning("GA4ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return pd.DataFrame()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
            metrics = [
                'sessions',
                'totalUsers', 
                'screenPageViews',
                'bounceRate',
                'averageSessionDuration',
                'conversions'
            ]
            
            dimensions = [
                'date',
                'pagePath',
                'sessionDefaultChannelGrouping',
                'deviceCategory'
            ]
            
            # GA4ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            request_body = {
                'dateRanges': [{'startDate': start_date, 'endDate': end_date}],
                'metrics': [{'name': metric} for metric in metrics],
                'dimensions': [{'name': dimension} for dimension in dimensions],
                'limit': 10000
            }
            
            # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£IDã‚’å–å¾—
            property_id = self.config.get('sites', {}).get('moodmark', {}).get('ga4_property_id')
            if not property_id:
                logger.error("GA4ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return pd.DataFrame()
            
            # APIå‘¼ã³å‡ºã—
            response = self.api_integration.ga4_service.properties().runReport(
                property=f"properties/{property_id}",
                body=request_body
            ).execute()
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
            data = []
            if 'rows' in response:
                for row in response['rows']:
                    row_data = {}
                    
                    # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³å€¤ã®å–å¾—
                    for i, dimension in enumerate(dimensions):
                        if i < len(row.get('dimensionValues', [])):
                            row_data[dimension] = row['dimensionValues'][i].get('value', '')
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã®å–å¾—
                    for i, metric in enumerate(metrics):
                        if i < len(row.get('metricValues', [])):
                            value = row['metricValues'][i].get('value', '0')
                            try:
                                row_data[metric] = float(value)
                            except ValueError:
                                row_data[metric] = value
                    
                    data.append(row_data)
            
            df = pd.DataFrame(data)
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ")
            return df
            
        except Exception as e:
            logger.error(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def split_data_by_site(self, ga4_data: pd.DataFrame):
        """pagePathã§ã‚µã‚¤ãƒˆåˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²"""
        try:
            # moodmarkã‚µã‚¤ãƒˆã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ/moodmark/ã§å§‹ã¾ã‚‹ãƒ‘ã‚¹ï¼‰
            moodmark_data = ga4_data[ga4_data['pagePath'].str.startswith('/moodmark/', na=False)].copy()
            
            # moodmarkgiftã‚µã‚¤ãƒˆã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ/moodmarkgift/ã§å§‹ã¾ã‚‹ãƒ‘ã‚¹ï¼‰
            moodmarkgift_data = ga4_data[ga4_data['pagePath'].str.startswith('/moodmarkgift/', na=False)].copy()
            
            logger.info(f"moodmarkãƒ‡ãƒ¼ã‚¿: {len(moodmark_data)}è¡Œ")
            logger.info(f"moodmarkgiftãƒ‡ãƒ¼ã‚¿: {len(moodmarkgift_data)}è¡Œ")
            
            return {
                'moodmark': moodmark_data,
                'moodmarkgift': moodmarkgift_data
            }
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
            return {'moodmark': pd.DataFrame(), 'moodmarkgift': pd.DataFrame()}
    
    def generate_site_summary(self, site_data: pd.DataFrame, site_name: str):
        """ã‚µã‚¤ãƒˆåˆ¥ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        try:
            if site_data.empty:
                return {
                    'total_sessions': 0,
                    'total_users': 0,
                    'total_pageviews': 0,
                    'avg_bounce_rate': 0,
                    'avg_session_duration': 0,
                    'total_conversions': 0,
                    'data_rows': 0
                }
            
            summary = {
                'total_sessions': int(site_data['sessions'].sum()) if 'sessions' in site_data.columns else 0,
                'total_users': int(site_data['totalUsers'].sum()) if 'totalUsers' in site_data.columns else 0,
                'total_pageviews': int(site_data['screenPageViews'].sum()) if 'screenPageViews' in site_data.columns else 0,
                'avg_bounce_rate': float(site_data['bounceRate'].mean()) if 'bounceRate' in site_data.columns else 0,
                'avg_session_duration': float(site_data['averageSessionDuration'].mean()) if 'averageSessionDuration' in site_data.columns else 0,
                'total_conversions': int(site_data['conversions'].sum()) if 'conversions' in site_data.columns else 0,
                'data_rows': len(site_data)
            }
            
            logger.info(f"{site_name}ã‚µãƒãƒªãƒ¼ç”Ÿæˆå®Œäº†")
            return summary
            
        except Exception as e:
            logger.error(f"{site_name}ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def get_top_organic_landing_pages(self, site_data: pd.DataFrame, site_name: str, limit: int = 10):
        """ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯é›†å®¢ã®å¼·ã„ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸TOP10ã‚’å–å¾—"""
        try:
            if site_data.empty:
                return []
            
            # ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯æ¤œç´¢ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            organic_data = site_data[
                site_data['sessionDefaultChannelGrouping'].str.contains('Organic Search', na=False)
            ].copy()
            
            if organic_data.empty:
                logger.warning(f"{site_name}: ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
            
            # ãƒšãƒ¼ã‚¸åˆ¥ã§é›†è¨ˆ
            page_stats = organic_data.groupby('pagePath').agg({
                'sessions': 'sum',
                'totalUsers': 'sum',
                'screenPageViews': 'sum',
                'bounceRate': 'mean',
                'averageSessionDuration': 'mean',
                'conversions': 'sum'
            }).reset_index()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦TOP10ã‚’å–å¾—
            top_pages = page_stats.sort_values('sessions', ascending=False).head(limit)
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            result = []
            for _, row in top_pages.iterrows():
                result.append({
                    'page_path': row['pagePath'],
                    'sessions': int(row['sessions']),
                    'users': int(row['totalUsers']),
                    'pageviews': int(row['screenPageViews']),
                    'bounce_rate': float(row['bounceRate']),
                    'avg_session_duration': float(row['averageSessionDuration']),
                    'conversions': int(row['conversions'])
                })
            
            logger.info(f"{site_name}: ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸TOP{limit}ã‚’å–å¾—å®Œäº†")
            return result
            
        except Exception as e:
            logger.error(f"{site_name}: ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def compare_organic_pages(self, current_pages: List[Dict], previous_pages: List[Dict], site_name: str):
        """ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã®å‰å¹´å¯¾æ¯”"""
        try:
            if not current_pages or not previous_pages:
                return []
            
            # å‰å¹´ã®ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸ã‚’ä½œæˆ
            previous_dict = {page['page_path']: page for page in previous_pages}
            
            comparison_result = []
            
            for current_page in current_pages:
                page_path = current_page['page_path']
                previous_page = previous_dict.get(page_path)
                
                if previous_page:
                    # å‰å¹´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                    sessions_growth = ((current_page['sessions'] - previous_page['sessions']) / previous_page['sessions'] * 100) if previous_page['sessions'] > 0 else 0
                    users_growth = ((current_page['users'] - previous_page['users']) / previous_page['users'] * 100) if previous_page['users'] > 0 else 0
                    pageviews_growth = ((current_page['pageviews'] - previous_page['pageviews']) / previous_page['pageviews'] * 100) if previous_page['pageviews'] > 0 else 0
                    
                    comparison_result.append({
                        'page_path': page_path,
                        'current': current_page,
                        'previous': previous_page,
                        'growth_rates': {
                            'sessions': sessions_growth,
                            'users': users_growth,
                            'pageviews': pageviews_growth
                        },
                        'has_previous_data': True
                    })
                else:
                    # å‰å¹´ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼ˆæ–°è¦ãƒšãƒ¼ã‚¸ï¼‰
                    comparison_result.append({
                        'page_path': page_path,
                        'current': current_page,
                        'previous': None,
                        'growth_rates': None,
                        'has_previous_data': False
                    })
            
            logger.info(f"{site_name}: ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒšãƒ¼ã‚¸å‰å¹´å¯¾æ¯”å®Œäº†")
            return comparison_result
            
        except Exception as e:
            logger.error(f"{site_name}: ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒšãƒ¼ã‚¸å‰å¹´å¯¾æ¯”ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_gsc_summary(self, gsc_data: pd.DataFrame, site_name: str):
        """GSCã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        try:
            if gsc_data.empty:
                return {
                    'total_clicks': 0,
                    'total_impressions': 0,
                    'avg_ctr': 0,
                    'avg_position': 0,
                    'top_pages_count': 0,
                    'top_queries_count': 0
                }
            
            # ãƒšãƒ¼ã‚¸åˆ¥é›†è¨ˆ
            page_stats = gsc_data.groupby('page').agg({
                'clicks': 'sum',
                'impressions': 'sum',
                'ctr': 'mean',
                'position': 'mean'
            }).reset_index()
            
            # ã‚¯ã‚¨ãƒªåˆ¥é›†è¨ˆ
            query_stats = gsc_data.groupby('query').agg({
                'clicks': 'sum',
                'impressions': 'sum',
                'ctr': 'mean',
                'position': 'mean'
            }).reset_index()
            
            summary = {
                'total_clicks': int(gsc_data['clicks'].sum()),
                'total_impressions': int(gsc_data['impressions'].sum()),
                'avg_ctr': float(gsc_data['ctr'].mean() * 100),
                'avg_position': float(gsc_data['position'].mean()),
                'top_pages_count': len(page_stats),
                'top_queries_count': len(query_stats)
            }
            
            logger.info(f"{site_name}: GSCã‚µãƒãƒªãƒ¼ç”Ÿæˆå®Œäº†")
            return summary
            
        except Exception as e:
            logger.error(f"{site_name}: GSCã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def get_top_gsc_pages(self, gsc_data: pd.DataFrame, site_name: str, limit: int = 10):
        """GSCãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        try:
            if gsc_data.empty:
                return []
            
            # ãƒšãƒ¼ã‚¸åˆ¥ã§é›†è¨ˆ
            page_stats = gsc_data.groupby('page').agg({
                'clicks': 'sum',
                'impressions': 'sum',
                'ctr': 'mean',
                'position': 'mean'
            }).reset_index()
            
            # ã‚¯ãƒªãƒƒã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦TOP10ã‚’å–å¾—
            top_pages = page_stats.sort_values('clicks', ascending=False).head(limit)
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            result = []
            for _, row in top_pages.iterrows():
                result.append({
                    'page': row['page'],
                    'clicks': int(row['clicks']),
                    'impressions': int(row['impressions']),
                    'ctr': float(row['ctr'] * 100),
                    'position': float(row['position'])
                })
            
            logger.info(f"{site_name}: GSCãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸TOP{limit}ã‚’å–å¾—å®Œäº†")
            return result
            
        except Exception as e:
            logger.error(f"{site_name}: GSCãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_top_gsc_queries(self, gsc_data: pd.DataFrame, site_name: str, limit: int = 20):
        """GSCãƒˆãƒƒãƒ—ã‚¯ã‚¨ãƒªã‚’å–å¾—"""
        try:
            if gsc_data.empty:
                return []
            
            # ã‚¯ã‚¨ãƒªåˆ¥ã§é›†è¨ˆ
            query_stats = gsc_data.groupby('query').agg({
                'clicks': 'sum',
                'impressions': 'sum',
                'ctr': 'mean',
                'position': 'mean'
            }).reset_index()
            
            # ã‚¯ãƒªãƒƒã‚¯æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦TOP20ã‚’å–å¾—
            top_queries = query_stats.sort_values('clicks', ascending=False).head(limit)
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            result = []
            for _, row in top_queries.iterrows():
                result.append({
                    'query': row['query'],
                    'clicks': int(row['clicks']),
                    'impressions': int(row['impressions']),
                    'ctr': float(row['ctr'] * 100),
                    'position': float(row['position'])
                })
            
            logger.info(f"{site_name}: GSCãƒˆãƒƒãƒ—ã‚¯ã‚¨ãƒªTOP{limit}ã‚’å–å¾—å®Œäº†")
            return result
            
        except Exception as e:
            logger.error(f"{site_name}: GSCãƒˆãƒƒãƒ—ã‚¯ã‚¨ãƒªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def compare_periods(self, current_data: Dict[str, pd.DataFrame], previous_data: Dict[str, pd.DataFrame]):
        """æœŸé–“æ¯”è¼ƒåˆ†æ"""
        try:
            comparison = {
                'current_period': '2025-10-01 - 2025-10-15',
                'previous_period': '2024-10-01 - 2024-10-15',
                'sites': {}
            }
            
            for site_name in ['moodmark', 'moodmarkgift']:
                current_site_data = current_data.get(site_name, pd.DataFrame())
                previous_site_data = previous_data.get(site_name, pd.DataFrame())
                
                if current_site_data.empty or previous_site_data.empty:
                    comparison['sites'][site_name] = {
                        'comparison_available': False,
                        'reason': 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³'
                    }
                    continue
                
                current_summary = self.generate_site_summary(current_site_data, site_name)
                previous_summary = self.generate_site_summary(previous_site_data, site_name)
                
                site_comparison = {
                    'comparison_available': True,
                    'current': current_summary,
                    'previous': previous_summary,
                    'growth_rates': {}
                }
                
                # æˆé•·ç‡ã®è¨ˆç®—
                for metric in ['total_sessions', 'total_users', 'total_pageviews', 'total_conversions']:
                    current_val = current_summary.get(metric, 0)
                    previous_val = previous_summary.get(metric, 0)
                    
                    if previous_val > 0:
                        growth_rate = ((current_val - previous_val) / previous_val) * 100
                        site_comparison['growth_rates'][metric] = {
                            'growth_rate': growth_rate,
                            'direction': 'increase' if growth_rate > 0 else 'decrease'
                        }
                
                comparison['sites'][site_name] = site_comparison
            
            return comparison
            
        except Exception as e:
            logger.error(f"æœŸé–“æ¯”è¼ƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def generate_corrected_report(self, start_date: str, end_date: str, previous_start_date: str, previous_end_date: str):
        """ä¿®æ­£ç‰ˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            logger.info("ä¿®æ­£ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
            
            # ç¾åœ¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
            current_ga4_data = self.get_ga4_data_for_period(start_date, end_date)
            if current_ga4_data.empty:
                logger.error("ç¾åœ¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
                return None
            
            # å‰å¹´åŒæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
            previous_ga4_data = self.get_ga4_data_for_period(previous_start_date, previous_end_date)
            if previous_ga4_data.empty:
                logger.warning("å‰å¹´åŒæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
                previous_ga4_data = pd.DataFrame()
            
            # ã‚µã‚¤ãƒˆåˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            current_sites_data = self.split_data_by_site(current_ga4_data)
            previous_sites_data = self.split_data_by_site(previous_ga4_data) if not previous_ga4_data.empty else {'moodmark': pd.DataFrame(), 'moodmarkgift': pd.DataFrame()}
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = {
                'report_metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'current_period': f"{start_date} - {end_date}",
                    'previous_period': f"{previous_start_date} - {previous_end_date}",
                    'sites_analyzed': ['moodmark', 'moodmarkgift'],
                    'analysis_type': 'pagePath_based_split'
                },
                'sites': {},
                'comparison_analysis': {}
            }
            
            # å„ã‚µã‚¤ãƒˆã®åˆ†æ
            for site_name, site_data in current_sites_data.items():
                site_url = f"https://isetan.mistore.jp/{site_name}/"
                
                # GSCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                current_gsc_data = self.get_gsc_data_for_period(start_date, end_date, site_url)
                
                # ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸TOP10ã‚’å–å¾—
                current_organic_pages = self.get_top_organic_landing_pages(site_data, site_name, 10)
                
                # GSCã‚µãƒãƒªãƒ¼ã¨ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ãƒ»ã‚¯ã‚¨ãƒªã‚’å–å¾—
                gsc_summary = self.get_gsc_summary(current_gsc_data, site_name)
                top_gsc_pages = self.get_top_gsc_pages(current_gsc_data, site_name, 10)
                top_gsc_queries = self.get_top_gsc_queries(current_gsc_data, site_name, 20)
                
                site_report = {
                    'site_url': site_url,
                    'period': f"{start_date} - {end_date}",
                    'ga4_summary': self.generate_site_summary(site_data, site_name),
                    'gsc_summary': gsc_summary,
                    'top_organic_landing_pages': current_organic_pages,
                    'top_gsc_pages': top_gsc_pages,
                    'top_gsc_queries': top_gsc_queries,
                    'recommendations': []
                }
                
                # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
                self._generate_recommendations(site_report)
                
                report['sites'][site_name] = site_report
                
                # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                if not site_data.empty:
                    filename = f'ga4_{site_name}_corrected_{start_date.replace("-", "")}_{end_date.replace("-", "")}.csv'
                    self.api_integration.export_to_csv(site_data, filename)
                
                if not current_gsc_data.empty:
                    filename = f'gsc_{site_name}_corrected_{start_date.replace("-", "")}_{end_date.replace("-", "")}.csv'
                    self.api_integration.export_to_csv(current_gsc_data, filename)
            
            # æœŸé–“æ¯”è¼ƒåˆ†æ
            if not previous_ga4_data.empty:
                comparison = self.compare_periods(current_sites_data, previous_sites_data)
                report['comparison_analysis'] = comparison
                
                # å„ã‚µã‚¤ãƒˆã®ãƒ¬ãƒãƒ¼ãƒˆã«æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                for site_name in ['moodmark', 'moodmarkgift']:
                    if site_name in comparison['sites']:
                        report['sites'][site_name]['year_over_year_comparison'] = comparison['sites'][site_name]
                        
                        # ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã®å‰å¹´å¯¾æ¯”ã‚’è¿½åŠ 
                        current_organic_pages = report['sites'][site_name].get('top_organic_landing_pages', [])
                        previous_organic_pages = self.get_top_organic_landing_pages(previous_sites_data.get(site_name, pd.DataFrame()), site_name, 10)
                        organic_comparison = self.compare_organic_pages(current_organic_pages, previous_organic_pages, site_name)
                        report['sites'][site_name]['organic_pages_year_over_year'] = organic_comparison
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = f'data/processed/corrected_report_{start_date.replace("-", "")}_{end_date.replace("-", "")}.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            markdown_report = self._generate_markdown_report(report)
            markdown_file = f'data/processed/corrected_report_{start_date.replace("-", "")}_{end_date.replace("-", "")}.md'
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown_report)
            
            logger.info(f"ä¿®æ­£ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
            return report
            
        except Exception as e:
            logger.error(f"ä¿®æ­£ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _generate_recommendations(self, site_report: Dict[str, Any]):
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        ga4_summary = site_report.get('ga4_summary', {})
        if ga4_summary.get('avg_bounce_rate', 0) > 0.6:
            recommendations.append({
                'type': 'performance',
                'priority': 'high',
                'message': f"ãƒã‚¦ãƒ³ã‚¹ç‡ãŒ{ga4_summary['avg_bounce_rate']:.1%}ã¨é«˜ã™ãã¾ã™ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚"
            })
        
        if ga4_summary.get('avg_session_duration', 0) < 60:
            recommendations.append({
                'type': 'engagement',
                'priority': 'medium',
                'message': f"å¹³å‡ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ãŒ{ga4_summary['avg_session_duration']:.0f}ç§’ã¨çŸ­ã™ãã¾ã™ã€‚ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‘ä¸ŠãŒå¿…è¦ã§ã™ã€‚"
            })
        
        site_report['recommendations'] = recommendations
    
    def _generate_markdown_report(self, report: Dict[str, Any]) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            metadata = report['report_metadata']
            
            markdown = f"""# ğŸ“Š MOO-D MARK ä¿®æ­£ç‰ˆæœŸé–“åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
**åˆ†ææœŸé–“**: {metadata['current_period']}
**å‰å¹´åŒæœŸé–“**: {metadata['previous_period']}
**åˆ†ææ–¹å¼**: pagePathãƒ™ãƒ¼ã‚¹ã®ã‚µã‚¤ãƒˆåˆ†å‰²

## ğŸ“ˆ åˆ†ææ¦‚è¦

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€MOO-D MARKã®2ã¤ã®ã‚µã‚¤ãƒˆã«ã¤ã„ã¦ã€pagePathã§åˆ†å‰²ã—ã¦åˆ†æã—ãŸã‚‚ã®ã§ã™ã€‚

### åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆ
- **moodmark**: https://isetan.mistore.jp/moodmark/
- **moodmarkgift**: https://isetan.mistore.jp/moodmarkgift/

## ğŸ“Š ã‚µã‚¤ãƒˆåˆ¥åˆ†æçµæœ

"""
            
            # å„ã‚µã‚¤ãƒˆã®è©³ç´°åˆ†æ
            for site_name, site_data in report['sites'].items():
                site_display_name = site_name.upper()
                markdown += f"### ğŸŒ {site_display_name}\n\n"
                markdown += f"**ã‚µã‚¤ãƒˆURL**: {site_data.get('site_url', '')}\n\n"
                
                # GA4ã‚µãƒãƒªãƒ¼
                ga4_summary = site_data.get('ga4_summary', {})
                if ga4_summary:
                    markdown += "#### ğŸ“ˆ GA4 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n\n"
                    markdown += f"- **ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°**: {ga4_summary.get('total_sessions', 0):,}\n"
                    markdown += f"- **ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°**: {ga4_summary.get('total_users', 0):,}\n"
                    markdown += f"- **ç·ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°**: {ga4_summary.get('total_pageviews', 0):,}\n"
                    markdown += f"- **å¹³å‡ãƒã‚¦ãƒ³ã‚¹ç‡**: {ga4_summary.get('avg_bounce_rate', 0):.1%}\n"
                    markdown += f"- **å¹³å‡ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“**: {ga4_summary.get('avg_session_duration', 0):.0f}ç§’\n"
                    markdown += f"- **ç·ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°**: {ga4_summary.get('total_conversions', 0):,}\n"
                    markdown += f"- **ãƒ‡ãƒ¼ã‚¿è¡Œæ•°**: {ga4_summary.get('data_rows', 0):,}\n\n"
                
                # å‰å¹´åŒæœŸé–“å¯¾æ¯”
                yoy_comparison = site_data.get('year_over_year_comparison', {})
                if yoy_comparison and yoy_comparison.get('comparison_available', False):
                    markdown += "#### ğŸ“Š å‰å¹´åŒæœŸé–“å¯¾æ¯”\n\n"
                    
                    growth_rates = yoy_comparison.get('growth_rates', {})
                    if growth_rates:
                        markdown += "**ä¸»è¦æŒ‡æ¨™ã®å¤‰åŒ–**:\n"
                        for metric, data in growth_rates.items():
                            growth_rate = data.get('growth_rate', 0)
                            direction = "ğŸ“ˆ" if growth_rate > 0 else "ğŸ“‰" if growth_rate < 0 else "â¡ï¸"
                            metric_name = {
                                'total_sessions': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°',
                                'total_users': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°',
                                'total_pageviews': 'ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°',
                                'total_conversions': 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°'
                            }.get(metric, metric)
                            markdown += f"- {metric_name}: {direction} {growth_rate:+.1f}%\n"
                        markdown += "\n"
                elif yoy_comparison and not yoy_comparison.get('comparison_available', False):
                    markdown += "#### ğŸ“Š å‰å¹´åŒæœŸé–“å¯¾æ¯”\n\n"
                    markdown += f"**æ¯”è¼ƒä¸å¯**: {yoy_comparison.get('reason', 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³')}\n\n"
                
                # ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸TOP10
                organic_pages = site_data.get('top_organic_landing_pages', [])
                if organic_pages:
                    markdown += "#### ğŸ” ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯é›†å®¢ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ TOP10\n\n"
                    markdown += "| é †ä½ | ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ | ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•° | ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•° | ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•° | ãƒã‚¦ãƒ³ã‚¹ç‡ | ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ | ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•° |\n"
                    markdown += "|------|------------|------------|------------|----------------|------------|----------------|------------------|\n"
                    
                    for i, page in enumerate(organic_pages, 1):
                        page_path = page.get('page_path', '')[:50] + "..." if len(page.get('page_path', '')) > 50 else page.get('page_path', '')
                        markdown += f"| {i} | {page_path} | {page.get('sessions', 0):,} | {page.get('users', 0):,} | {page.get('pageviews', 0):,} | {page.get('bounce_rate', 0):.1%} | {page.get('avg_session_duration', 0):.0f}ç§’ | {page.get('conversions', 0):,} |\n"
                    markdown += "\n"
                
                # ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒšãƒ¼ã‚¸ã®å‰å¹´å¯¾æ¯”
                organic_yoy = site_data.get('organic_pages_year_over_year', [])
                if organic_yoy:
                    markdown += "#### ğŸ“Š ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯ãƒšãƒ¼ã‚¸å‰å¹´å¯¾æ¯”\n\n"
                    markdown += "| ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ | ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°å¤‰åŒ– | ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°å¤‰åŒ– | ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°å¤‰åŒ– | çŠ¶æ³ |\n"
                    markdown += "|------------|------------------|----------------|-------------------|------|\n"
                    
                    for page_comparison in organic_yoy:
                        page_path = page_comparison.get('page_path', '')[:40] + "..." if len(page_comparison.get('page_path', '')) > 40 else page_comparison.get('page_path', '')
                        
                        if page_comparison.get('has_previous_data', False):
                            growth_rates = page_comparison.get('growth_rates', {})
                            sessions_growth = growth_rates.get('sessions', 0)
                            users_growth = growth_rates.get('users', 0)
                            pageviews_growth = growth_rates.get('pageviews', 0)
                            
                            sessions_emoji = "ğŸ“ˆ" if sessions_growth > 0 else "ğŸ“‰" if sessions_growth < 0 else "â¡ï¸"
                            users_emoji = "ğŸ“ˆ" if users_growth > 0 else "ğŸ“‰" if users_growth < 0 else "â¡ï¸"
                            pageviews_emoji = "ğŸ“ˆ" if pageviews_growth > 0 else "ğŸ“‰" if pageviews_growth < 0 else "â¡ï¸"
                            
                            markdown += f"| {page_path} | {sessions_emoji} {sessions_growth:+.1f}% | {users_emoji} {users_growth:+.1f}% | {pageviews_emoji} {pageviews_growth:+.1f}% | ç¶™ç¶š |\n"
                        else:
                            markdown += f"| {page_path} | ğŸ†• æ–°è¦ | ğŸ†• æ–°è¦ | ğŸ†• æ–°è¦ | æ–°è¦ãƒšãƒ¼ã‚¸ |\n"
                    markdown += "\n"
                
                # GSCã‚µãƒãƒªãƒ¼
                gsc_summary = site_data.get('gsc_summary', {})
                if gsc_summary:
                    markdown += "#### ğŸ” æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ– (SEO)\n\n"
                    markdown += f"- **ç·ã‚¯ãƒªãƒƒã‚¯æ•°**: {gsc_summary.get('total_clicks', 0):,}\n"
                    markdown += f"- **ç·ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°**: {gsc_summary.get('total_impressions', 0):,}\n"
                    markdown += f"- **å¹³å‡CTR**: {gsc_summary.get('avg_ctr', 0):.2f}%\n"
                    markdown += f"- **å¹³å‡æ¤œç´¢é †ä½**: {gsc_summary.get('avg_position', 0):.1f}ä½\n\n"
                
                # GSCãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
                top_gsc_pages = site_data.get('top_gsc_pages', [])
                if top_gsc_pages:
                    markdown += "#### ğŸ† GSCãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ (ä¸Šä½10ä»¶)\n\n"
                    markdown += "| é †ä½ | ãƒšãƒ¼ã‚¸ | ã‚¯ãƒªãƒƒã‚¯æ•° | ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•° | CTR | å¹³å‡é †ä½ |\n"
                    markdown += "|------|--------|------------|-------------------|-----|----------|\n"
                    for i, page in enumerate(top_gsc_pages, 1):
                        page_path = page.get('page', '')[:50] + "..." if len(page.get('page', '')) > 50 else page.get('page', '')
                        markdown += f"| {i} | {page_path} | {page.get('clicks', 0):,} | {page.get('impressions', 0):,} | {page.get('ctr', 0):.2f}% | {page.get('position', 0):.1f} |\n"
                    markdown += "\n"
                
                # GSCãƒˆãƒƒãƒ—ã‚¯ã‚¨ãƒª
                top_gsc_queries = site_data.get('top_gsc_queries', [])
                if top_gsc_queries:
                    markdown += "#### ğŸ” GSCãƒˆãƒƒãƒ—ã‚¯ã‚¨ãƒª (ä¸Šä½20ä»¶)\n\n"
                    markdown += "| é †ä½ | ã‚¯ã‚¨ãƒª | ã‚¯ãƒªãƒƒã‚¯æ•° | ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•° | CTR | å¹³å‡é †ä½ |\n"
                    markdown += "|------|--------|------------|-------------------|-----|----------|\n"
                    for i, query in enumerate(top_gsc_queries, 1):
                        markdown += f"| {i} | {query.get('query', '')} | {query.get('clicks', 0):,} | {query.get('impressions', 0):,} | {query.get('ctr', 0):.2f}% | {query.get('position', 0):.1f} |\n"
                    markdown += "\n"
                
                # æ¨å¥¨äº‹é …
                recommendations = site_data.get('recommendations', [])
                if recommendations:
                    markdown += "#### ğŸ’¡ æ¨å¥¨äº‹é …\n\n"
                    for rec in recommendations:
                        priority_emoji = "ğŸ”´" if rec.get('priority') == 'high' else "ğŸŸ¡" if rec.get('priority') == 'medium' else "ğŸŸ¢"
                        markdown += f"- {priority_emoji} **{rec.get('type', '').upper()}**: {rec.get('message', '')}\n"
                    markdown += "\n"
                
                markdown += "---\n\n"
            
            markdown += """## ğŸ“‹ ã¾ã¨ã‚

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€pagePathãƒ™ãƒ¼ã‚¹ã§ã‚µã‚¤ãƒˆã‚’åˆ†å‰²ã—ã¦åˆ†æã—ãŸä¿®æ­£ç‰ˆã§ã™ã€‚

### ä¸»è¦ãªç™ºè¦‹
- å„ã‚µã‚¤ãƒˆã®å®Ÿéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’æ­£ç¢ºã«åˆ†æ
- pagePathã«ã‚ˆã‚‹é©åˆ‡ãªã‚µã‚¤ãƒˆåˆ†å‰²ã«ã‚ˆã‚Šã€æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
- å‰å¹´åŒæœŸé–“ã¨ã®æ¯”è¼ƒã«ã‚ˆã‚Šæˆé•·ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŠŠæ¡

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. å„ã‚µã‚¤ãƒˆã®å€‹åˆ¥æ”¹å–„è¨ˆç”»ã®ç­–å®š
2. ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨æ”¹å–„ã®å®Ÿæ–½
3. å®šæœŸçš„ãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«ã‚ˆã‚‹é€²æ—ç®¡ç†

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ä¿®æ­£ç‰ˆã¨ã—ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚è©³ç´°ãªãƒ‡ãƒ¼ã‚¿ã¯æ·»ä»˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚*
"""
            
            return markdown
            
        except Exception as e:
            logger.error(f"Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return "ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=== ä¿®æ­£ç‰ˆæœŸé–“åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ  ===")
    
    # åˆ†ææœŸé–“ã®è¨­å®š
    current_start_date = "2025-10-01"
    current_end_date = "2025-10-15"
    previous_start_date = "2024-10-01"
    previous_end_date = "2024-10-15"
    
    print(f"åˆ†ææœŸé–“: {current_start_date} - {current_end_date}")
    print(f"å‰å¹´åŒæœŸé–“: {previous_start_date} - {previous_end_date}")
    print("åˆ†ææ–¹å¼: pagePathãƒ™ãƒ¼ã‚¹ã®ã‚µã‚¤ãƒˆåˆ†å‰²")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    analyzer = CorrectedPeriodAnalysis()
    
    # ä¿®æ­£ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_corrected_report(
        current_start_date, current_end_date,
        previous_start_date, previous_end_date
    )
    
    if report:
        print("\n=== ä¿®æ­£ç‰ˆåˆ†æå®Œäº† ===")
        print(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: data/processed/corrected_report_{current_start_date.replace('-', '')}_{current_end_date.replace('-', '')}.json")
        print(f"Markdownãƒ¬ãƒãƒ¼ãƒˆ: data/processed/corrected_report_{current_start_date.replace('-', '')}_{current_end_date.replace('-', '')}.md")
        
        # ç°¡å˜ãªã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n=== ä¿®æ­£ç‰ˆåˆ†æã‚µãƒãƒªãƒ¼ ===")
        for site_name, site_data in report['sites'].items():
            print(f"\nğŸŒ {site_name.upper()}")
            ga4_summary = site_data.get('ga4_summary', {})
            
            if ga4_summary:
                print(f"  ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {ga4_summary.get('total_sessions', 0):,}")
                print(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {ga4_summary.get('total_users', 0):,}")
                print(f"  ãƒšãƒ¼ã‚¸ãƒ“ãƒ¥ãƒ¼æ•°: {ga4_summary.get('total_pageviews', 0):,}")
                print(f"  ãƒã‚¦ãƒ³ã‚¹ç‡: {ga4_summary.get('avg_bounce_rate', 0):.1%}")
                print(f"  ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“: {ga4_summary.get('avg_session_duration', 0):.0f}ç§’")
                print(f"  ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°: {ga4_summary.get('total_conversions', 0):,}")
                print(f"  ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {ga4_summary.get('data_rows', 0):,}")
    else:
        print("ä¿®æ­£ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
