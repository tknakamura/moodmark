#!/usr/bin/env python3
"""
ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ„ãƒ¼ãƒ«
- ãƒšãƒ¼ã‚¸åˆ¥ã®ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ç®—å‡º
- æµå…¥ãƒãƒ£ãƒãƒ«åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
- é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒšãƒ¼ã‚¸ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º
- æ”¹å–„ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã®ç‰¹å®šï¼ˆé«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ»ä½CVRï¼‰
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import logging

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¾ãŸã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
try:
    from .oauth_google_apis import OAuthGoogleAPIsIntegration
except ImportError:
    from oauth_google_apis import OAuthGoogleAPIsIntegration

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/content_performance_analyzer.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ContentPerformanceAnalyzer:
    def __init__(self):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ„ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        self.api_integration = OAuthGoogleAPIsIntegration()
        self.config = self._load_config()
        
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs('logs', exist_ok=True)
        os.makedirs('data/processed', exist_ok=True)
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        config_file = 'config/analytics_config.json'
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_file}")
                return {}
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def get_ga4_data_for_period(self, start_date: str, end_date: str):
        """æœŸé–“æŒ‡å®šã§GA4ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            if not self.api_integration.ga4_service:
                logger.warning("GA4ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return pd.DataFrame()
            
            # GA4ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            request = {
                'dateRanges': [{'startDate': start_date, 'endDate': end_date}],
                'dimensions': [
                    {'name': 'pagePath'},
                    {'name': 'sessionDefaultChannelGrouping'},
                    {'name': 'deviceCategory'},
                    {'name': 'country'}
                ],
                'metrics': [
                    {'name': 'sessions'},
                    {'name': 'totalUsers'},
                    {'name': 'screenPageViews'},
                    {'name': 'bounceRate'},
                    {'name': 'averageSessionDuration'},
                    {'name': 'conversions'},
                    {'name': 'newUsers'},
                    {'name': 'engagedSessions'}
                ],
                'limit': 10000
            }
            
            # APIå‘¼ã³å‡ºã—
            response = self.api_integration.ga4_service.properties().runReport(
                property=f"properties/{self.config.get('sites', {}).get('moodmark', {}).get('ga4_property_id', '')}",
                body=request
            ).execute()
            
            # ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
            data = []
            if 'rows' in response:
                for row in response['rows']:
                    row_data = {}
                    
                    # ãƒ‡ã‚£ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³
                    for i, dimension in enumerate(request['dimensions']):
                        row_data[dimension['name']] = row['dimensionValues'][i]['value']
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                    for i, metric in enumerate(request['metrics']):
                        row_data[metric['name']] = float(row['metricValues'][i]['value'])
                    
                    data.append(row_data)
            
            df = pd.DataFrame(data)
            logger.info(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}è¡Œ")
            return df
            
        except Exception as e:
            logger.error(f"GA4ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def segment_data_by_site(self, ga4_data: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """pagePathã§ã‚µã‚¤ãƒˆã‚’åˆ†å‰²"""
        try:
            if ga4_data.empty:
                return {'moodmark': pd.DataFrame(), 'moodmarkgift': pd.DataFrame()}
            
            # moodmarkãƒ‡ãƒ¼ã‚¿ï¼ˆ/moodmark/ã§å§‹ã¾ã‚‹ãƒ‘ã‚¹ï¼‰
            moodmark_data = ga4_data[ga4_data['pagePath'].str.startswith('/moodmark/', na=False)].copy()
            
            # moodmarkgiftãƒ‡ãƒ¼ã‚¿ï¼ˆ/moodmarkgift/ã§å§‹ã¾ã‚‹ãƒ‘ã‚¹ï¼‰
            moodmarkgift_data = ga4_data[ga4_data['pagePath'].str.startswith('/moodmarkgift/', na=False)].copy()
            
            logger.info(f"moodmarkãƒ‡ãƒ¼ã‚¿: {len(moodmark_data)}è¡Œ")
            logger.info(f"moodmarkgiftãƒ‡ãƒ¼ã‚¿: {len(moodmarkgift_data)}è¡Œ")
            
            return {
                'moodmark': moodmark_data,
                'moodmarkgift': moodmarkgift_data
            }
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
            return {'moodmark': pd.DataFrame(), 'moodmarkgift': pd.DataFrame()}
    
    def calculate_page_conversion_rates(self, site_data: pd.DataFrame, site_name: str) -> pd.DataFrame:
        """ãƒšãƒ¼ã‚¸åˆ¥ã®ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ç®—å‡º"""
        try:
            if site_data.empty:
                return pd.DataFrame()
            
            # ãƒšãƒ¼ã‚¸åˆ¥ã§é›†è¨ˆ
            page_stats = site_data.groupby('pagePath').agg({
                'sessions': 'sum',
                'totalUsers': 'sum',
                'screenPageViews': 'sum',
                'bounceRate': 'mean',
                'averageSessionDuration': 'mean',
                'conversions': 'sum',
                'newUsers': 'sum',
                'engagedSessions': 'sum'
            }).reset_index()
            
            # ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã‚’è¨ˆç®—
            page_stats['conversion_rate'] = (page_stats['conversions'] / page_stats['sessions'] * 100).fillna(0)
            page_stats['engagement_rate'] = (page_stats['engagedSessions'] / page_stats['sessions'] * 100).fillna(0)
            page_stats['new_user_rate'] = (page_stats['newUsers'] / page_stats['totalUsers'] * 100).fillna(0)
            
            # æœ€å°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            page_stats = page_stats[page_stats['sessions'] >= 10]
            
            # ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã§ã‚½ãƒ¼ãƒˆ
            page_stats = page_stats.sort_values('conversion_rate', ascending=False)
            
            logger.info(f"{site_name}: ãƒšãƒ¼ã‚¸åˆ¥CVRåˆ†æå®Œäº† - {len(page_stats)}ãƒšãƒ¼ã‚¸")
            return page_stats
            
        except Exception as e:
            logger.error(f"{site_name}: ãƒšãƒ¼ã‚¸åˆ¥CVRåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    
    def analyze_channel_performance(self, site_data: pd.DataFrame, site_name: str) -> Dict:
        """æµå…¥ãƒãƒ£ãƒãƒ«åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
        try:
            if site_data.empty:
                return {}
            
            # ãƒãƒ£ãƒãƒ«åˆ¥ã§é›†è¨ˆ
            channel_stats = site_data.groupby('sessionDefaultChannelGrouping').agg({
                'sessions': 'sum',
                'totalUsers': 'sum',
                'screenPageViews': 'sum',
                'bounceRate': 'mean',
                'averageSessionDuration': 'mean',
                'conversions': 'sum',
                'newUsers': 'sum',
                'engagedSessions': 'sum'
            }).reset_index()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            channel_stats['conversion_rate'] = (channel_stats['conversions'] / channel_stats['sessions'] * 100).fillna(0)
            channel_stats['engagement_rate'] = (channel_stats['engagedSessions'] / channel_stats['sessions'] * 100).fillna(0)
            channel_stats['new_user_rate'] = (channel_stats['newUsers'] / channel_stats['totalUsers'] * 100).fillna(0)
            channel_stats['pages_per_session'] = channel_stats['screenPageViews'] / channel_stats['sessions']
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆ
            channel_stats = channel_stats.sort_values('sessions', ascending=False)
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            result = []
            for _, row in channel_stats.iterrows():
                result.append({
                    'channel': row['sessionDefaultChannelGrouping'],
                    'sessions': int(row['sessions']),
                    'users': int(row['totalUsers']),
                    'pageviews': int(row['screenPageViews']),
                    'bounce_rate': round(row['bounceRate'] * 100, 2),
                    'avg_session_duration': round(row['averageSessionDuration'], 1),
                    'conversions': int(row['conversions']),
                    'conversion_rate': round(row['conversion_rate'], 2),
                    'engagement_rate': round(row['engagement_rate'], 2),
                    'new_user_rate': round(row['new_user_rate'], 2),
                    'pages_per_session': round(row['pages_per_session'], 2)
                })
            
            logger.info(f"{site_name}: ãƒãƒ£ãƒãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Œäº† - {len(result)}ãƒãƒ£ãƒãƒ«")
            return {'channels': result}
            
        except Exception as e:
            logger.error(f"{site_name}: ãƒãƒ£ãƒãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def identify_high_performance_patterns(self, page_stats: pd.DataFrame, site_name: str) -> Dict:
        """é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒšãƒ¼ã‚¸ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""
        try:
            if page_stats.empty:
                return {}
            
            # é«˜CVRãƒšãƒ¼ã‚¸ã®å®šç¾©ï¼ˆä¸Šä½20%ã¾ãŸã¯CVR 5%ä»¥ä¸Šï¼‰
            high_cvr_threshold = max(page_stats['conversion_rate'].quantile(0.8), 5.0)
            high_cvr_pages = page_stats[page_stats['conversion_rate'] >= high_cvr_threshold].copy()
            
            if high_cvr_pages.empty:
                logger.warning(f"{site_name}: é«˜CVRãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {}
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            patterns = {
                'high_cvr_pages_count': len(high_cvr_pages),
                'avg_cvr_high_performers': round(high_cvr_pages['conversion_rate'].mean(), 2),
                'avg_sessions_high_performers': round(high_cvr_pages['sessions'].mean(), 0),
                'avg_bounce_rate_high_performers': round(high_cvr_pages['bounceRate'].mean() * 100, 2),
                'avg_session_duration_high_performers': round(high_cvr_pages['averageSessionDuration'].mean(), 1),
                'common_path_patterns': self._analyze_path_patterns(high_cvr_pages),
                'performance_insights': self._generate_performance_insights(high_cvr_pages)
            }
            
            logger.info(f"{site_name}: é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†")
            return patterns
            
        except Exception as e:
            logger.error(f"{site_name}: é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _analyze_path_patterns(self, high_cvr_pages: pd.DataFrame) -> List[Dict]:
        """ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        patterns = []
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆ†æ
        categories = {}
        for _, row in high_cvr_pages.iterrows():
            path = row['pagePath']
            cvr = row['conversion_rate']
            
            # ã‚«ãƒ†ã‚´ãƒªã®æŠ½å‡º
            if '/beauty/' in path:
                category = 'beauty'
            elif '/wedding/' in path:
                category = 'wedding'
            elif '/birthday/' in path:
                category = 'birthday'
            elif '/christmas/' in path:
                category = 'christmas'
            elif '/mombaby/' in path:
                category = 'mombaby'
            elif '/temiyage/' in path:
                category = 'temiyage'
            elif '/foods-drink/' in path:
                category = 'foods-drink'
            else:
                category = 'other'
            
            if category not in categories:
                categories[category] = []
            categories[category].append(cvr)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¹³å‡CVRã‚’è¨ˆç®—
        for category, cvrs in categories.items():
            if len(cvrs) >= 2:  # 2ãƒšãƒ¼ã‚¸ä»¥ä¸Šã‚ã‚‹ã‚«ãƒ†ã‚´ãƒªã®ã¿
                patterns.append({
                    'category': category,
                    'page_count': len(cvrs),
                    'avg_cvr': round(sum(cvrs) / len(cvrs), 2),
                    'max_cvr': round(max(cvrs), 2),
                    'pattern_type': 'category_performance'
                })
        
        return sorted(patterns, key=lambda x: x['avg_cvr'], reverse=True)
    
    def _generate_performance_insights(self, high_cvr_pages: pd.DataFrame) -> List[str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®ç”Ÿæˆ"""
        insights = []
        
        # ãƒã‚¦ãƒ³ã‚¹ç‡ã®åˆ†æ
        avg_bounce_rate = high_cvr_pages['bounceRate'].mean() * 100
        if avg_bounce_rate < 20:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯ä½ãƒã‚¦ãƒ³ã‚¹ç‡ï¼ˆ20%æœªæº€ï¼‰ã®å‚¾å‘")
        elif avg_bounce_rate < 40:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯ä¸­ç¨‹åº¦ã®ãƒã‚¦ãƒ³ã‚¹ç‡ï¼ˆ20-40%ï¼‰")
        else:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã§ã‚‚ãƒã‚¦ãƒ³ã‚¹ç‡ãŒé«˜ã„ï¼ˆ40%ä»¥ä¸Šï¼‰")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ã®åˆ†æ
        avg_duration = high_cvr_pages['averageSessionDuration'].mean()
        if avg_duration > 120:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯é•·ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ï¼ˆ2åˆ†ä»¥ä¸Šï¼‰")
        elif avg_duration > 60:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯ä¸­ç¨‹åº¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ï¼ˆ1-2åˆ†ï¼‰")
        else:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯çŸ­ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ï¼ˆ1åˆ†æœªæº€ï¼‰")
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã®åˆ†æ
        avg_sessions = high_cvr_pages['sessions'].mean()
        if avg_sessions > 1000:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯é«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ï¼ˆ1,000ã‚»ãƒƒã‚·ãƒ§ãƒ³ä»¥ä¸Šï¼‰")
        elif avg_sessions > 100:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯ä¸­ç¨‹åº¦ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ï¼ˆ100-1,000ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰")
        else:
            insights.append("é«˜CVRãƒšãƒ¼ã‚¸ã¯ä½ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ï¼ˆ100ã‚»ãƒƒã‚·ãƒ§ãƒ³æœªæº€ï¼‰")
        
        return insights
    
    def identify_improvement_opportunities(self, page_stats: pd.DataFrame, site_name: str) -> Dict:
        """æ”¹å–„ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã®ç‰¹å®šï¼ˆ4è±¡é™åˆ†æï¼‰"""
        try:
            if page_stats.empty:
                return {}
            
            # é–¾å€¤ã®è¨­å®š
            high_traffic_threshold = page_stats['sessions'].quantile(0.7)  # ä¸Šä½30%
            high_cvr_threshold = page_stats['conversion_rate'].quantile(0.7)  # ä¸Šä½30%
            
            # 4è±¡é™ã«åˆ†é¡
            opportunities = {
                'high_priority': [],  # é«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ»ä½CVR
                'reinforce': [],      # é«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ»é«˜CVR
                'maintain': [],       # ä½ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ»é«˜CVR
                'low_priority': []    # ä½ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ»ä½CVR
            }
            
            for _, row in page_stats.iterrows():
                page_info = {
                    'page_path': row['pagePath'],
                    'sessions': int(row['sessions']),
                    'conversion_rate': round(row['conversion_rate'], 2),
                    'conversions': int(row['conversions']),
                    'bounce_rate': round(row['bounceRate'] * 100, 2),
                    'avg_session_duration': round(row['averageSessionDuration'], 1)
                }
                
                is_high_traffic = row['sessions'] >= high_traffic_threshold
                is_high_cvr = row['conversion_rate'] >= high_cvr_threshold
                
                if is_high_traffic and not is_high_cvr:
                    opportunities['high_priority'].append(page_info)
                elif is_high_traffic and is_high_cvr:
                    opportunities['reinforce'].append(page_info)
                elif not is_high_traffic and is_high_cvr:
                    opportunities['maintain'].append(page_info)
                else:
                    opportunities['low_priority'].append(page_info)
            
            # å„ã‚«ãƒ†ã‚´ãƒªã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã§ã‚½ãƒ¼ãƒˆ
            for category in opportunities:
                opportunities[category].sort(key=lambda x: x['sessions'], reverse=True)
            
            # æ”¹å–„ææ¡ˆã®ç”Ÿæˆ
            opportunities['improvement_suggestions'] = self._generate_improvement_suggestions(opportunities)
            
            logger.info(f"{site_name}: æ”¹å–„æ©Ÿä¼šåˆ†æå®Œäº†")
            logger.info(f"  - æœ€å„ªå…ˆæ”¹å–„: {len(opportunities['high_priority'])}ãƒšãƒ¼ã‚¸")
            logger.info(f"  - å¼·åŒ–æ¨å¥¨: {len(opportunities['reinforce'])}ãƒšãƒ¼ã‚¸")
            logger.info(f"  - ç¶­æŒ: {len(opportunities['maintain'])}ãƒšãƒ¼ã‚¸")
            logger.info(f"  - ä½å„ªå…ˆ: {len(opportunities['low_priority'])}ãƒšãƒ¼ã‚¸")
            
            return opportunities
            
        except Exception as e:
            logger.error(f"{site_name}: æ”¹å–„æ©Ÿä¼šåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _generate_improvement_suggestions(self, opportunities: Dict) -> List[Dict]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        # æœ€å„ªå…ˆæ”¹å–„ãƒšãƒ¼ã‚¸ã®ææ¡ˆ
        for page in opportunities['high_priority'][:5]:
            suggestions.append({
                'page_path': page['page_path'],
                'priority': 'high',
                'current_cvr': page['conversion_rate'],
                'sessions': page['sessions'],
                'potential_impact': f"+{int(page['sessions'] * 0.02)} CV/æœˆï¼ˆCVR 2%æ”¹å–„æ™‚ï¼‰",
                'suggested_actions': [
                    "CTAãƒœã‚¿ãƒ³ã®æœ€é©åŒ–",
                    "ãƒšãƒ¼ã‚¸é€Ÿåº¦ã®æ”¹å–„",
                    "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å……å®Ÿ",
                    "ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š"
                ]
            })
        
        # å¼·åŒ–æ¨å¥¨ãƒšãƒ¼ã‚¸ã®ææ¡ˆ
        for page in opportunities['reinforce'][:3]:
            suggestions.append({
                'page_path': page['page_path'],
                'priority': 'medium',
                'current_cvr': page['conversion_rate'],
                'sessions': page['sessions'],
                'potential_impact': f"æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä»–ãƒšãƒ¼ã‚¸ã¸ã®å±•é–‹",
                'suggested_actions': [
                    "æˆåŠŸè¦å› ã®åˆ†æ",
                    "ä»–ãƒšãƒ¼ã‚¸ã¸ã®æ¨ªå±•é–‹",
                    "ã•ã‚‰ãªã‚‹æœ€é©åŒ–"
                ]
            })
        
        return suggestions
    
    def generate_content_performance_report(self, start_date: str, end_date: str, min_sessions: int = 10):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            logger.info("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
            
            # GA4ãƒ‡ãƒ¼ã‚¿å–å¾—
            ga4_data = self.get_ga4_data_for_period(start_date, end_date)
            if ga4_data.empty:
                logger.error("GA4ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return
            
            # ã‚µã‚¤ãƒˆåˆ¥ã«åˆ†å‰²
            sites_data = self.segment_data_by_site(ga4_data)
            
            report = {
                'report_metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'analysis_period': f"{start_date} - {end_date}",
                    'min_sessions_filter': min_sessions,
                    'analysis_type': 'content_performance_analyzer'
                },
                'sites': {}
            }
            
            # å„ã‚µã‚¤ãƒˆã®åˆ†æ
            for site_name, site_data in sites_data.items():
                if site_data.empty:
                    continue
                
                logger.info(f"{site_name}ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æé–‹å§‹")
                
                # ãƒšãƒ¼ã‚¸åˆ¥CVRåˆ†æ
                page_stats = self.calculate_page_conversion_rates(site_data, site_name)
                
                # ãƒãƒ£ãƒãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
                channel_performance = self.analyze_channel_performance(site_data, site_name)
                
                # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
                high_performance_patterns = self.identify_high_performance_patterns(page_stats, site_name)
                
                # æ”¹å–„æ©Ÿä¼šåˆ†æ
                improvement_opportunities = self.identify_improvement_opportunities(page_stats, site_name)
                
                site_report = {
                    'site_name': site_name,
                    'total_pages_analyzed': len(page_stats),
                    'page_performance': page_stats.to_dict('records') if not page_stats.empty else [],
                    'channel_performance': channel_performance,
                    'high_performance_patterns': high_performance_patterns,
                    'improvement_opportunities': improvement_opportunities,
                    'summary_metrics': {
                        'avg_conversion_rate': round(page_stats['conversion_rate'].mean(), 2) if not page_stats.empty else 0,
                        'total_sessions': int(site_data['sessions'].sum()),
                        'total_conversions': int(site_data['conversions'].sum()),
                        'high_cvr_pages_count': len(page_stats[page_stats['conversion_rate'] >= 5.0]) if not page_stats.empty else 0
                    }
                }
                
                report['sites'][site_name] = site_report
                
                # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                if not page_stats.empty:
                    filename = f'content_performance_{site_name}_{start_date.replace("-", "")}_{end_date.replace("-", "")}.csv'
                    self.api_integration.export_to_csv(page_stats, filename)
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = f'data/processed/content_performance_{start_date.replace("-", "")}_{end_date.replace("-", "")}.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._generate_markdown_report(report, start_date, end_date)
            
            logger.info(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
            
        except Exception as e:
            logger.error(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _generate_markdown_report(self, report: Dict, start_date: str, end_date: str):
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            markdown = f"""# ğŸ“Š ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
**åˆ†ææœŸé–“**: {start_date} - {end_date}
**åˆ†ææ–¹å¼**: GA4 APIé€£æºã«ã‚ˆã‚‹åŒ…æ‹¬çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ

## ğŸ“ˆ åˆ†ææ¦‚è¦

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€GA4ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æã—ã€æ”¹å–„æ©Ÿä¼šã‚’ç‰¹å®šã—ãŸã‚‚ã®ã§ã™ã€‚

### åˆ†æå¯¾è±¡ã‚µã‚¤ãƒˆ
"""
            
            for site_name, site_data in report['sites'].items():
                summary = site_data.get('summary_metrics', {})
                markdown += f"- **{site_name}**: {summary.get('total_pages_analyzed', 0)}ãƒšãƒ¼ã‚¸åˆ†æ\n"
                markdown += f"  - ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {summary.get('total_sessions', 0):,}\n"
                markdown += f"  - ç·ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°: {summary.get('total_conversions', 0):,}\n"
                markdown += f"  - å¹³å‡CVR: {summary.get('avg_conversion_rate', 0):.2f}%\n"
                markdown += f"  - é«˜CVRãƒšãƒ¼ã‚¸æ•°: {summary.get('high_cvr_pages_count', 0)}ãƒšãƒ¼ã‚¸\n\n"
            
            # å„ã‚µã‚¤ãƒˆã®è©³ç´°åˆ†æ
            for site_name, site_data in report['sites'].items():
                markdown += f"## ğŸŒ {site_name.upper()}\n\n"
                
                # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                summary = site_data.get('summary_metrics', {})
                markdown += f"### ğŸ“Š ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹\n\n"
                markdown += f"- **åˆ†æãƒšãƒ¼ã‚¸æ•°**: {site_data.get('total_pages_analyzed', 0)}ãƒšãƒ¼ã‚¸\n"
                markdown += f"- **å¹³å‡CVR**: {summary.get('avg_conversion_rate', 0):.2f}%\n"
                markdown += f"- **é«˜CVRãƒšãƒ¼ã‚¸æ•°**: {summary.get('high_cvr_pages_count', 0)}ãƒšãƒ¼ã‚¸\n\n"
                
                # ãƒãƒ£ãƒãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
                channel_perf = site_data.get('channel_performance', {})
                if channel_perf.get('channels'):
                    markdown += "### ğŸ”„ æµå…¥ãƒãƒ£ãƒãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n\n"
                    markdown += "| ãƒãƒ£ãƒãƒ« | ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•° | CVR | ãƒã‚¦ãƒ³ã‚¹ç‡ | ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ | ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ |\n"
                    markdown += "|----------|------------|-----|------------|----------------|------------------|\n"
                    
                    for channel in channel_perf['channels'][:10]:
                        markdown += f"| {channel['channel']} | {channel['sessions']:,} | {channel['conversion_rate']}% | {channel['bounce_rate']}% | {channel['avg_session_duration']}ç§’ | {channel['engagement_rate']}% |\n"
                    markdown += "\n"
                
                # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
                patterns = site_data.get('high_performance_patterns', {})
                if patterns.get('high_cvr_pages_count', 0) > 0:
                    markdown += "### ğŸ† é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³\n\n"
                    markdown += f"- **é«˜CVRãƒšãƒ¼ã‚¸æ•°**: {patterns.get('high_cvr_pages_count', 0)}ãƒšãƒ¼ã‚¸\n"
                    markdown += f"- **å¹³å‡CVR**: {patterns.get('avg_cvr_high_performers', 0):.2f}%\n"
                    markdown += f"- **å¹³å‡ãƒã‚¦ãƒ³ã‚¹ç‡**: {patterns.get('avg_bounce_rate_high_performers', 0):.2f}%\n"
                    markdown += f"- **å¹³å‡ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“**: {patterns.get('avg_session_duration_high_performers', 0):.1f}ç§’\n\n"
                    
                    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
                    if patterns.get('common_path_patterns'):
                        markdown += "#### ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n\n"
                        markdown += "| ã‚«ãƒ†ã‚´ãƒª | ãƒšãƒ¼ã‚¸æ•° | å¹³å‡CVR | æœ€é«˜CVR |\n"
                        markdown += "|----------|----------|---------|----------|\n"
                        
                        for pattern in patterns['common_path_patterns'][:5]:
                            markdown += f"| {pattern['category']} | {pattern['page_count']} | {pattern['avg_cvr']}% | {pattern['max_cvr']}% |\n"
                        markdown += "\n"
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
                    if patterns.get('performance_insights'):
                        markdown += "#### ğŸ’¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ\n\n"
                        for insight in patterns['performance_insights']:
                            markdown += f"- {insight}\n"
                        markdown += "\n"
                
                # æ”¹å–„æ©Ÿä¼š
                opportunities = site_data.get('improvement_opportunities', {})
                if opportunities.get('high_priority'):
                    markdown += "### ğŸ¯ æ”¹å–„æ©Ÿä¼šåˆ†æ\n\n"
                    markdown += f"#### ğŸ”´ æœ€å„ªå…ˆæ”¹å–„ï¼ˆé«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ»ä½CVRï¼‰\n\n"
                    markdown += "| ãƒšãƒ¼ã‚¸ãƒ‘ã‚¹ | ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•° | ç¾åœ¨CVR | æ”¹å–„ä½™åœ° |\n"
                    markdown += "|------------|------------|---------|----------|\n"
                    
                    for page in opportunities['high_priority'][:10]:
                        improvement_potential = f"+{int(page['sessions'] * 0.02)} CV/æœˆ"
                        markdown += f"| {page['page_path'][:50]}... | {page['sessions']:,} | {page['conversion_rate']}% | {improvement_potential} |\n"
                    markdown += "\n"
                
                if opportunities.get('improvement_suggestions'):
                    markdown += "#### ğŸ’¡ æ”¹å–„ææ¡ˆ\n\n"
                    for suggestion in opportunities['improvement_suggestions'][:5]:
                        priority_emoji = "ğŸ”´" if suggestion['priority'] == 'high' else "ğŸŸ¡"
                        markdown += f"{priority_emoji} **{suggestion['page_path'][:40]}...**\n"
                        markdown += f"   - ç¾åœ¨CVR: {suggestion['current_cvr']}%\n"
                        markdown += f"   - æœŸå¾…åŠ¹æœ: {suggestion['potential_impact']}\n"
                        markdown += f"   - æ¨å¥¨æ–½ç­–: {', '.join(suggestion['suggested_actions'][:2])}\n\n"
                
                markdown += "---\n\n"
            
            markdown += """## ğŸ“‹ ã¾ã¨ã‚

### ä¸»è¦ãªç™ºè¦‹
- é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒšãƒ¼ã‚¸ã®å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
- æ”¹å–„ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã®å„ªå…ˆé †ä½ã‚’æ˜ç¢ºåŒ–
- æµå…¥ãƒãƒ£ãƒãƒ«åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®ã‚’å¯è¦–åŒ–

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. æœ€å„ªå…ˆæ”¹å–„ãƒšãƒ¼ã‚¸ã®CVRå‘ä¸Šæ–½ç­–å®Ÿæ–½
2. é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä»–ãƒšãƒ¼ã‚¸ã¸ã®å±•é–‹
3. ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒãƒ«ã®æ”¹å–„
4. å®šæœŸçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨æ”¹å–„

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ„ãƒ¼ãƒ«ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            markdown_file = f'data/processed/content_performance_{start_date.replace("-", "")}_{end_date.replace("-", "")}.md'
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown)
            
            logger.info(f"Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {markdown_file}")
            
        except Exception as e:
            logger.error(f"Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--start-date', required=True, help='åˆ†æé–‹å§‹æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--end-date', required=True, help='åˆ†æçµ‚äº†æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--min-sessions', type=int, default=10, help='æœ€å°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼')
    
    args = parser.parse_args()
    
    print("=== ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ„ãƒ¼ãƒ« ===")
    print(f"åˆ†ææœŸé–“: {args.start_date} - {args.end_date}")
    print(f"æœ€å°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {args.min_sessions}")
    print()
    
    analyzer = ContentPerformanceAnalyzer()
    analyzer.generate_content_performance_report(
        start_date=args.start_date,
        end_date=args.end_date,
        min_sessions=args.min_sessions
    )
    
    print("=== ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Œäº† ===")
    print(f"ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: data/processed/content_performance_{args.start_date.replace('-', '')}_{args.end_date.replace('-', '')}.json")
    print(f"Markdownãƒ¬ãƒãƒ¼ãƒˆ: data/processed/content_performance_{args.start_date.replace('-', '')}_{args.end_date.replace('-', '')}.md")

if __name__ == "__main__":
    main()







