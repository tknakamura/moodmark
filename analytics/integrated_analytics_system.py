#!/usr/bin/env python3
"""
çµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ 
- Google APIsçµ±åˆ
- Looker Studioé€£æº
- è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
"""

import os
import json
import schedule
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import pandas as pd
import logging

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¾ãŸã¯çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦ã¿ã‚‹
try:
    from .google_apis_integration import GoogleAPIsIntegration
    from .looker_studio_connector import LookerStudioConnector
    from .notion_integration import NotionIntegration
    from .notion_report_converter import NotionReportConverter
except ImportError:
    from google_apis_integration import GoogleAPIsIntegration
    from looker_studio_connector import LookerStudioConnector
    from notion_integration import NotionIntegration
    from notion_report_converter import NotionReportConverter

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/analytics_system.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IntegratedAnalyticsSystem:
    def __init__(self):
        """çµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        self.api_integration = GoogleAPIsIntegration()
        self.looker_connector = LookerStudioConnector()
        self.notion_integration = None
        self.notion_converter = None
        self.is_running = False
        
        # è¨­å®šã®èª­ã¿è¾¼ã¿
        self.config = self._load_config()
        
        # Notionçµ±åˆã®åˆæœŸåŒ–
        self._initialize_notion_integration()
        
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        os.makedirs('logs', exist_ok=True)
        os.makedirs('data/processed', exist_ok=True)
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        config_file = 'config/analytics_config.json'
        
        default_config = {
            'data_collection': {
                'ga4_date_range_days': 30,
                'gsc_date_range_days': 30,
                'top_pages_limit': 100,
                'top_queries_limit': 100
            },
            'reporting': {
                'auto_report_enabled': True,
                'report_frequency': 'daily',
                'looker_studio_enabled': True
            },
            'alerts': {
                'performance_threshold': {
                    'bounce_rate': 0.7,
                    'avg_position': 10,
                    'ctr': 0.02
                },
                'email_notifications': False
            },
            'notion': {
                'enabled': True,
                'auto_sync': True,
                'sync_after_report_generation': True,
                'create_database_if_missing': True
            }
        }
        
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¨ãƒãƒ¼ã‚¸
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä¿å­˜
                os.makedirs('config', exist_ok=True)
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, ensure_ascii=False, indent=2)
                return default_config
        except Exception as e:
            logger.error(f"è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return default_config
    
    def _initialize_notion_integration(self):
        """Notionçµ±åˆã®åˆæœŸåŒ–"""
        try:
            if not self.config.get('notion', {}).get('enabled', False):
                logger.info("Notionçµ±åˆãŒç„¡åŠ¹ã§ã™")
                return
            
            # Notionçµ±åˆã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
            self.notion_integration = NotionIntegration()
            self.notion_converter = NotionReportConverter()
            
            if self.notion_integration.client:
                logger.info("Notionçµ±åˆãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç¢ºèªãƒ»ä½œæˆ
                if (self.config.get('notion', {}).get('create_database_if_missing', False) and
                    not self.notion_integration.database_id):
                    
                    database_id = self.notion_integration.create_analytics_database()
                    if database_id:
                        logger.info(f"æ–°ã—ã„Analyticsãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆã—ã¾ã—ãŸ: {database_id}")
            else:
                logger.warning("Notionèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Notioné€£æºã¯ç„¡åŠ¹ã§ã™ã€‚")
                self.notion_integration = None
                
        except Exception as e:
            logger.error(f"Notionçµ±åˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.notion_integration = None
            self.notion_converter = None
    
    def collect_data(self):
        """ãƒ‡ãƒ¼ã‚¿åé›†ã®å®Ÿè¡Œ"""
        try:
            logger.info("ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
            
            # GA4ãƒ‡ãƒ¼ã‚¿å–å¾—
            ga4_data = self.api_integration.get_ga4_data(
                date_range_days=self.config['data_collection']['ga4_date_range_days']
            )
            
            # GSCãƒšãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿å–å¾—
            gsc_pages = self.api_integration.get_top_pages_gsc(
                date_range_days=self.config['data_collection']['gsc_date_range_days'],
                limit=self.config['data_collection']['top_pages_limit']
            )
            
            # GSCã‚¯ã‚¨ãƒªãƒ‡ãƒ¼ã‚¿å–å¾—
            gsc_queries = self.api_integration.get_top_queries_gsc(
                date_range_days=self.config['data_collection']['gsc_date_range_days'],
                limit=self.config['data_collection']['top_queries_limit']
            )
            
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            if not ga4_data.empty:
                self.api_integration.export_to_csv(
                    ga4_data, 
                    f'ga4_data_{timestamp}.csv'
                )
            
            if not gsc_pages.empty:
                self.api_integration.export_to_csv(
                    gsc_pages, 
                    f'gsc_pages_{timestamp}.csv'
                )
            
            if not gsc_queries.empty:
                self.api_integration.export_to_csv(
                    gsc_queries, 
                    f'gsc_queries_{timestamp}.csv'
                )
            
            logger.info("ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
            return {
                'ga4_data': ga4_data,
                'gsc_pages': gsc_pages,
                'gsc_queries': gsc_queries,
                'timestamp': timestamp
            }
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def generate_analytics_report(self, data):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        try:
            logger.info("åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
            
            # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            summary = self.api_integration.generate_summary_report(
                date_range_days=self.config['data_collection']['ga4_date_range_days']
            )
            
            # è©³ç´°åˆ†æ
            detailed_analysis = self._perform_detailed_analysis(data)
            
            # ãƒ¬ãƒãƒ¼ãƒˆçµ±åˆ
            report = {
                'summary': summary,
                'detailed_analysis': detailed_analysis,
                'generated_at': datetime.now().isoformat(),
                'system_status': 'healthy'
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = f'data/processed/analytics_report_{data["timestamp"]}.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
            
            # Notionã«é€ä¿¡ï¼ˆè¨­å®šãŒæœ‰åŠ¹ãªå ´åˆï¼‰
            if (self.notion_integration and 
                self.config.get('notion', {}).get('sync_after_report_generation', False)):
                
                notion_page_id = self._sync_report_to_notion(report_file, report)
                if notion_page_id:
                    report['notion_page_id'] = notion_page_id
                    logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’Notionã«é€ä¿¡ã—ã¾ã—ãŸ: {notion_page_id}")
            
            return report
            
        except Exception as e:
            logger.error(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _perform_detailed_analysis(self, data):
        """è©³ç´°åˆ†æã®å®Ÿè¡Œ"""
        analysis = {
            'performance_analysis': {},
            'seo_analysis': {},
            'content_analysis': {},
            'recommendations': []
        }
        
        try:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            if not data['ga4_data'].empty:
                ga4_data = data['ga4_data']
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ
                if 'sessions' in ga4_data.columns:
                    total_sessions = ga4_data['sessions'].sum()
                    analysis['performance_analysis']['total_sessions'] = total_sessions
                
                # ãƒã‚¦ãƒ³ã‚¹ç‡åˆ†æ
                if 'bounceRate' in ga4_data.columns:
                    avg_bounce_rate = ga4_data['bounceRate'].mean()
                    analysis['performance_analysis']['avg_bounce_rate'] = avg_bounce_rate
                    
                    if avg_bounce_rate > self.config['alerts']['performance_threshold']['bounce_rate']:
                        analysis['recommendations'].append({
                            'type': 'performance',
                            'priority': 'high',
                            'message': f'ãƒã‚¦ãƒ³ã‚¹ç‡ãŒ{avg_bounce_rate:.2%}ã¨é«˜ã™ãã¾ã™ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚'
                        })
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“åˆ†æ
                if 'averageSessionDuration' in ga4_data.columns:
                    avg_duration = ga4_data['averageSessionDuration'].mean()
                    analysis['performance_analysis']['avg_session_duration'] = avg_duration
            
            # SEOåˆ†æ
            if not data['gsc_pages'].empty:
                gsc_pages = data['gsc_pages']
                
                # å¹³å‡æ¤œç´¢é †ä½
                if 'avg_position' in gsc_pages.columns:
                    avg_position = gsc_pages['avg_position'].mean()
                    analysis['seo_analysis']['avg_position'] = avg_position
                    
                    if avg_position > self.config['alerts']['performance_threshold']['avg_position']:
                        analysis['recommendations'].append({
                            'type': 'seo',
                            'priority': 'high',
                            'message': f'å¹³å‡æ¤œç´¢é †ä½ãŒ{avg_position:.1f}ä½ã¨ä½ã™ãã¾ã™ã€‚SEOæ”¹å–„ãŒå¿…è¦ã§ã™ã€‚'
                        })
                
                # CTRåˆ†æ
                if 'ctr_calculated' in gsc_pages.columns:
                    avg_ctr = gsc_pages['ctr_calculated'].mean()
                    analysis['seo_analysis']['avg_ctr'] = avg_ctr
                    
                    if avg_ctr < self.config['alerts']['performance_threshold']['ctr'] * 100:
                        analysis['recommendations'].append({
                            'type': 'seo',
                            'priority': 'medium',
                            'message': f'CTRãŒ{avg_ctr:.2f}%ã¨ä½ã™ãã¾ã™ã€‚ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã®æœ€é©åŒ–ãŒå¿…è¦ã§ã™ã€‚'
                        })
                
                # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸åˆ†æ
                top_pages = gsc_pages.head(10)
                analysis['seo_analysis']['top_pages'] = top_pages.to_dict('records')
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ
            if not data['gsc_queries'].empty:
                gsc_queries = data['gsc_queries']
                
                # ãƒˆãƒƒãƒ—ã‚¯ã‚¨ãƒªåˆ†æ
                top_queries = gsc_queries.head(20)
                analysis['content_analysis']['top_queries'] = top_queries.to_dict('records')
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
                keyword_analysis = self._analyze_keywords(gsc_queries)
                analysis['content_analysis']['keyword_analysis'] = keyword_analysis
            
            return analysis
            
        except Exception as e:
            logger.error(f"è©³ç´°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return analysis
    
    def _analyze_keywords(self, gsc_queries):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ"""
        try:
            if gsc_queries.empty:
                return {}
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒªã®åˆ†æ
            categories = {
                'gift_related': ['ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ', 'ã‚®ãƒ•ãƒˆ', 'è´ˆã‚Šç‰©', 'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ'],
                'occasion_related': ['èª•ç”Ÿæ—¥', 'ã‚¯ãƒªã‚¹ãƒã‚¹', 'ãƒãƒ¬ãƒ³ã‚¿ã‚¤ãƒ³', 'æ¯ã®æ—¥', 'çˆ¶ã®æ—¥'],
                'person_related': ['å½¼æ°', 'å½¼å¥³', 'å‹é”', 'å®¶æ—', 'ä¸Šå¸'],
                'product_related': ['ã‚¹ã‚¤ãƒ¼ãƒ„', 'ã‚³ã‚¹ãƒ¡', 'èŠ±æŸ', 'ãŠé…’']
            }
            
            keyword_analysis = {}
            
            for category, keywords in categories.items():
                category_data = gsc_queries[
                    gsc_queries['query'].str.contains('|'.join(keywords), case=False, na=False)
                ]
                
                if not category_data.empty:
                    keyword_analysis[category] = {
                        'total_clicks': category_data['clicks'].sum(),
                        'total_impressions': category_data['impressions'].sum(),
                        'avg_position': category_data['avg_position'].mean(),
                        'top_queries': category_data.head(5).to_dict('records')
                    }
            
            return keyword_analysis
            
        except Exception as e:
            logger.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def create_looker_studio_dashboard(self, data):
        """Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ"""
        try:
            if not self.config['reporting']['looker_studio_enabled']:
                logger.info("Looker Studioé€£æºãŒç„¡åŠ¹ã§ã™")
                return None
            
            logger.info("Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆé–‹å§‹")
            
            # ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š
            report_config = {
                'date_range_days': self.config['data_collection']['ga4_date_range_days'],
                'top_pages_limit': self.config['data_collection']['top_pages_limit'],
                'top_queries_limit': self.config['data_collection']['top_queries_limit']
            }
            
            # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
            dashboard_info = self.looker_connector.create_automated_report_system(
                self.api_integration, 
                report_config
            )
            
            if dashboard_info:
                logger.info(f"Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆå®Œäº†: {dashboard_info['dashboard_id']}")
                return dashboard_info
            else:
                logger.warning("Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆã«å¤±æ•—")
                return None
                
        except Exception as e:
            logger.error(f"Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def run_analysis_cycle(self):
        """åˆ†æã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿè¡Œ"""
        try:
            logger.info("=== åˆ†æã‚µã‚¤ã‚¯ãƒ«é–‹å§‹ ===")
            
            # ãƒ‡ãƒ¼ã‚¿åé›†
            data = self.collect_data()
            if not data:
                logger.error("ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = self.generate_analytics_report(data)
            if not report:
                logger.error("åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # Looker Studioãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if self.config['reporting']['auto_report_enabled']:
                dashboard_info = self.create_looker_studio_dashboard(data)
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
            self._check_alerts(report)
            
            logger.info("=== åˆ†æã‚µã‚¤ã‚¯ãƒ«å®Œäº† ===")
            
        except Exception as e:
            logger.error(f"åˆ†æã‚µã‚¤ã‚¯ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _check_alerts(self, report):
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯"""
        try:
            alerts = []
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆ
            if 'detailed_analysis' in report:
                analysis = report['detailed_analysis']
                
                # æ¨å¥¨äº‹é …ã®ãƒã‚§ãƒƒã‚¯
                if 'recommendations' in analysis:
                    for rec in analysis['recommendations']:
                        if rec.get('priority') == 'high':
                            alerts.append({
                                'type': 'high_priority',
                                'message': rec['message'],
                                'timestamp': datetime.now().isoformat()
                            })
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ­ã‚°
            if alerts:
                logger.warning(f"ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿ: {len(alerts)}ä»¶")
                for alert in alerts:
                    logger.warning(f"  - {alert['message']}")
                
                # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                alert_file = f'data/processed/alerts_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                with open(alert_file, 'w', encoding='utf-8') as f:
                    json.dump(alerts, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _sync_report_to_notion(self, report_file: str, report_data: Dict[str, Any]) -> Optional[str]:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’Notionã«é€ä¿¡"""
        try:
            if not self.notion_integration or not self.notion_converter:
                logger.error("Notionçµ±åˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
            
            logger.info("ãƒ¬ãƒãƒ¼ãƒˆã®Notioné€ä¿¡é–‹å§‹")
            
            # å¯¾å¿œã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            markdown_file = self._find_corresponding_markdown(report_file)
            
            # ãƒ¬ãƒãƒ¼ãƒˆã‚’Notionå½¢å¼ã«å¤‰æ›
            converted_report = self.notion_converter.convert_analysis_report(
                report_file, 
                markdown_file
            )
            
            if not converted_report:
                logger.error("ãƒ¬ãƒãƒ¼ãƒˆå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
            # Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—
            markdown_content = ""
            if markdown_file:
                with open(markdown_file, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
            
            # Notionãƒšãƒ¼ã‚¸ã§ã®Reporting
            page_id = self.notion_integration.create_report_page(
                converted_report, 
                markdown_content
            )
            
            if page_id:
                logger.info(f"Notionãƒ¬ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸ä½œæˆæˆåŠŸ: {page_id}")
                return page_id
            else:
                logger.error("Notionãƒšãƒ¼ã‚¸ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
                
        except Exception as e:
            logger.error(f"Notioné€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _find_corresponding_markdown(self, json_file_path: str) -> Optional[str]:
        """å¯¾å¿œã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™"""
        try:
            # JSONãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¯¾å¿œã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨æ¸¬
            base_name = os.path.basename(json_file_path)
            
            # ä¸€èˆ¬çš„ãªMarkdownãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€ã‚’æ¤œç´¢
            markdown_locations = [
                'docs/analytics/',
                'data/processed/',
                os.path.dirname(json_file_path) + '/'
            ]
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if 'purchase' in base_name:
                keywords = ['purchase', 'moodmark', '7days', 'analysis']
            else:
                keywords = ['moodmark', '7days', 'analysis']
            
            # å„å ´æ‰€ã§Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for location in markdown_locations:
                if os.path.exists(location):
                    for filename in os.listdir(location):
                        if filename.endswith('.md'):
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                            matches = sum(1 for keyword in keywords if keyword in filename.lower())
                            if matches >= 2:  # æœ€ä½2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãƒãƒƒãƒã—ãŸå ´åˆ
                                markdown_path = os.path.join(location, filename)
                                logger.info(f"å¯¾å¿œã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {markdown_path}")
                                return markdown_path
            
            # ç›´æ¥çš„ãªãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            markdown_patterns = [
                'docs/analytics/moodmark_7days_analysis_report.md',
                'docs/analytics/analysis_report.md',
                'data/processed/report.md'
            ]
            
            for pattern in markdown_patterns:
                if os.path.exists(pattern):
                    logger.info(f"æ¨™æº–Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {pattern}")
                    return pattern
            
            logger.warning("å¯¾å¿œã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
            
        except Exception as e:
            logger.error(f"Markdownãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def create_notion_kpi_dashboard(self) -> Optional[str]:
        """Notion KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ä½œæˆ"""
        try:
            if not self.notion_integration:
                logger.error("Notionçµ±åˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
            
            logger.info("Notion KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆé–‹å§‹")
            
            # æœ€æ–°ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            data = self.collect_data()
            if not data:
                logger.error("ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
            
            # KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            kpi_report = self._generate_kpi_report(data)
            
            # Notionãƒšãƒ¼ã‚¸ã®ä½œæˆ
            dashboard_page_id = self.notion_integration.create_report_page(
                kpi_report,
                self._format_kpi_dashboard_content(kpi_report)
            )
            
            if dashboard_page_id:
                logger.info(f"KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆå®Œäº†: {dashboard_page_id}")
                return dashboard_page_id
            else:
                logger.error("KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
                
        except Exception as e:
            logger.error(f"KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _generate_kpi_report(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """KPIç”¨ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        return {
            'report_date': datetime.now().isoformat(),
            'period': 'KPI Dashboard',
            'site_url': 'https://isetan.mistore.jp/moodmark',
            'summary': data.get('ga4_data', pd.DataFrame()).to_dict('records')[0] if not data.get('ga4_data', pd.DataFrame()).empty else {},
            'recommendations': ['KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®å®šæœŸçš„ãªç›£è¦–ã‚’æ¨å¥¨ã—ã¾ã™ã€‚']
        }
    
    def _format_kpi_dashboard_content(self, kpi_report: Dict[str, Any]) -> str:
        """KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ•´å½¢"""
        content = f"""
# ğŸ“ˆ MOO-D MARK KPIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

**æ›´æ–°æ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}

## ä¸»è¦æŒ‡æ¨™

ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¯è‡ªå‹•æ›´æ–°ã•ã‚Œã¾ã™ã€‚æœ€æ–°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã§ãã¾ã™ã€‚

## æ³¨æ„äº‹é …

- ãƒ‡ãƒ¼ã‚¿ã¯æ¯æ—¥è‡ªå‹•æ›´æ–°ã•ã‚Œã¾ã™
- ç•°å¸¸å€¤ã‚’ç™ºè¦‹ã—ãŸå ´åˆã¯ã€è©³ç´°ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„
- æ”¹å–„ææ¡ˆã«ã¤ã„ã¦ã¯ã€ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„
        """
        
        return content.strip()
    
    def start_scheduled_analysis(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æã®é–‹å§‹"""
        try:
            logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æé–‹å§‹")
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
            if self.config['reporting']['report_frequency'] == 'daily':
                schedule.every().day.at("09:00").do(self.run_analysis_cycle)
            elif self.config['reporting']['report_frequency'] == 'weekly':
                schedule.every().monday.at("09:00").do(self.run_analysis_cycle)
            elif self.config['reporting']['report_frequency'] == 'hourly':
                schedule.every().hour.do(self.run_analysis_cycle)
            
            # åˆå›å®Ÿè¡Œ
            self.run_analysis_cycle()
            
            self.is_running = True
            logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æé–‹å§‹å®Œäº†")
            
            # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
            while self.is_running:
                schedule.run_pending()
                time.sleep(60)  # 1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
                
        except KeyboardInterrupt:
            logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æåœæ­¢")
            self.is_running = False
        except Exception as e:
            logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            self.is_running = False
    
    def stop_scheduled_analysis(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æã®åœæ­¢"""
        self.is_running = False
        logger.info("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†æåœæ­¢è¦æ±‚")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=== çµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ èµ·å‹• ===")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = IntegratedAnalyticsSystem()
    
    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    if not system.api_integration.credentials:
        print("Google APIsèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    if not system.looker_connector.credentials:
        print("Looker Studioèªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
    
    print("çµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ
    import sys
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        
        if mode == 'once':
            # ä¸€å›ã ã‘å®Ÿè¡Œ
            system.run_analysis_cycle()
        elif mode == 'schedule':
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
            system.start_scheduled_analysis()
        else:
            print("ä½¿ç”¨æ³•: python integrated_analytics_system.py [once|schedule]")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¸€å›ã ã‘å®Ÿè¡Œ
        system.run_analysis_cycle()

if __name__ == "__main__":
    main()
